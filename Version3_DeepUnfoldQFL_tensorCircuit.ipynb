{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNasv9ChctVTgJRIKA489Vw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanikairoshi/QFL-with-DUN/blob/main/Version3_DeepUnfoldQFL_tensorCircuit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdaWzw9Dnz2B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorcircuit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dh_SRnsMn_yR",
        "outputId": "933e52cc-87b0-49e6-d83f-83caa10116c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorcircuit\n",
            "  Downloading tensorcircuit-0.11.0-py3-none-any.whl (329 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/329.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/329.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.4/329.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorcircuit) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tensorcircuit) (1.11.4)\n",
            "Collecting tensornetwork (from tensorcircuit)\n",
            "  Downloading tensornetwork-0.4.6-py3-none-any.whl (364 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.3/364.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from tensorcircuit) (3.2.1)\n",
            "Requirement already satisfied: graphviz>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from tensornetwork->tensorcircuit) (0.20.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from tensornetwork->tensorcircuit) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensornetwork->tensorcircuit) (3.9.0)\n",
            "Installing collected packages: tensornetwork, tensorcircuit\n",
            "Successfully installed tensorcircuit-0.11.0 tensornetwork-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorcircuit as tc\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"false\"\n",
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
        "\n",
        "plt.rcParams[\"font.family\"] = \"serif\"\n",
        "plt.rcParams['mathtext.fontset'] = 'cm'\n",
        "plt.rcParams['mathtext.rm'] = 'serif'\n",
        "plt.rc('font', size=14)\n",
        "\n",
        "K = tc.set_backend('jax')\n",
        "key = jax.random.PRNGKey(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "\n",
        "dataset = 'mnist'\n",
        "# dataset = 'fashion'\n",
        "readout_mode = 'softmax'\n",
        "# readout_mode = 'sample'\n",
        "encoding_mode = 'vanilla'\n",
        "# encoding_mode = 'mean'\n",
        "# encoding_mode = 'half'\n",
        "\n",
        "T=5\n",
        "n = 8\n",
        "n_node = 8\n",
        "k = 20\n",
        "\n",
        "def filter(x, y, class_list):\n",
        "    keep = jnp.zeros(len(y)).astype(bool)\n",
        "    for c in class_list:\n",
        "        keep = keep | (y == c)\n",
        "    x, y = x[keep], y[keep]\n",
        "    y = jax.nn.one_hot(y, n_node)\n",
        "    return x, y\n",
        "\n",
        "def clf(params, c, k):\n",
        "    for j in range(k):\n",
        "        for i in range(n - 1):\n",
        "            c.cnot(i, i + 1)\n",
        "        for i in range(n):\n",
        "            c.rx(i, theta=params[3 * j, i])\n",
        "            c.rz(i, theta=params[3 * j + 1, i])\n",
        "            c.rx(i, theta=params[3 * j + 2, i])\n",
        "    return c\n",
        "\n",
        "def readout(c):\n",
        "    if readout_mode == 'softmax':\n",
        "        logits = []\n",
        "        for i in range(n_node):\n",
        "            logits.append(jnp.real(c.expectation([tc.gates.z(), [i,]])))\n",
        "        logits = jnp.stack(logits, axis=-1) * 10\n",
        "        probs = jax.nn.softmax(logits)\n",
        "    elif readout_mode == 'sample':\n",
        "        wf = jnp.abs(c.wavefunction()[:n_node])**2\n",
        "        probs = wf / jnp.sum(wf)\n",
        "    return probs\n",
        "\n",
        "def loss(params, x, y, k):\n",
        "    c = tc.Circuit(n, inputs=x)\n",
        "    c = clf(params, c, k)\n",
        "    probs = readout(c)\n",
        "    #print(\"Length of probability:\",len(probs))\n",
        "    #print(-jnp.mean(jnp.sum(y * jnp.log(probs + 1e-7), axis=-1)))\n",
        "    return -jnp.mean(jnp.sum(y * jnp.log(probs + 1e-7), axis=-1))\n",
        "loss = K.jit(loss, static_argnums=[3])\n",
        "\n",
        "def accuracy(params, x, y, k):\n",
        "    c = tc.Circuit(n, inputs=x)\n",
        "    c = clf(params, c, k)\n",
        "    probs = readout(c)\n",
        "    return jnp.argmax(probs, axis=-1) == jnp.argmax(y, axis=-1)\n",
        "accuracy = K.jit(accuracy, static_argnums=[3])\n",
        "\n",
        "compute_loss = K.jit(K.vectorized_value_and_grad(loss, vectorized_argnums=[1, 2]), static_argnums=[3])\n",
        "compute_accuracy = K.jit(K.vmap(accuracy, vectorized_argnums=[1, 2]), static_argnums=[3])\n",
        "\n",
        "def pred(params, x, k):\n",
        "    c = tc.Circuit(n, inputs=x)\n",
        "    c = clf(params, c, k)\n",
        "    probs = readout(c)\n",
        "    return probs\n",
        "pred = K.vmap(pred, vectorized_argnums=[1])\n",
        "\n",
        "compute_loss = K.jit(K.vectorized_value_and_grad(loss, vectorized_argnums=[1, 2]), static_argnums=[3])\n",
        "compute_accuracy = K.jit(K.vmap(accuracy, vectorized_argnums=[1, 2]), static_argnums=[3])"
      ],
      "metadata": {
        "id": "jZrTWhArn0y3"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy data\n",
        "if dataset == 'mnist':\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "elif dataset == 'fashion':\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "ind = y_test == 9\n",
        "x_test, y_test = x_test[~ind], y_test[~ind]\n",
        "ind = y_test == 8\n",
        "x_test, y_test = x_test[~ind], y_test[~ind]\n",
        "ind = y_train == 9\n",
        "x_train, y_train = x_train[~ind], y_train[~ind]\n",
        "ind = y_train == 8\n",
        "x_train, y_train = x_train[~ind], y_train[~ind]\n",
        "\n",
        "x_train = x_train / 255.0\n",
        "if encoding_mode == 'vanilla':\n",
        "    mean = 0\n",
        "elif encoding_mode == 'mean':\n",
        "    mean = jnp.mean(x_train, axis=0)\n",
        "elif encoding_mode == 'half':\n",
        "    mean = 0.5\n",
        "x_train = x_train - mean\n",
        "x_train = tf.image.resize(x_train[..., tf.newaxis], (int(2**(n/2)), int(2**(n/2)))).numpy()[..., 0].reshape(-1, 2**n)\n",
        "x_train = x_train / jnp.sqrt(jnp.sum(x_train**2, axis=-1, keepdims=True))\n",
        "\n",
        "x_test = x_test / 255.0\n",
        "x_test = x_test - mean\n",
        "x_test = tf.image.resize(x_test[..., tf.newaxis], (int(2**(n/2)), int(2**(n/2)))).numpy()[..., 0].reshape(-1, 2**n)\n",
        "x_test = x_test / jnp.sqrt(jnp.sum(x_test**2, axis=-1, keepdims=True))\n",
        "y_test = jax.nn.one_hot(y_test, n_node)\n",
        "\n",
        "def filter_pair(x, y, a, b):\n",
        "    keep = (y == a) | (y == b)\n",
        "    x, y = x[keep], y[keep]\n",
        "    y = jax.nn.one_hot(y, n_node)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "RtVOoTXdQ7l2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KzqVaZ-jRWHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Unfolded_FLtraining(params_list,opt_state_list):\n",
        "\n",
        "\n",
        "  paramslist_all = []\n",
        "  xlist_all = []\n",
        "  ylist_all = []\n",
        "  opt_statenodeall=[]\n",
        "\n",
        "\n",
        "\n",
        "  params_listnode=[]\n",
        "  xnode=[]\n",
        "  ynode=[]\n",
        "  opt_statenode=[]\n",
        "  loss_list=[]\n",
        "  acc_list = []\n",
        "\n",
        "\n",
        "  params_list=params_list\n",
        "  opt_state_list=opt_state_list\n",
        "\n",
        "  for t in tqdm(range(T), leave=False):\n",
        "    #for b in range(100):\n",
        "      print(\"Iteration \", t)\n",
        "\n",
        "\n",
        "      params_lists=[]\n",
        "      xnodelists=[]\n",
        "      ynodelists=[]\n",
        "      for node in range(n_node-1):\n",
        "        try:\n",
        "            x, y = next(iter_list[node])\n",
        "        except StopIteration:\n",
        "            iter_list[node] = iter(data_list[node])\n",
        "            x, y = next(iter_list[node])\n",
        "        x = x.numpy()\n",
        "        y = y.numpy()\n",
        "        loss_val, grad_val = compute_loss(params_list[node], x, y, k)\n",
        "        updates, opt_state_list[node] = opt.update(grad_val, opt_state_list[node], params_list[node])\n",
        "        params_list[node] = optax.apply_updates(params_list[node], updates)\n",
        "\n",
        "        params_listnode.append(params_list[node])\n",
        "        opt_statenode.append(opt_state_list[node])\n",
        "        xnode.append(x)\n",
        "        ynode.append(y)\n",
        "\n",
        "      avg_params = jnp.mean(jnp.stack(params_list, axis=0), axis=0)\n",
        "      for node in range(n_node-1):\n",
        "        params_list[node] = avg_params\n",
        "\n",
        "      paramslist_all.append(params_listnode)\n",
        "      xlist_all.append(xnode)\n",
        "      ylist_all.append(ynode)\n",
        "      opt_statenodeall.append(opt_statenode)\n",
        "        #if b % 25 == 0:\n",
        "      avg_loss = jnp.mean(compute_loss(avg_params, x_test[:1024], y_test[:1024], k)[0])\n",
        "      loss_list.append(avg_loss)\n",
        "      acc_list.append(compute_accuracy(avg_params, x_test[:1024], y_test[:1024], k).mean())\n",
        "      print(f\"Epoch {t}, loss {avg_loss}, accuracy {acc_list[-1]}\")\n",
        "\n",
        "  plt.plot(loss_list)\n",
        "  plt.plot(acc_list)\n",
        "  plt.legend(['loss', 'accuracy'])\n",
        "  plt.ylim(0, 3)\n",
        "  plt.show()\n",
        "\n",
        "  #jnp.save(f'./{dataset}/fedavg_loss.npy', loss_list)\n",
        "  #jnp.save(f'./{dataset}/fedavg_acc.npy', acc_list)\n",
        "\n",
        "  return paramslist_all, xlist_all, ylist_all, opt_statenodeall\n",
        "\n"
      ],
      "metadata": {
        "id": "x9OwtsheRWO9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dunloss(paramslist_all, xlist_all, ylist_all, k): #pass karanawa iteration one and two node wala params and x,y\n",
        "#e pass karan param tika man ganna one fed process eke yaddi save karagathha ewa.\n",
        "#me process ekedi api eka eka iteration eke nemei all iteration waladi loss balala params set eka update keranwa.\n",
        "    losses=[]\n",
        "    # Iterate through the values row-wise for the specified column\n",
        "    for t in range (T):\n",
        "      for node in range(n_node-1):\n",
        "\n",
        "        #value = paramslist_all[t][node]\n",
        "        #inputs = xlist_all[t][node]\n",
        "\n",
        "        # Print the shape of the inputs for debugging\n",
        "        #print(f\"Node: {node}, Iteration: {t}, Input Shape: {inputs.shape}\")\n",
        "\n",
        "\n",
        "        #print(f\"Value for Column {0} Row {row + 1}: {value}\")\n",
        "        c = tc.Circuit(n, inputs=xlist_all[t][node])\n",
        "        probs = readout(c)\n",
        "        loss = -jnp.mean(jnp.sum(ylist_all[t][node] * jnp.log(probs + 1e-7), axis=-1))\n",
        "        #losses.append(loss)\n",
        "        #print(f\"Value for Column {0} Row {row + 1}: {probs}\")\n",
        "    #return jnp.sum(losses)  # Sum the losses for all nodes and iterations\n",
        "    return -jnp.mean(jnp.sum(ylist_all[t][node] * jnp.log(probs + 1e-7), axis=-1))\n",
        "\n",
        "dunloss = K.jit(dunloss, static_argnums=[3])\n",
        "\n",
        "compute_dunloss = K.jit(K.vectorized_value_and_grad(dunloss, vectorized_argnums=[1, 2]), static_argnums=[3])"
      ],
      "metadata": {
        "id": "a65mKrQYICUC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_list = []\n",
        "opt_state_list = []\n",
        "data_list = []\n",
        "iter_list = []\n",
        "for node in tqdm(range(n_node-1)):\n",
        "    x_train_node, y_train_node = filter_pair(x_train, y_train, 0, node + 1)\n",
        "    data = tf.data.Dataset.from_tensor_slices((x_train_node, y_train_node)).batch(128)\n",
        "    data_list.append(data)\n",
        "    iter_list.append(iter(data))\n",
        "\n",
        "    key, subkey = jax.random.split(key)\n",
        "    params = jax.random.normal(subkey, (3 * k, n))\n",
        "    opt = optax.adam(learning_rate=1e-2)\n",
        "    opt_state = opt.init(params)\n",
        "    params_list.append(params)\n",
        "    opt_state_list.append(opt_state)\n",
        "\n",
        "Unfoldlayers=2\n",
        "for l in range(Unfoldlayers):\n",
        "    print(f\"Unfolding layer\", l)\n",
        "    paramslist_all, xlist_all, ylist_all, opt_statenodeall=Unfolded_FLtraining(params_list,opt_state_list)\n",
        "\n",
        "    unfoldlayer_wise_weights=[]\n",
        "\n",
        "    loss_val, grad_val = compute_dunloss(paramslist_all, xlist_all, ylist_all, k)\n",
        "\n",
        "    for t in range (T):\n",
        "      for node in range(n_node-1):\n",
        "          updates,opt_statenodeall[t][node]= opt.update(grad_val[t][node], opt_statenodeall[t][node], paramslist_all[t][node])\n",
        "          paramslist_all[t][node] = optax.apply_updates(paramslist_all[t][node], updates)\n",
        "          #print(paramslist_all[t][node])\n",
        "          #Assign last parameter set aggregation in last iteration in first deep layer\n",
        "    #at the end of the last iteration of each deep unfolding layer,  need to get average parameters and assign it to each nodes parameters.\n",
        "    #if t==T:\n",
        "    avg_params = jnp.mean(jnp.stack(paramslist_all[T-1], axis=0), axis=0)\n",
        "      #params=avg_params\n",
        "    for node in range(n_node-1):\n",
        "          params_list[node] = avg_params\n",
        "\n",
        "    #Calculate the weights using parameters now.\n",
        "    # Calculate sum of squares for each node and get normailzed values\n",
        "    sum_of_squares_per_iter = []\n",
        "    for t in range(T):\n",
        "        sum_of_squares_for_t =0\n",
        "        for node in range(n_node-1):\n",
        "            sum_of_squares_for_t+=jnp.sum(paramslist_all[t][node]**2)\n",
        "            print(sum_of_squares_for_t)\n",
        "            sum_of_squares_per_iter.append(sum_of_squares_for_t)\n",
        "    for t in range(T):\n",
        "      print(\"Sum of squared in iteration\",t,\" is \", sum_of_squares_per_iter[t])\n",
        "\n",
        "    normalized_weights_Per_it = []\n",
        "    for t in range(T):\n",
        "        normalized_weights_node=[]\n",
        "        for node in range(n_node-1):\n",
        "           #print(\"Iteration\", t)\n",
        "           nodeweights=jnp.sum((paramslist_all[t][node])**2)/jnp.array(sum_of_squares_per_iter[t])\n",
        "           #print(f\"Iteration {t}, Node {node}\", nodeweights )\n",
        "           #node_sum_of_squares[t][node] = (jnp.sum(paramslist_all[t][node]**2))/jnp.array(sum_of_squares_per_iter[t])\n",
        "           #print(len(node_sum_of_squares[]))\n",
        "\n",
        "           normalized_weights_node.append(nodeweights)\n",
        "        normalized_weights_Per_it.append(normalized_weights_node)\n",
        "    #print(normalized_weights_Per_it)\n",
        "\n",
        "    import csv\n",
        "\n",
        "    with open('weights.csv', 'w', newline='') as csvfile:\n",
        "      writer = csv.writer(csvfile)\n",
        "\n",
        "      # Write the header row\n",
        "      writer.writerow(['Iteration', 'Node', 'Weight'])\n",
        "\n",
        "      # Write the data rows\n",
        "      for i in range(T):\n",
        "        for j in range(n_node-1):\n",
        "            writer.writerow([i, j, normalized_weights_Per_it[i][j]])\n",
        "\n",
        "    with open('weights.csv', 'r') as csvfile:\n",
        "      reader = csv.reader(csvfile)\n",
        "\n",
        "      # Read the header row\n",
        "      header = next(reader)\n",
        "\n",
        "      # Read the data rows\n",
        "      for row in reader:\n",
        "        iteration, node, weight = row\n",
        "        #print(f'Iteration {iteration}, Node {node}, Weight {weight}')\n",
        "\n",
        "    print(\"Deep unfolding\", \"Layer\", l, \"done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kMqzqPi07y5u",
        "outputId": "f18cce4d-ed13-4ddd-8a6b-e1b6d9d56b1b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 31.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unfolding layer 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 1/5 [00:04<00:17,  4.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss 2.070585250854492, accuracy 0.169921875\n",
            "Iteration  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:07<00:11,  3.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss 2.054739475250244, accuracy 0.166015625\n",
            "Iteration  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [00:10<00:06,  3.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, loss 2.0378098487854004, accuracy 0.173828125\n",
            "Iteration  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [00:14<00:03,  3.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, loss 2.0215420722961426, accuracy 0.1748046875\n",
            "Iteration  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                             "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, loss 2.006730794906616, accuracy 0.1728515625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "460.49744\n",
            "884.9363\n",
            "1312.4197\n",
            "1783.8882\n",
            "2254.972\n",
            "2715.3345\n",
            "3259.1133\n",
            "460.49744\n",
            "884.9363\n",
            "1312.4197\n",
            "1783.8882\n",
            "2254.972\n",
            "2715.3345\n",
            "3259.1133\n",
            "460.49744\n",
            "884.9363\n",
            "1312.4197\n",
            "1783.8882\n",
            "2254.972\n",
            "2715.3345\n",
            "3259.1133\n",
            "460.49744\n",
            "884.9363\n",
            "1312.4197\n",
            "1783.8882\n",
            "2254.972\n",
            "2715.3345\n",
            "3259.1133\n",
            "460.49744\n",
            "884.9363\n",
            "1312.4197\n",
            "1783.8882\n",
            "2254.972\n",
            "2715.3345\n",
            "3259.1133\n",
            "Sum of squared in iteration 0  is  3259.1133\n",
            "Sum of squared in iteration 1  is  3259.1133\n",
            "Sum of squared in iteration 2  is  3259.1133\n",
            "Sum of squared in iteration 3  is  3259.1133\n",
            "Sum of squared in iteration 4  is  3259.1133\n",
            "Deep unfolding Layer 0 done\n",
            "Unfolding layer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 1/5 [00:03<00:13,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss 2.010016918182373, accuracy 0.1806640625\n",
            "Iteration  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:06<00:09,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss 1.9930768013000488, accuracy 0.1865234375\n",
            "Iteration  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [00:10<00:07,  3.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, loss 1.9779231548309326, accuracy 0.1875\n",
            "Iteration  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [00:13<00:03,  3.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, loss 1.9654345512390137, accuracy 0.1875\n",
            "Iteration  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                             "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, loss 1.950136423110962, accuracy 0.1943359375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGoCAYAAABlvr66AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAz/0lEQVR4nO3de3TU9Z3/8dfMkAQCZkBIMk0IYFoRocvFbGoBqQpSQbDaFqtQRaTauoZN+wO1RahQcKGK3VYKq1Y0oRbwVqGuh9bFnNJKWQ4x3BYxNW0JhGsJJDNAQm7z+f0BM2Qyk2QmJJlvkufjnO9Jvp/P5/uZ92RG5uX3NjZjjBEAAICF2aNdAAAAQHMILAAAwPIILAAAwPIILAAAwPIILAAAwPIILAAAwPIILAAAwPIILAAAwPIILAAAwPIiDiy1tbV666239NBDD2nYsGFKSUlRYmKiRo4cqWeffVbnzp2LaL7NmzfrK1/5ipKSkpScnKzJkydr586dkZYFAAA6sYgDS2lpqe69917t3btXb775po4dO6bjx48rOztbTz31lG677TbV1taGNdeaNWs0ZcoU3X777Tp27JiKi4t1zTXX6KabbtKWLVsifjIAAKBzskX6XUInTpzQ5z73Oe3atUujRo0K6Lvnnnv0zjvv6MMPP9SECROanOfo0aO69tprlZmZqT/96U/+9traWl133XWqqqpSUVGRevToEUl5AACgE4p4D0vfvn21bds2jRw5Mqhv4MCBkiS3293sPK+++qoqKys1ffr0gPZu3bpp2rRpOnr0qN59991IywMAAJ1QxIElJiZGY8eOlc1mC+rbuXOnunfvrhtvvLHZefLy8iRJGRkZQX2+Ng4LAQAAqRWuEvJ6vTp48KCysrK0a9cu5eTkKDU1tdntPv30U0kKOdbXVlhYeKXlAQCATqDblWz8/vvva8aMGTp79qzS09O1fv16fe1rXwtr2/LycklSz549g/p8bWVlZY1uX1VVpaqqKv+61+vVmTNn1Ldv35B7fwAAgPUYY3T27FmlpKTIbm9iP4ppBf/85z/NypUrTXx8vJk6dao5e/Zss9vExMQYSaa8vDyob/fu3UaSGTx4cKPbL1q0yEhiYWFhYWFh6QRLSUlJk7kh4quEmvKf//mfmjdvnubMmaNf/vKXTY5NSkrSqVOndPToUaWkpAT0/eUvf9FNN92kG2+8UTt27Ai5fcM9LG63WwMGDFBJSYkSEhKu/MkAAIA25/F4lJaWpvLycjmdzkbHXdEhoYamTp2qefPmadOmTc0Gluuvv77RwHL06FFJ0pAhQxrdPi4uTnFxcUHtCQkJBBYAADqY5k7niPik261bt+rtt98O2RcfHy9JOn36dLPz+O7TUlBQENTna5s4cWKk5QEAgE6oRYFl0aJF8nq9QX2+y5AbXtZ85MgR1dXVBbTNnj1bPXr00IYNGwLaa2tr9c477yg1NVVf//rXIy0PAAB0Qi26rPnTTz/VI488omPHjkmSqqur9c4772jevHlKSEjQ888/7x/7s5/9TGlpafrGN74RMEf//v31wgsv6M9//rOWLVumuro6XbhwQdnZ2Tp8+LBee+01/x4bAADQtUV8DsucOXOUnJysjRs3auzYsTp//rwqKyuVmpqqGTNm6PHHH9egQYP84z/3uc+pZ8+e/rvg1vfII48oNTVVy5cv189//nPZbDbdcMMN2rZtW1g3nwMAAF1Dq14lFE0ej0dOp1Nut5uTbgEA6CDC/fxu1auEAADWVlNTE3ROIdCaHA6HYmJiWn1eAgsAdAEej0elpaUB968C2kpcXJz69evXqkc8CCwA0Ml5PB4dPXpUvXr1Ur9+/RQTE8NXmKBNGGNUU1Mjt9vtv6daa4UWAgsAdHKlpaXq1auX+vfvT1BBm+vRo4euuuoqHTlyRKWlpa0WWK7425oBANZVU1OjqqoqOZ1Owgrajc1mk9PpVFVVlWpqalplTgILAHRivhNs2+IkSKApvvdca53kTWABgC6AvStob639niOwAAAAyyOwAAAAyyOwAAA6rVdeeUUul0uxsbEcFuvgCCwAgE7rkUce0YkTJzRmzJhol4IrRGABAACWR2ABAACWR2ABAHRZGzdu1C233KKkpCQlJSXpi1/8opYtWxb0nUter1erVq3SqFGjlJKSotTUVGVkZOhHP/qRPvvss4jHIXLcmh8AuihjjCprrP/NzT1iHG1ywuyiRYu0ZMkSLVu2TB988IHi4uKUl5enb37zm/rDH/6gLVu2KC4uTpL09NNP6+c//7k2b96sm2++WZK0ZcsWTZs2Td27d9fixYsjGofIEVgAoIuqrKnT0Kc/iHYZzTqw5HbFx7bux1VBQYGWLFmiL3/5y5o/f76/fcKECXr88cf14x//WCtWrNDChQslSe+++66uu+46fwiRpIkTJ2ru3Lnq27evvy3ccYgch4QAAF3Ob37zG0nSN7/5zaC+adOmSZJ+/etf+9sGDRqk3bt3a9GiRTpx4oS/fdGiRZozZ07E4xA59rAAQBfVI8ahA0tuj3YZzeoR42j1Of/6179Kkvr37x/Ul5qaKkn629/+prq6OjkcDq1atUozZszQkiVL9Mwzz+jGG2/U3XffrZkzZ8rlcvm3DXccIsceFgDoomw2m+Jju1l+scIN39LT07Vjxw59/PHH+uEPf6jS0lL98Ic/1Be+8AW99957EY9D5AgsAIAuZ8iQIZKkI0eOBPX52q699lo5HBf37vi+cTgjI0PLli3TZ599pk2bNqmmpkbZ2dn+bcMdh8gRWAAAXc4DDzwgm82m3/72t0F9vraZM2f62z7/+c9rx44dAePuuusuDRs2TGfOnIl4HCJHYAEAdDmjRo3S4sWLtWPHDi1btkzV1dWSpLy8PP3sZz/TuHHj9Pjjjwds89RTT+nQoUOSLl4S/rvf/U6ffPKJHnzwwRaNQ2RsxhgT7SJag8fjkdPplNvtVkJCQrTLAQBLuHDhgg4ePKhrrrlG3bt3j3Y57e6VV17Rj3/8Y505c0Y1NTVKTk7W3XffrZdeekmStGnTJv3iF7/QgQMHJEmJiYn69re/rXnz5vnvwSJJmzdv1vr167Vz506dO3dOxhj1799fs2fP1ne/+13/oaNwx3UF4b73wv38JrAAQCfW1QMLoqe1AwuHhAAAgOURWAAAgOURWAAAgOURWAAAgOURWAAAgOURWAAAgOURWAAAgOURWAAAgOURWAAAgOURWAAAgOURWAAAgOURWAAAgOURWAAAgOURWAAAgOURWAAAgOURWAAAgOURWAAAgOURWAAAnZbH49Hy5cs1duxY9e/fX3369NHnP/95Pf744zp79mzIbYqLi/Wd73xHaWlpcrlcGjBggG655RY9++yzOn36dMRjb7nlFl199dWy2WxavHixf9ucnBy5XC45HA4NGjTI315RUSGXyyWn0ymbzabc3FytWrVK//Iv/6KEhISAef74xz/qwQcf1LXXXiuXy6U+ffpowoQJ2rJlS6N/k5ycHN14441KTEyUy+XSF7/4Rc2aNUt5eXmSpOuuu06xsbGy2WxKTEzUjBkz/NvOmDFDffv2ld1ul8vl0pkzZyJ5Oa6M6STcbreRZNxud7RLAQDLqKysNAcOHDCVlZXBnV6vMVXnrL94vS1+/vn5+UaSefbZZ01NTY3xer1m27ZtxuVymS9/+cumrq4uYPyePXtMnz59zNixY82hQ4eMMcaUl5eb7373u0aSycnJadHYP/7xj0aSWbRoUVCNAwcONAMHDgxqz8nJMZLMyJEjzbJly0xlZaWprKw0Y8eO9c8zZcoUM3ToUPPZZ58ZY4zxeDzmscceMzabzbz//vtBc86ePds4HA7z6quvmtraWmOMMX/+859NUlJSQA1PPvmkkWTWrVsXNMfzzz9vpk6dGtTeUJPvvXrC/fzu1n7RCABgKTUV0rKUaFfRvKeOSbE9W7RpfHy87rzzTj355JP+trFjx2rhwoWaM2eO/ud//keTJk2SJBlj9OCDD8rtdmv9+vUaMGCAJMnpdOrFF1/UBx984J8jkrFXqnfv3po/f75/fcWKFXI4HJKk9PR0PfbYY7r22mslSVdddZVWrlypd999V0uXLtWUKVP8223cuFGvvfaaHn74Yc2ePdvfPm7cOC1btkxLly71tz366KN6/vnntXr16oA9LMYYvfjii1q1alWrPb9wEVgAAJ3W0KFD9d577wW1X3/99ZKkffv2+QPLvn37tHfvXo0aNcofQHzsdrs2btyopKSkiMdeqYkTJwasjx492v/7ypUrg8Y7HA5de+21+vjjjwPaf/3rX0uS7rzzzqBtvvWtb+kLX/iCf/2aa67RpEmTtHnzZu3du1cjRoyQJH3wwQey2Wy6/fbbW/6EWojAAgBdVUz8xb0XVhcTf0Wbv/POO1qzZo2Kiop09uxZ2e12VVdXS7p4vojPX//6V0lSWlpayHlGjRrVorFXyuVyNdp35MgR/eIXv1BeXp6OHz/ubz9z5oxqamoCxjZV81VXXaWbb745oC0rK0ubN2/W6tWr9atf/UqStHr1av3bv/2bbDZbi59PSxFYAKCrstlafKilo3j66ae1dOlSfetb39JHH32klJSLh8C2bt2qW2+9NeQ2VVVVYc8fydjGeL1e2e2NXwPTWN+JEyeUkZGhiooKvfHGG/rqV7+qmJgYSRdP9P3Tn/50RTVPmjRJ6enpWr9+vVasWKHy8nJt3brVv6emvUV0lVBVVZXefPNNTZ06VS6XS3379lViYqKmTJmiDz/8MOx5iouL5XA45HK5Qi7r16+P+IkAANDQ6tWrJV08dOILK40ZMmSIpIt7LUIpLS31X/kTyVhJ/iDRcK+HMUYnT55s7mmE9Pbbb+uf//yn7r//fk2ZMsX/GI1pqubq6modOXIkoD673a7vfe97On/+vNauXauXXnpJ9957r/r06dOieq9URIHliSee0H333afhw4erqKhIp0+fVkFBgaqrqzVx4sSITsJJS0vTiRMnQi71T/ABAKClfB/iDQ9hFBcXB40dPny4Ro4cqQMHDvgPn/hUV1dr2LBhysnJiXisJPXv31+SdOjQoYCx//u//+s/PNVazy3U40jSzJkzJUm//e1vg/pefPFFjRw5Ul6vN6D9O9/5juLi4rR69Wq9+uqrysrKalGtraLZ65LqycrKMuPGjQtqP3XqlOnRo4eJi4szZWVlzc5z8ODBkJdwXQkuawaAYOFeWtpZPf7440aSmT59uikvLzfGGLN3715zzTXXhLzM2Hep8k033WQOHz5sjDHm9OnTZsaMGWbIkCH+OSIda4wx//qv/2qcTqfJz883xlz8LLzzzjtN3759m7ysuf7l0fWVlJSYPn36mF69epm8vDxjjDEXLlww8+fPN5JMqI9432XNr732mqmtrTVer9d88MEHxul0mpdeeink48ycOdNIMqNHjw7Z35jWvqw5osDy3//93+YPf/hDyL5Ro0YZSf4/WlMILADQPrp6YKmurjbLli0z1113nenRo4cZMGCAueOOO8yKFSuMJNOzZ0+TnJwc8Nnxj3/8wzz00EMmNTXVJCcnm0GDBplHH33UHD9+PGj+SMYeOXLETJs2zSQmJhqXy2XuvPNO87e//c0MHDjQ2O12k5ycbJ555hljjDHp6ekmISHBSDIJCQkmOTk55D1Rdu/ebaZMmWL69etnevfubYYOHWqWLFliRo8ebSSZ5ORk8+Mf/9g/3uv1mldffdV86UtfMv369fPfj+att95q9G+4Y8cOI8n85je/iehv39qBxWaMMa2xp+aLX/yiPvnkE+3Zs8d/+VNjiouLdcstt4TcJddSHo9HTqdTbrdbCQkJrTYvAHRkFy5c0MGDB3XNNdeoe/fu0S4HHVBBQYHuuOMOlZSUKDY2Nuztwn3vhfv53Sq35i8tLVVRUZGGDh2q4cOHh7VNRUWF5s6dq2HDhik5OVnp6emaMWOG8vPzw9q+qqpKHo8nYAEAAFempqZG586d86+/+OKLevjhhyMKK22hVQLLypUrVVtbq5UrV4Z9bXZZWZlcLpe2bdumY8eOadOmTSopKdHo0aOVm5vb7PbLly+X0+n0L41dCw8AAML3l7/8RTfddJNqamq0f/9+bdy4UdnZ2dEuS1d8SGjHjh36yle+okWLFmnBggVhbVNXV6eysjL169cvoP3MmTNKT09XdXW1Dh48qOTk5EbnqKqqCriW3OPxKC0tjUNCAFAPh4QQqT179ujOO+/UuXPnlJiYqBUrVuiuu+6KeJ7WPiR0RTeOO3DggKZOnars7Oyww4p08bbBDcOKJF199dUaP368Nm7cqM2bN+uhhx5qdI64uDjFxcW1qG4AABDayJEjVVJSEu0ygrT4kND+/fs1fvx4zZ49W88//3yrFeS7sU/9WwwDAICurUWBZffu3br11lv16KOP6rnnnvO3FxcX69ix5r+XIjc3t9GTa33bt9aXRgEAgI4v4sCSn5+vCRMm6IknntDixYsD+hYvXuz/giTp4i2HQ+1Wys3NDbgDoI/vewpiY2P9354JAAAQ0Tks27dv1+TJk5WSkqKKioqgwLJnzx4NGjTIv56dna1Vq1YpOztbL7zwQsDYV155RTfccINmzpyp2NhY/f3vf9f3vvc9lZeX6xe/+IX/NsYAgCvXSrfcAsLW2u+5iALLc88957/nyU9+8pOQY+6++27/72lpaYqPjw+65Pjll1/WG2+8oTVr1mjRokWqrKxUTEyMRo8erby8vEa/QRMAEBmHwyHp4r01evToEeVq0JX4vkjR9x68Uq12p9to4063ABDaP/7xD8XExKh///5h3ysLuBLGGP+3P6enpzc5tl0uawYAWF+/fv109OhRHTlyRE6nUzExMQQXtAljjGpqauR2u3Xu3Dmlpqa22twEFgDo5Hz/11paWqqjR49GuRp0BXFxcUpNTW3VIx4EFgDoAhISEpSQkKCamhrV1dVFuxx0Yg6HQzExMa0+L4EFALqQmJiYNvkwAdpaq3z5IQAAQFsisAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMsjsAAAAMuLKLBUVVXpzTff1NSpU+VyudS3b18lJiZqypQp+vDDDyN+8Ndff12ZmZlKSkqSy+XSPffco88++yzieQAAQOcWUWB54okndN9992n48OEqKirS6dOnVVBQoOrqak2cOFGrVq0Ke66FCxdq1qxZysrK0smTJ1VYWKja2lplZmZq3759ET8RAADQedmMMSbcwXPmzNG+ffv05z//OaC9tLRUAwYMkNfr1YkTJ9S7d+8m5ykoKFBmZqYeeOABrV271t/udruVlpamwYMHKz8/XzabLewn4vF45HQ65Xa7lZCQEPZ2AAAgesL9/I5oD8ukSZO0YMGCoPZ+/fppyJAhqqqq0q5du5qdZ/Xq1TLGaPr06QHtTqdTkydPVkFBgbZv3x5JaQAAoBPrFsngqVOnNtpXXV0tSerbt2+z8+Tl5UmSMjIygvoyMjL01ltvacuWLRo7dmwk5bWJbUWlOnTmvGLsdnVz2NTNYVc3u03d7DbFOC62OXy/X/p5cd2mbr5tLv30zeGoNw4AADQvosDSmNLSUhUVFWno0KEaPnx4k2MrKip0+PBhxcbGKjExMag/NTVVklRYWNgapV2xtz4u0Xt7j7XJ3DabLoWfS4HGF3bsl4KRw+bvj7kUluqHoZhL4aebw64Yu00O/7hLc/r6AsLT5baLjxU6iF2c93IQu1zjxd8d9QJbjN0uhyNwWwAAWlOrBJaVK1eqtrZWK1eubPa8k/LycklSfHx8yP6ePXtKksrKypqcp6qqSlVVVf51j8cTQcXhG97fqQs1dar1GtXUeVVbZ1TnNarxXvy9ps6rOq8J6K/1GtWG6G/IGKmmzqimrk6qaZPyo6LZIGa/HKoCQldA0AoviNXfu1V/7oZ7s3ztDrtvzsvrl+sJXHfYA4Oeb51ABgDt74oDy44dO/TTn/5US5Ys0YQJE1qjprAsX75cP/nJT9r8cR4el66Hx6Vf8TzGXAoydfXCjLdeAKrz+kPPxXWj2kttF7fzXmzzBvbXeI3q/NvW38YXmIzqvBfH+fsuzVNT/7F9dV3q99UTTn/wc+2cQczHF8jqB5gYR+B6qABUf5tQ6w775W1iGqw3FqoaC1mhampy3b83L3jdYbdFdAI8ALSFKwosBw4c0NSpU5WdnR3yZNxQfFcQVVRUhOw/f/68JKlPnz5NzjN//nzNnTvXv+7xeJSWlhZWDdFgs138gIhxSJIj2uW0mvpBrMbrVV29IFY/5ATsfQoVzryB/b6AFRTO6gWxywHKFwAvhbO6xsNZrW+OS3113sshsP543+OGfs6+QGYkedv3Dx4lDrvvEOLlPWL+9XqHCevvyeoWYv1yUGq43nzYs9suByiH3S6HXYE/bTZ/nd3sNtnt9bZzNNy+3tLYdvbgxyW4AdHT4sCyf/9+3XbbbZo9e7aee+65sLeLj4/XgAEDdPjwYZ06dSroPJajR49KkoYMGdLkPHFxcYqLi4u8cLSq+kGsRycKYj5NBRpfwKofgGrrhaqLhw4D1+vv/bo85+W9VZeDVP09XJGt++a8HNACD02G3ObS8wqxw8z/d6jzGlW375/fcuw2hQw6vuDUzW6X3ffT5lsPDECB2zUITo6LP5sLTo1t13xgazroBWzfRNCzN6ixfi12DpmijbQosOzevVtf/epXlZWVpcWLF/vbi4uLFRsbq5SUlCa3nzBhgnJyclRQUKBJkyYF9BUUFEiSJk6c2JLSgFZ18R/izhfEGuP1GtWZBsGsYVBrNATVW6+3B6x+IAq5Xv+8r3rrvrBYW2+9zns5lHkv7d3zXhrn9epijUaq83pV5/X9vLSduTh3nTGh+y793lhokySvkbx1je99w8VDpo2FoFCHQOuvx9Tby+Yf69+LF7juO7/t8raX17vVf0xH4Hlr3ert3Wtsvdmxjnq12glp7SWiG8dJUn5+vm6//Xb96Ec/0pNPPhnQN2vWLA0aNMgfYowxOnLkSNChmuZuHHfttdfq448/5sZxANqdMY2Foss/QwWdoDBVF7x94Hb1Q9XlANXk4waErhBLI9v56mlqu8afx8U9b/VDYVOhriuy2RQQYEIfBo38/DXfhQQh15voC33YNXQwa+rctsbOvWvtQ6Phfn5HtIdl+/btmjx5slJSUlRRURGwd0WS9uzZo0GDBvnXs7OztWrVKmVnZ+uFF17wt2dkZOipp57S8uXLNX78eM2cOVMej0ezZs2SJL322mscKwYQFTab7yq1aFdiXb5Q12RwaiSw1T9sWltvveF5ZgHr9S5ACGfvXk3A3sHwD482PK+tJsR66L+HVF3nlera+YWIghfuG6m7RqZG5bEjCizPPfecPB6PPB5Po1fo3H333f7f09LSFB8fH/Jk2GeeeUaDBw/WCy+8oCeeeEI2m03jxo1Tfn6+rrvuusieBQCg3fhDXbQLaWfGGP/eptoQhy4bC1zB56k1sR7B2MbORWvsUG7D8+waO/Raf72hbvaIbpDfqiI+JGRVHBICAKD1+AJa/b1S3WPsimvl3Y9tckgIAAB0DTabTQ6bLHPhQfT27QAAAISJwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACyPwAIAACzvigLL/v37NWbMGNlsNhUXF0e0bXFxsRwOh1wuV8hl/fr1V1IaAADoRLq1ZKMLFy5o6dKleumll2S3tzzzpKWlRRx0AABA19OitPGDH/xAn3zyifbu3athw4a1dk0AAAABWrSHZf78+Ro4cGBr1wIAABBSi/awEFYAAEB7iupVQhUVFZo7d66GDRum5ORkpaena8aMGcrPz29226qqKnk8noAFAAB0TlENLGVlZXK5XNq2bZuOHTumTZs2qaSkRKNHj1Zubm6T2y5fvlxOp9O/pKWltU/RAACg3dmMMeZKJrjlllv0pz/9SQcPHtSgQYPC3q6urk5lZWXq169fQPuZM2eUnp6u6upqHTx4UMnJySG3r6qqUlVVlX/d4/EoLS1NbrdbCQkJLXouAACgfXk8HjmdzmY/v6O2h8XhcASFFUm6+uqrNX78eFVWVmrz5s2Nbh8XF6eEhISABQAAdE6WvNNtSkqKJOn48eNRrgQAAFhB1AJLbm5uoyfXHjt2TJKUlJTUniUBAACLavPAYoxRSUlJUHtubq5ycnKC2svLy7V161bFxsZq0qRJbV0eAADoANo8sGRnZ2vAgAH6/ve/H9T3yiuvaM2aNaqurpYk/f3vf9e0adNUXl6uFStWqH///m1dHgAA6ABaFFh27tzp/5LC7du3S5IyMzPlcrk0b968gLFpaWmKj48Puuz45Zdf1sKFC7VmzRpdc801uvrqqzVmzBj16tVLeXl5ys7ObuFTAgAAnc0VX9ZsFeFeFgUAAKzD8pc1AwAAhIvAAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALI/AAgAALO+KAsv+/fs1ZswY2Ww2FRcXt2iO119/XZmZmUpKSpLL5dI999yjzz777ErKAgAAnUyLAsuFCxe0YMEC3XzzzSoqKmrxgy9cuFCzZs1SVlaWTp48qcLCQtXW1iozM1P79u1r8bwAAKBzaVFg+cEPfqBPPvlEe/fu1bBhw1r0wAUFBVq2bJnuv/9+zZo1SzabTb1791Zubq6MMZo9e7aMMS2aGwAAdC4tCizz58/Xpk2b1L9//xY/8OrVq2WM0fTp0wPanU6nJk+erIKCAm3fvr3F8wMAgM6jRYFl4MCBV/zAeXl5kqSMjIygPl/bli1brvhxAABAxxeVq4QqKip0+PBhxcbGKjExMag/NTVVklRYWNjepQEAAAvqFo0HLS8vlyTFx8eH7O/Zs6ckqaysrNE5qqqqVFVV5V/3eDytVyAAALCUDnsfluXLl8vpdPqXtLS0aJcEAADaSFQCS+/evSVdPDQUyvnz5yVJffr0aXSO+fPny+12+5eSkpJWrxMAAFhDVA4JxcfHa8CAATp8+LBOnToVdB7L0aNHJUlDhgxpdI64uDjFxcW1aZ0AAMAaonZIaMKECZIu3o+lIV/bxIkT27UmAABgTW0eWIwxIQ/XZGVlyWazacOGDQHtbrdbv//973XDDTdozJgxbV0eAADoANo8sGRnZ2vAgAH6/ve/H9CekZGhp556Sr/5zW+0du1aGWPkdrs1a9YsSdJrr70mm83W1uUBAIAOoEWBZefOnXK5XHK5XP670WZmZsrlcmnevHkBY9PS0hQfHx/yKp5nnnlGOTk5WrlypZKTkzV48GA5HA7l5+drxIgRLSkNAAB0QjbTSb6wx+PxyOl0yu12KyEhIdrlAACAMIT7+d1h78MCAAC6DgILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwPAILAACwvBYFlsLCQk2bNk0ul0tJSUnKzMzUunXrwt6+uLhYDodDLpcr5LJ+/fqWlAUAADqpbpFusHv3bt18882aMGGCCgsL5XQ6tXbtWs2cOVNFRUVavHhxWPOkpaWpuLg40ocHAABdUER7WIwxeuihhyRJubm56t27t2w2m2bNmqVvf/vbWrp0qfbs2dMWdQIAgC4sosDy0Ucfae/evbrjjjvkdDoD+qZPny6v16vVq1e3aoEAAAARBZa8vDxJUkZGRlCfr23Lli2tUBYAAMBlEQWWTz/9VJKUmpoa1JeYmKiYmBgdOnRIlZWVzc5VUVGhuXPnatiwYUpOTlZ6erpmzJih/Pz8SEoCAABdQESBpby8XJLUs2fPoD6bzab4+PiAcU0pKyuTy+XStm3bdOzYMW3atEklJSUaPXq0cnNzm92+qqpKHo8nYAEAAJ1TVO7DkpaWpuPHj+vJJ59Unz595HA4NHz4cP3ud79Tr1699Nhjj+nkyZNNzrF8+XI5nU7/kpaW1k7VAwCA9hZRYOndu7ck6fz580F9xhhVVFQEjGuMw+FQv379gtqvvvpqjR8/XpWVldq8eXOTc8yfP19ut9u/lJSUhPckAABAhxPRfViuv/56SdLRo0eD+k6dOqWamhoNHDhQPXr0aHFBKSkpkqTjx483OS4uLk5xcXEtfhwAANBxRLSHZcKECZKkgoKCoD5f28SJE5udJzc3t9GTa48dOyZJSkpKiqQ0AADQiUUUWMaNG6cRI0Zo8+bNcrvdAX0bNmyQ3W5XVlaWv80YE/JQTW5urnJycoLay8vLtXXrVsXGxmrSpEmRlAYAADqxiAKLzWZTTk6O/463brdbxhjl5uZq3bp1WrhwoUaOHOkfn52drQEDBuj73/9+0FyvvPKK1qxZo+rqaknS3//+d02bNk3l5eVasWKF+vfvf2XPDAAAdBoRf5fQqFGjlJ+frwULFmjw4MHyer0aOHCg1q5dq/vvvz9gbFpamuLj44Ou4Hn55Zf1xhtvaM2aNVq0aJEqKysVExOj0aNHKy8vT7feeuuVPSsAANCp2IwxJtpFtAaPxyOn0ym3262EhIRolwMAAMIQ7ud3VO7DAgAAEAkCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsDwCCwAAsLxu0S4AAICwGXNp8V5e1GDdeIPHNNrX1LhL/SHnb8mcjbQrknq8klFk9TRafzjtDcbc9P+kQWOj8tITWJpTVyuZussvWsg3VsM3dGuMbWKOVh3b2Js6krEK0R5qrGmivaVjG7Q39h9+fcY0eJFNE/0t7WvQ32qPGcG8Dbss91wieEx0fP5/+8IICE2N4X0RXSPui9pDE1ia8+4j0ifvRrsKAECL2CSbTbLZG1ka9CnU2ObamprfHkYN9edo61qbeQw1U0dqRtReSQJLc2yRnOZT702pRt7MAW/cpsbaQrcHtTVsb/hYkY61NdHe0rHNzdGSsQrjb1N/7KW/p/+lqve777ULWLW1Ql+D/lZ7zIbzqom+tnrMKPz90PGF/YHc3Id7w38DmpqP91BnYTMmaD9sh+TxeOR0OuV2u5WQkNB6E1efl7y1zXwg2vgPAwCAFgj385s9LM2J7RntCgAA6PIiOd4BAAAQFQQWAABgeQQWAABgeQQWAABgeQQWAABgeQQWAABgeQQWAABgeQQWAABgeQQWAABgeQQWAABgeQQWAABgeQQWAABgeQQWAABgeS0KLIWFhZo2bZpcLpeSkpKUmZmpdevWRTzP66+/rszMTCUlJcnlcumee+7RZ5991pKSAABAJxZxYNm9e7e+9KUvqa6uToWFhTp58qSysrI0c+ZMLV68OOx5Fi5cqFmzZikrK0snT55UYWGhamtrlZmZqX379kVaFgAA6MRsxhgT7mBjjEaNGqV//OMfKikpkdPp9PfNnDlT69atU0FBgUaOHNnkPAUFBcrMzNQDDzygtWvX+tvdbrfS0tI0ePBg5efny2azhf1EPB6PnE6n3G63EhISwt4OAABET7if3xHtYfnoo4+0d+9e3XHHHQFhRZKmT58ur9er1atXNzvP6tWrZYzR9OnTA9qdTqcmT56sgoICbd++PZLSAABAJxZRYMnLy5MkZWRkBPX52rZs2dJu8wAAgK6hWySDP/30U0lSampqUF9iYqJiYmJ06NAhVVZWqkePHiHnqKio0OHDhxUbG6vExMSgft/chYWFTdZSVVWlqqoq/7rb7ZZ0cdcSAADoGHyf282doRJRYCkvL5ck9ezZM6jPZrMpPj5ebrdb5eXljQYW3xzx8fEh+31zl5WVNVnL8uXL9ZOf/CSoPS0trcntAACA9Zw9ezbodJP6IgosVjJ//nzNnTvXv+71enXmzBn17ds3opN1m+PxeJSWlqaSkhJO5u2geA07Pl7Djo/XsGNry9fPGKOzZ88qJSWlyXERBZbevXtLks6fPx/yASsqKgLGNTWHb2xDvrn79OnTZC1xcXGKi4sLOXdbSEhI4D+yDo7XsOPjNez4eA07trZ6/Zras+IT0Um3119/vSTp6NGjQX2nTp1STU2NBg4c2OjhIOnioaABAwaourpap06dCur3zT1kyJBISgMAAJ1YRIFlwoQJki7eR6UhX9vEiRPbbR4AANA1RBRYxo0bpxEjRmjz5s3+q3J8NmzYILvdrqysLH+bMUYlJSVB82RlZclms2nDhg0B7W63W7///e91ww03aMyYMZGU1mbi4uK0aNGioMNP6Dh4DTs+XsOOj9ewY7PE62citGvXLtOrVy/z9a9/3ZSXlxuv12tycnKM3W43Tz/9dMDYOXPmGEkmOzs7aJ4FCxYYu91ucnNzjdfrNeXl5ebuu+82V111ldmzZ0+kZQEAgE4s4u8SGjVqlP+2+YMHD1ZSUpJWrVqltWvXBl1mnJaWpvj4+JCXGj/zzDPKycnRypUrlZycrMGDB8vhcCg/P18jRoxocQADAACdT0TfJQQAABANEe9hAQAAaG8ElkYUFhZq2rRpcrlcSkpKUmZmptatWxftshCh/fv3a8yYMbLZbCouLo52OQhTVVWV3nzzTU2dOlUul0t9+/ZVYmKipkyZog8//DDa5SEMtbW1euutt/TQQw9p2LBhSklJUWJiokaOHKlnn31W586di3aJaIE5c+bIZrNp1qxZ7f7YBJYQdu/erS996Uuqq6tTYWGhTp48qaysLM2cOVOLFy+OdnkIw4ULF7RgwQLdfPPNKioqinY5iNATTzyh++67T8OHD1dRUZFOnz6tgoICVVdXa+LEiVq1alW0S0QzSktLde+992rv3r168803dezYMR0/flzZ2dl66qmndNttt6m2tjbaZSICH374of7rv/4regVE+6xfq/F6vWbEiBHmqquuMuXl5QF9DzzwgLHb7Wb37t3RKQ5h+973vmfuuusuU1JSYm6++WYjyRw8eDDaZSFMWVlZZty4cUHtp06dMj169DBxcXGmrKys/QtD2I4fP24kmV27dgX1TZs2zUgyH374YRQqQ0uUlZWZ/v37mwceeMBIMg8++GC718AelgY++ugj7d27V3fccUfQrYKnT58ur9er1atXR6k6hGv+/PnatGmT+vfvH+1S0AKTJk3SggULgtr79eunIUOGqKqqSrt27YpCZQhX3759tW3bNo0cOTKob+DAgZIUdD8vWNecOXM0cuRIzZ49O2o1dNgvP2wreXl5kqSMjIygPl/bli1b2rUmRM73DyI6pqlTpzbaV11dLeniByKsKyYmRmPHjg3Zt3PnTnXv3l033nhjO1eFlvjtb3+rDz74QP/3f/+nwsLCqNXBHpYGPv30U0lSampqUF9iYqJiYmJ06NAhVVZWtndpQJdXWlqqoqIiDR06VMOHD492OYiA1+vVwYMHlZWVpV27diknJyfkv7OwlpMnT+rRRx/Viy++KJfLFdVaCCwNlJeXS5J69uwZ1Gez2RQfHx8wDkD7WblypWpra7Vy5UrZbLZol4Mwvf/+++rdu7fS09P1hz/8QevXr9d9990X7bIQhocfflhf/epXNW3atGiXwiEhAB3Djh079NOf/lRLlizxf4EqOoapU6fK4/Ho1KlTeuONNzR9+nSNHz9eGzZsUK9evaJdHhrx6quvateuXdq/f3+0S5HEHpYgvXv3liSdP38+qM8Yo4qKioBxANregQMHNHXqVGVnZ4c8GRcdQ2Jiov793/9dS5cu1fvvv6/58+dHuyQ0ori4WHPnztWrr76qPn36RLscSQSWINdff70k6ejRo0F9p06dUk1NjQYOHKgePXq0d2lAl7R//36NHz9es2fP1vPPPx/tctAKfCdVb9q0KbqFoFGbN2+W1+vVrFmz5HK5/Ms3vvENSdKbb77pb/vd737XLjURWBrw7WouKCgI6vO1TZw4sV1rArqq3bt369Zbb9Wjjz6q5557zt9eXFysY8eORbEyNGfr1q16++23Q/b5zgU8ffp0e5aECDz22GM6e/asTpw4EbC8++67kqR7773X33bXXXe1S00ElgbGjRunESNGaPPmzUH3CNiwYYPsdruysrKiVB3QdeTn52vChAl64okngu4wvXjxYv3qV7+KTmEIy9atW7Vo0SJ5vd6gPt+tIbisGZHgpNsGbDabcnJy9JWvfEUPPfSQcnJylJCQoLVr12rdunVauHBhyBshAWg927dv1+TJk5WSkqKKioqgwLJnzx4NGjQoKrUhfJ9++qkeeeQRLV26VCkpKaqurtZ7772nefPmKSEhgUN8iIjNGGOiXYQVFRYWasGCBdq2bZu8Xq8GDhyoH/zgB7r//vujXRrCsHPnTn3ta1+TJJ05c0Y1NTXq16+fHA6Hvv3tb+tnP/tZlCtEU+6+++5mj4svWrSI7/aysNLSUr399tvauHGjioqKdP78eVVWVio1NVW33XabHn/8cUJnB/If//Ef+uUvf6nq6mqVlZWpe/fucjqdSkpK0r59+9qlBgILAACwPM5hAQAAlkdgAQAAlkdgAQAAlkdgAQAAlkdgAQAAlkdgAQAAlkdgAQAAlkdgAQAAlkdgAQAAlkdgAQAAlkdgAQAAlkdgAQAAlkdgAQAAlvf/AcXLMLOc056yAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72.14206\n",
            "144.48607\n",
            "215.78136\n",
            "287.95898\n",
            "361.01572\n",
            "432.9206\n",
            "505.2682\n",
            "72.14206\n",
            "144.48607\n",
            "215.78136\n",
            "287.95898\n",
            "361.01572\n",
            "432.9206\n",
            "505.2682\n",
            "72.14206\n",
            "144.48607\n",
            "215.78136\n",
            "287.95898\n",
            "361.01572\n",
            "432.9206\n",
            "505.2682\n",
            "72.14206\n",
            "144.48607\n",
            "215.78136\n",
            "287.95898\n",
            "361.01572\n",
            "432.9206\n",
            "505.2682\n",
            "72.14206\n",
            "144.48607\n",
            "215.78136\n",
            "287.95898\n",
            "361.01572\n",
            "432.9206\n",
            "505.2682\n",
            "Sum of squared in iteration 0  is  505.2682\n",
            "Sum of squared in iteration 1  is  505.2682\n",
            "Sum of squared in iteration 2  is  505.2682\n",
            "Sum of squared in iteration 3  is  505.2682\n",
            "Sum of squared in iteration 4  is  505.2682\n",
            "Deep unfolding Layer 1 done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the data rows\n",
        "for i in range(T):\n",
        "    for j in range(n_node-1):\n",
        "        print([i, j, normalized_weights_Per_it[i][j]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsdPQDRNZ7e4",
        "outputId": "98eb21a5-5146-429f-cee7-2d8ea64d0e87"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, Array(0.14387189, dtype=float32)]\n",
            "[0, 1, Array(0.1430365, dtype=float32)]\n",
            "[0, 2, Array(0.14209446, dtype=float32)]\n",
            "[0, 3, Array(0.14292443, dtype=float32)]\n",
            "[0, 4, Array(0.14296223, dtype=float32)]\n",
            "[0, 5, Array(0.14301957, dtype=float32)]\n",
            "[0, 6, Array(0.14209089, dtype=float32)]\n",
            "[1, 0, Array(0.14387189, dtype=float32)]\n",
            "[1, 1, Array(0.1430365, dtype=float32)]\n",
            "[1, 2, Array(0.14209446, dtype=float32)]\n",
            "[1, 3, Array(0.14292443, dtype=float32)]\n",
            "[1, 4, Array(0.14296223, dtype=float32)]\n",
            "[1, 5, Array(0.14301957, dtype=float32)]\n",
            "[1, 6, Array(0.14209089, dtype=float32)]\n",
            "[2, 0, Array(0.14387189, dtype=float32)]\n",
            "[2, 1, Array(0.1430365, dtype=float32)]\n",
            "[2, 2, Array(0.14209446, dtype=float32)]\n",
            "[2, 3, Array(0.14292443, dtype=float32)]\n",
            "[2, 4, Array(0.14296223, dtype=float32)]\n",
            "[2, 5, Array(0.14301957, dtype=float32)]\n",
            "[2, 6, Array(0.14209089, dtype=float32)]\n",
            "[3, 0, Array(0.14387189, dtype=float32)]\n",
            "[3, 1, Array(0.1430365, dtype=float32)]\n",
            "[3, 2, Array(0.14209446, dtype=float32)]\n",
            "[3, 3, Array(0.14292443, dtype=float32)]\n",
            "[3, 4, Array(0.14296223, dtype=float32)]\n",
            "[3, 5, Array(0.14301957, dtype=float32)]\n",
            "[3, 6, Array(0.14209089, dtype=float32)]\n",
            "[4, 0, Array(0.14387189, dtype=float32)]\n",
            "[4, 1, Array(0.1430365, dtype=float32)]\n",
            "[4, 2, Array(0.14209446, dtype=float32)]\n",
            "[4, 3, Array(0.14292443, dtype=float32)]\n",
            "[4, 4, Array(0.14296223, dtype=float32)]\n",
            "[4, 5, Array(0.14301957, dtype=float32)]\n",
            "[4, 6, Array(0.14209089, dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "   for t in tqdm(range(T)):\n",
        "        weights_dict={}\n",
        "        with open('weights.csv', 'r') as csvfile:\n",
        "          reader = csv.reader(csvfile)\n",
        "          # Read the header row\n",
        "          header = next(reader)\n",
        "          # Read the data rows for the desired iteration\n",
        "          for row in reader:\n",
        "            iteration, node, weight = map(float, row)  # Convert to int assuming iteration and node are integers\n",
        "            if iteration == t:\n",
        "                  weights_dict[node] = weight\n",
        "          # Now you have a dictionary with node as keys and weights as values for the specified iteration\n",
        "        print(\"Weights for iteration\", t, \":\", weights_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ6DhEhRK6N4",
        "outputId": "9833d570-11d3-46f8-baee-0324ca92f5a5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 1131.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights for iteration 0 : {0.0: 0.14247216, 1.0: 0.14304182, 2.0: 0.14500764, 3.0: 0.14281717, 4.0: 0.14207053, 5.0: 0.1438871, 6.0: 0.14070356}\n",
            "Weights for iteration 1 : {0.0: 0.14247216, 1.0: 0.14304182, 2.0: 0.14500764, 3.0: 0.14281717, 4.0: 0.14207053, 5.0: 0.1438871, 6.0: 0.14070356}\n",
            "Weights for iteration 2 : {0.0: 0.14247216, 1.0: 0.14304182, 2.0: 0.14500764, 3.0: 0.14281717, 4.0: 0.14207053, 5.0: 0.1438871, 6.0: 0.14070356}\n",
            "Weights for iteration 3 : {0.0: 0.14247216, 1.0: 0.14304182, 2.0: 0.14500764, 3.0: 0.14281717, 4.0: 0.14207053, 5.0: 0.1438871, 6.0: 0.14070356}\n",
            "Weights for iteration 4 : {0.0: 0.14247216, 1.0: 0.14304182, 2.0: 0.14500764, 3.0: 0.14281717, 4.0: 0.14207053, 5.0: 0.1438871, 6.0: 0.14070356}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "#def fedlearning(weights,method,params):\n",
        "    # numpy data\n",
        "    if dataset == 'mnist':\n",
        "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    elif dataset == 'fashion':\n",
        "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    ind = y_test == 9\n",
        "    x_test, y_test = x_test[~ind], y_test[~ind]\n",
        "    ind = y_test == 8\n",
        "    x_test, y_test = x_test[~ind], y_test[~ind]\n",
        "    ind = y_train == 9\n",
        "    x_train, y_train = x_train[~ind], y_train[~ind]\n",
        "    ind = y_train == 8\n",
        "    x_train, y_train = x_train[~ind], y_train[~ind]\n",
        "\n",
        "    x_train = x_train / 255.0\n",
        "    if encoding_mode == 'vanilla':\n",
        "        mean = 0\n",
        "    elif encoding_mode == 'mean':\n",
        "        mean = jnp.mean(x_train, axis=0)\n",
        "    elif encoding_mode == 'half':\n",
        "        mean = 0.5\n",
        "    x_train = x_train - mean\n",
        "    x_train = tf.image.resize(x_train[..., tf.newaxis], (int(2**(n/2)), int(2**(n/2)))).numpy()[..., 0].reshape(-1, 2**n)\n",
        "    x_train = x_train / jnp.sqrt(jnp.sum(x_train**2, axis=-1, keepdims=True))\n",
        "\n",
        "    x_test = x_test / 255.0\n",
        "    x_test = x_test - mean\n",
        "    x_test = tf.image.resize(x_test[..., tf.newaxis], (int(2**(n/2)), int(2**(n/2)))).numpy()[..., 0].reshape(-1, 2**n)\n",
        "    x_test = x_test / jnp.sqrt(jnp.sum(x_test**2, axis=-1, keepdims=True))\n",
        "    y_test = jax.nn.one_hot(y_test, n_node)\n",
        "\n",
        "    world_train_loss = []\n",
        "    world_test_loss = []\n",
        "    world_train_acc = []\n",
        "    world_test_acc = []\n",
        "    paramslist_all = []\n",
        "    xlist_all = []\n",
        "    ylist_all = []\n",
        "    opt_statenodeall=[]\n",
        "\n",
        "    params_list = []\n",
        "    opt_state_list = []\n",
        "    data_list = []\n",
        "    iter_list = []\n",
        "    params_lists=[]\n",
        "    xnodelists=[]\n",
        "    ynodelists=[]\n",
        "    for node in range(n_node-1):\n",
        "        x_train_node, y_train_node = filter(x_train, y_train, [0, node+1])\n",
        "        # x_train_node, y_train_node = x_train, jax.nn.one_hot(y_train, n_node)\n",
        "        data = tf.data.Dataset.from_tensor_slices((x_train_node, y_train_node)).batch(128)\n",
        "        data_list.append(data)\n",
        "        iter_list.append(iter(data))\n",
        "\n",
        "        key, subkey = jax.random.split(key)\n",
        "        params = jax.random.normal(subkey, (3 * k, n))\n",
        "            #params=params\n",
        "        opt = optax.adam(learning_rate=1e-2)\n",
        "        opt_state = opt.init(params)\n",
        "        params_list.append(params)\n",
        "        opt_state_list.append(opt_state)\n",
        "\n",
        "        #weights=[0.14114611,0.14295208,14467925,14332035,14269364,14413896,14106953]\n",
        "\n",
        "    for t in tqdm(range(T)):\n",
        "        weights_dict={}\n",
        "        with open('weights.csv', 'r') as csvfile:\n",
        "          reader = csv.reader(csvfile)\n",
        "          # Read the header row\n",
        "          header = next(reader)\n",
        "          # Read the data rows for the desired iteration\n",
        "          for row in reader:\n",
        "            iteration, node, weight = map(float, row)  # Convert to int assuming iteration and node are integers\n",
        "            if iteration == t:\n",
        "                  weights_dict[node] = weight\n",
        "          # Now you have a dictionary with node as keys and weights as values for the specified iteration\n",
        "        print(\"Weights for iteration\", t, \":\", weights_dict)\n",
        "\n",
        "\n",
        "        loss_list = []\n",
        "        acc_list = []\n",
        "        params_listnode=[]\n",
        "        xnode=[]\n",
        "        ynode=[]\n",
        "        opt_statenode=[]\n",
        "        weighted_paramlist=[]\n",
        "\n",
        "        for node in range(n_node-1):\n",
        "            #for b in range(100):\n",
        "            #print(\"node\", node)\n",
        "\n",
        "            try:\n",
        "              x, y = next(iter_list[node])\n",
        "            except StopIteration:\n",
        "              iter_list[node] = iter(data_list[node])\n",
        "              x, y = next(iter_list[node])\n",
        "            x = x.numpy()\n",
        "            y = y.numpy()\n",
        "            loss_val, grad_val = compute_loss(params_list[node], x, y, k)\n",
        "            #print(\"loss lenth\",node, \"is\", len(loss_val))\n",
        "            #print(\"grad lenth\",node, \"is\", len(grad_val))\n",
        "            #print(\"para length\",node, \"is\", len(params_list[node]))\n",
        "            updates, opt_state_list[node] = opt.update(grad_val, opt_state_list[node], params_list[node])\n",
        "            params_list[node] = optax.apply_updates(params_list[node], updates)\n",
        "                #print(loss_val)\n",
        "            #if method==\"dunQFL\":\n",
        "            #params_list[node] = params_list[node]*normalized_weights_Per_it[t][node]\n",
        "            #if method==\"QFL\":\n",
        "              #params_list[node]=params_list[node]\n",
        "\n",
        "\n",
        "            #print(weighted_paramlist[node])\n",
        "            params_listnode.append(params_list[node])\n",
        "            opt_statenode.append(opt_state_list[node])\n",
        "            xnode.append(x)\n",
        "            ynode.append(y)\n",
        "            print(weights_dict[node])\n",
        "            params_list[node]=params_list[node] * weights_dict[node]\n",
        "\n",
        "        avg_params = jnp.mean(jnp.stack(params_list, axis=0), axis=0)\n",
        "\n",
        "\n",
        "        for node in range(n_node-1):\n",
        "            params_list[node] = avg_params\n",
        "\n",
        "                #if b % 25 == 0:\n",
        "        avg_loss = jnp.mean(compute_loss(avg_params, x_test[:1024], y_test[:1024], k)[0])\n",
        "        loss_list.append(avg_loss)\n",
        "        acc_list.append(compute_accuracy(avg_params, x_test[:1024], y_test[:1024], k).mean())\n",
        "        tqdm.write(f\"Iteration {t}, loss {avg_loss}, accuracy {acc_list[-1]}\")\n",
        "\n",
        "        paramslist_all.append(params_listnode)\n",
        "        xlist_all.append(xnode)\n",
        "        ylist_all.append(ynode)\n",
        "        opt_statenodeall.append(opt_statenode)\n",
        "        test_acc = jnp.mean(pred(avg_params, x_test[:1024], k).argmax(axis=-1) == y_test[:1024].argmax(axis=-1))\n",
        "        test_loss = -jnp.mean(jnp.log(pred(avg_params, x_test[:1024], k)) * y_test[:1024])\n",
        "\n",
        "        world_train_loss.append(loss_list)\n",
        "        world_test_loss.append(test_loss)\n",
        "        world_train_acc.append(acc_list)\n",
        "        world_test_acc.append(test_acc)\n",
        "        #tqdm.write(f\"world {world}: test loss {test_loss}, test accuracy {test_acc}\")\n",
        "        #print(len(paramslist_all[0]))\n",
        "\n",
        "    #return paramslist_all, xlist_all, ylist_all,opt_statenodeall\n",
        "\n",
        "    #avg_test_loss = jnp.mean(jnp.array(world_test_loss), axis=0)\n",
        "    #avg_test_acc = jnp.mean(jnp.array(world_test_acc), axis=0)\n",
        "    #std_test_loss = jnp.std(jnp.array(world_test_loss), axis=0)\n",
        "    #std_test_acc = jnp.std(jnp.array(world_test_acc), axis=0)\n",
        "    #print(f'test loss: {avg_test_loss}+-{std_test_loss}, test acc: {avg_test_acc}+-{std_test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "id": "ec2o_RcDKhZQ",
        "outputId": "ff4ab085-b692-4b47-a25c-ea9ee4b0a54e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights for iteration 0 : {0.0: 0.14247216, 1.0: 0.14304182, 2.0: 0.14500764, 3.0: 0.14281717, 4.0: 0.14207053, 5.0: 0.1438871, 6.0: 0.14070356}\n",
            "0.14247216\n",
            "0.14304182\n",
            "0.14500764\n",
            "0.14281717\n",
            "0.14207053\n",
            "0.1438871\n",
            "0.14070356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:03<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, loss 3.1473207473754883, accuracy 0.134765625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 1/5 [00:09<00:37,  9.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights for iteration 1 : {0.0: 0.14247216, 1.0: 0.14304182, 2.0: 0.14500764, 3.0: 0.14281717, 4.0: 0.14207053, 5.0: 0.1438871, 6.0: 0.14070356}\n",
            "0.14247216\n",
            "0.14304182\n",
            "0.14500764\n",
            "0.14281717\n",
            "0.14207053\n",
            "0.1438871\n",
            "0.14070356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [00:12<00:37,  9.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss 3.28305721282959, accuracy 0.1298828125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:16<00:24,  8.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights for iteration 2 : {0.0: 0.14247216, 1.0: 0.14304182, 2.0: 0.14500764, 3.0: 0.14281717, 4.0: 0.14207053, 5.0: 0.1438871, 6.0: 0.14070356}\n",
            "0.14247216\n",
            "0.14304182\n",
            "0.14500764\n",
            "0.14281717\n",
            "0.14207053\n",
            "0.1438871\n",
            "0.14070356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 2/5 [00:21<00:24,  8.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss 3.2869949340820312, accuracy 0.130859375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:23<00:35, 11.67s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-4e922620b9da>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mopt_statenodeall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_statenode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mworld_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorcircuit/backends/jax_backend.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kws)\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0min_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectorized_argnums\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlibjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mvmap_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1256\u001b[0m     axis_size_ = (axis_size if axis_size is not None else\n\u001b[1;32m   1257\u001b[0m                   _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, \"vmap\"))\n\u001b[0;32m-> 1258\u001b[0;31m     out_flat = batching.batch(\n\u001b[0m\u001b[1;32m   1259\u001b[0m         \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_size_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_axes_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mflatten_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vmap out_axes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-4f9c7c198c3c>\u001b[0m in \u001b[0;36mpred\u001b[0;34m(params, x, k)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCircuit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-4f9c7c198c3c>\u001b[0m in \u001b[0;36mclf\u001b[0;34m(params, c, k)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/array.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    334\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mlax_numpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rewriting_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mlax_numpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rewriting_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   4477\u001b[0m   \u001b[0;31m# simplest cases: i.e. non-dynamic arrays indexed with integers and slices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4479\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0m_attempt_rewriting_take_via_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4480\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_attempt_rewriting_take_via_slice\u001b[0;34m(arr, idx, mode)\u001b[0m\n\u001b[1;32m   4454\u001b[0m     \u001b[0mint_start_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstart_indices\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4455\u001b[0m     \u001b[0mint_limit_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_start_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4456\u001b[0;31m     arr = lax.slice(\n\u001b[0m\u001b[1;32m   4457\u001b[0m         arr, start_indices=int_start_indices, limit_indices=int_limit_indices)\n\u001b[1;32m   4458\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lax/slicing.py\u001b[0m in \u001b[0;36mslice\u001b[0;34m(operand, start_indices, limit_indices, strides)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;34m-\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_slice\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \"\"\"\n\u001b[0;32m--> 108\u001b[0;31m   return slice_p.bind(operand, start_indices=tuple(start_indices),\n\u001b[0m\u001b[1;32m    109\u001b[0m                       \u001b[0mlimit_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                       strides=None if strides is None else tuple(strides))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    442\u001b[0m     assert (not config.enable_checks.value or\n\u001b[1;32m    443\u001b[0m             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args\n\u001b[0;32m--> 444\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lax/slicing.py\u001b[0m in \u001b[0;36m_slice_impl\u001b[0;34m(x, start_indices, limit_indices, strides)\u001b[0m\n\u001b[1;32m   1167\u001b[0m       limit_indices=limit_indices, strides=strides)\n\u001b[1;32m   1168\u001b[0m   \u001b[0mslice_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m   return dispatch.apply_primitive(dynamic_slice_p, x, *start_indices,\n\u001b[0m\u001b[1;32m   1170\u001b[0m                                   slice_sizes=slice_sizes)\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_jit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswap_thread_local_state_disable_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m       \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_jit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswap_thread_local_state_disable_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  plt.plot(loss_list)\n",
        "  plt.plot(acc_list)\n",
        "  plt.legend(['loss', 'accuracy'])\n",
        "  plt.ylim(0, 4)\n",
        "  plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "2XlfNkhALWTB",
        "outputId": "363c7be0-85e0-4a16-d655-1cf9a7279ec7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import csv\n",
        "\n",
        "# Write the world_train_loss list to a CSV file\n",
        "with open(f'QFLworld_train_loss.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow(['Iteration', 'Node', 'Loss'])\n",
        "\n",
        "    # Write the data rows\n",
        "    for i in range(len(world_train_loss)):\n",
        "        for j in range(len(world_train_loss[i])):\n",
        "            writer.writerow([i, j, world_train_loss[i][j]])\n",
        "\n",
        "# Write the world_test_loss list to a CSV file\n",
        "with open(f'QFLworld_test_loss.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow(['Iteration', 'Loss'])\n",
        "\n",
        "    # Write the data rows\n",
        "    for i in range(len(world_test_loss)):\n",
        "        writer.writerow([i, world_test_loss[i]])\n",
        "\n",
        "# Write the world_train_acc list to a CSV file\n",
        "with open(f'QFLworld_train_acc.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow(['Iteration', 'Node', 'Accuracy'])\n",
        "\n",
        "    # Write the data rows\n",
        "    for i in range(len(world_train_acc)):\n",
        "        for j in range(len(world_train_acc[i])):\n",
        "            writer.writerow([i, j, world_train_acc[i][j]])\n",
        "\n",
        "# Write the world_test_acc list to a CSV file\n",
        "with open(f'QFLworld_test_acc.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow(['Iteration', 'Accuracy'])\n",
        "\n",
        "    # Write the data rows\n",
        "    for i in range(len(world_test_acc)):\n",
        "        writer.writerow([i, world_test_acc[i]])"
      ],
      "metadata": {
        "id": "iX8HwT_rnthw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2jBUR-ILIpyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Deep unfolding process loss calcualtion#\n",
        "def dunloss(paramslist_all, xlist_all, ylist_all, k): #pass karanawa iteration one and two node wala params and x,y\n",
        "#e pass karan param tika man ganna one fed process eke yaddi save karagathha ewa.\n",
        "#me process ekedi api eka eka iteration eke nemei all iteration waladi loss balala params set eka update keranwa.\n",
        "\n",
        "    # Iterate through the values row-wise for the specified column\n",
        "    for node in range(n_node-1):\n",
        "      for t in range (T):\n",
        "        value = paramslist_all[t][node]\n",
        "        #print(f\"Value for Column {0} Row {row + 1}: {value}\")\n",
        "        c = tc.Circuit(n, inputs=xlist_all[t][node])\n",
        "        probs = readout(c)\n",
        "        #print(f\"Value for Column {0} Row {row + 1}: {probs}\")\n",
        "        return -jnp.mean(jnp.sum(ylist_all[t][node] * jnp.log(probs + 1e-7), axis=-1))\n",
        "\n",
        "    #for row in range(3):\n",
        "      #for t in range(2):\n",
        "        #\n",
        "        #c = clf(paramslist_all[n][t], c, k)\n",
        "        #probs = readout(c)\n",
        "        #return -jnp.mean(jnp.sum(ylist_all[n][t] * jnp.log(probs + 1e-7), axis=-1))\n",
        "dunloss = K.jit(dunloss, static_argnums=[3])\n",
        "\n",
        "compute_dunloss = K.jit(K.vectorized_value_and_grad(dunloss, vectorized_argnums=[1, 2]), static_argnums=[3])"
      ],
      "metadata": {
        "id": "MDAqR3z3Hz4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hkcFMzsYWRZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "19J58OX_WPpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M=10\n",
        "#key, subkey = jax.random.split(key)\n",
        "#params = jax.random.normal(subkey, (3 * k, n))\n",
        "#opt = optax.adam(learning_rate=1e-2)\n",
        "#opt_state = opt.init(params)\n",
        "for loop in range (M):\n",
        "    unfoldlayer_wise_weights=[]\n",
        "    #paramslist_all, xlist_all, ylist_all,opt_statenodeall = fedlearning(params) #function call for fed process.\n",
        "    loss_val, grad_val = compute_dunloss(paramslist_all, xlist_all, ylist_all, k)\n",
        "    print(\"Unfolding layer\", loop)\n",
        "\n",
        "    for t in range (T):\n",
        "      for node in range(n_node-1):\n",
        "          updates,opt_statenodeall[t][node]= opt.update(grad_val[t][node], opt_statenodeall[t][node], paramslist_all[t][node])\n",
        "          paramslist_all[t][node] = optax.apply_updates(paramslist_all[t][node], updates)\n",
        "          #print(paramslist_all[t][node])\n",
        "          #Assign last parameter set aggregation in last iteration in first deep layer\n",
        "    #at the end of the last iteration of each deep unfolding layer,  need to get average parameters and assign it to each nodes parameters.\n",
        "    if t==T:\n",
        "      avg_params = jnp.mean(jnp.stack(paramslist_all[T-1], axis=0), axis=0)\n",
        "      #params=avg_params\n",
        "      for node in range(n_node-1):\n",
        "            params_list[node] = avg_params\n",
        "\n",
        "\n",
        "    #Calculate the weights using parameters now.\n",
        "    # Calculate sum of squares for each node and get normailzed values\n",
        "    sum_of_squares_per_iter = []\n",
        "    for t in range(T):\n",
        "        sum_of_squares_for_t =0\n",
        "        for node in range(n_node-1):\n",
        "            sum_of_squares_for_t+=jnp.sum(paramslist_all[t][node]**2)\n",
        "            #print(sum_of_squares_for_t)\n",
        "        sum_of_squares_per_iter.append(sum_of_squares_for_t)\n",
        "        #print(\"Sum of squared in iteration\",t,\" is \", sum_of_squares_per_iter[t])\n",
        "\n",
        "    normalized_weights_Per_it = []\n",
        "    for t in range(T):\n",
        "        normalized_weights_node=[]\n",
        "        for node in range(n_node-1):\n",
        "           #print(\"Iteration\", t)\n",
        "           nodeweights=jnp.sum((paramslist_all[t][node])**2)/jnp.array(sum_of_squares_per_iter[t])\n",
        "           #print(f\"Iteration {t}, Node {node}\", nodeweights )\n",
        "           #node_sum_of_squares[t][node] = (jnp.sum(paramslist_all[t][node]**2))/jnp.array(sum_of_squares_per_iter[t])\n",
        "           #print(len(node_sum_of_squares[]))\n",
        "\n",
        "           normalized_weights_node.append(nodeweights)\n",
        "        normalized_weights_Per_it.append(normalized_weights_node)\n",
        "    #print(normalized_weights_Per_it)\n",
        "\n",
        "    import csv\n",
        "\n",
        "    with open('weights.csv', 'w', newline='') as csvfile:\n",
        "      writer = csv.writer(csvfile)\n",
        "\n",
        "      # Write the header row\n",
        "      writer.writerow(['Iteration', 'Node', 'Weight'])\n",
        "\n",
        "      # Write the data rows\n",
        "      for i in range(T):\n",
        "        for j in range(n_node-1):\n",
        "            writer.writerow([i, j, normalized_weights_Per_it[i][j]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    with open('weights.csv', 'r') as csvfile:\n",
        "      reader = csv.reader(csvfile)\n",
        "\n",
        "      # Read the header row\n",
        "      header = next(reader)\n",
        "\n",
        "      # Read the data rows\n",
        "      for row in reader:\n",
        "        iteration, node, weight = row\n",
        "        print(f'Iteration {iteration}, Node {node}, Weight {weight}')\n",
        "\n",
        "print(\"Deep unfolding done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvcNiHhwHkD2",
        "outputId": "d0de63b6-8cf0-42ad-e512-19b77d53062d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unfolding layer 0\n",
            "Iteration 0, Node 0, Weight 0.5580365\n",
            "Iteration 0, Node 1, Weight 0.07364059\n",
            "Iteration 0, Node 2, Weight 0.07351377\n",
            "Iteration 0, Node 3, Weight 0.073771246\n",
            "Iteration 0, Node 4, Weight 0.073975004\n",
            "Iteration 0, Node 5, Weight 0.07354213\n",
            "Iteration 0, Node 6, Weight 0.07352078\n",
            "Iteration 1, Node 0, Weight 0.5376451\n",
            "Iteration 1, Node 1, Weight 0.07721078\n",
            "Iteration 1, Node 2, Weight 0.07694454\n",
            "Iteration 1, Node 3, Weight 0.076960795\n",
            "Iteration 1, Node 4, Weight 0.07708155\n",
            "Iteration 1, Node 5, Weight 0.07689273\n",
            "Iteration 1, Node 6, Weight 0.07726447\n",
            "Iteration 2, Node 0, Weight 0.54938555\n",
            "Iteration 2, Node 1, Weight 0.07487465\n",
            "Iteration 2, Node 2, Weight 0.07486672\n",
            "Iteration 2, Node 3, Weight 0.07532531\n",
            "Iteration 2, Node 4, Weight 0.075077884\n",
            "Iteration 2, Node 5, Weight 0.075287275\n",
            "Iteration 2, Node 6, Weight 0.07518258\n",
            "Iteration 3, Node 0, Weight 0.5663277\n",
            "Iteration 3, Node 1, Weight 0.07204376\n",
            "Iteration 3, Node 2, Weight 0.07226076\n",
            "Iteration 3, Node 3, Weight 0.07236921\n",
            "Iteration 3, Node 4, Weight 0.07226167\n",
            "Iteration 3, Node 5, Weight 0.07244281\n",
            "Iteration 3, Node 6, Weight 0.07229413\n",
            "Iteration 4, Node 0, Weight 0.53289\n",
            "Iteration 4, Node 1, Weight 0.077863336\n",
            "Iteration 4, Node 2, Weight 0.07786236\n",
            "Iteration 4, Node 3, Weight 0.07779923\n",
            "Iteration 4, Node 4, Weight 0.077877864\n",
            "Iteration 4, Node 5, Weight 0.077872284\n",
            "Iteration 4, Node 6, Weight 0.07783484\n",
            "Iteration 5, Node 0, Weight 0.5247341\n",
            "Iteration 5, Node 1, Weight 0.079138875\n",
            "Iteration 5, Node 2, Weight 0.07910268\n",
            "Iteration 5, Node 3, Weight 0.079342164\n",
            "Iteration 5, Node 4, Weight 0.07923033\n",
            "Iteration 5, Node 5, Weight 0.07912119\n",
            "Iteration 5, Node 6, Weight 0.07933075\n",
            "Iteration 6, Node 0, Weight 0.5272258\n",
            "Iteration 6, Node 1, Weight 0.07872526\n",
            "Iteration 6, Node 2, Weight 0.078759715\n",
            "Iteration 6, Node 3, Weight 0.07870284\n",
            "Iteration 6, Node 4, Weight 0.078888096\n",
            "Iteration 6, Node 5, Weight 0.07879514\n",
            "Iteration 6, Node 6, Weight 0.078903176\n",
            "Iteration 7, Node 0, Weight 0.5424667\n",
            "Iteration 7, Node 1, Weight 0.076128826\n",
            "Iteration 7, Node 2, Weight 0.07627546\n",
            "Iteration 7, Node 3, Weight 0.07637305\n",
            "Iteration 7, Node 4, Weight 0.07637809\n",
            "Iteration 7, Node 5, Weight 0.07631383\n",
            "Iteration 7, Node 6, Weight 0.07606393\n",
            "Iteration 8, Node 0, Weight 0.5303833\n",
            "Iteration 8, Node 1, Weight 0.078066915\n",
            "Iteration 8, Node 2, Weight 0.07822181\n",
            "Iteration 8, Node 3, Weight 0.078313746\n",
            "Iteration 8, Node 4, Weight 0.07832176\n",
            "Iteration 8, Node 5, Weight 0.07847789\n",
            "Iteration 8, Node 6, Weight 0.07821457\n",
            "Iteration 9, Node 0, Weight 0.5349624\n",
            "Iteration 9, Node 1, Weight 0.07760456\n",
            "Iteration 9, Node 2, Weight 0.077388\n",
            "Iteration 9, Node 3, Weight 0.077549614\n",
            "Iteration 9, Node 4, Weight 0.07746466\n",
            "Iteration 9, Node 5, Weight 0.07724805\n",
            "Iteration 9, Node 6, Weight 0.07778269\n",
            "Unfolding layer 1\n",
            "Iteration 0, Node 0, Weight 0.5577578\n",
            "Iteration 0, Node 1, Weight 0.07368218\n",
            "Iteration 0, Node 2, Weight 0.073516466\n",
            "Iteration 0, Node 3, Weight 0.07385717\n",
            "Iteration 0, Node 4, Weight 0.0741204\n",
            "Iteration 0, Node 5, Weight 0.07354513\n",
            "Iteration 0, Node 6, Weight 0.0735208\n",
            "Iteration 1, Node 0, Weight 0.53748083\n",
            "Iteration 1, Node 1, Weight 0.077270694\n",
            "Iteration 1, Node 2, Weight 0.076924086\n",
            "Iteration 1, Node 3, Weight 0.0769544\n",
            "Iteration 1, Node 4, Weight 0.077120416\n",
            "Iteration 1, Node 5, Weight 0.07687701\n",
            "Iteration 1, Node 6, Weight 0.077372596\n",
            "Iteration 2, Node 0, Weight 0.54901105\n",
            "Iteration 2, Node 1, Weight 0.07487144\n",
            "Iteration 2, Node 2, Weight 0.07486294\n",
            "Iteration 2, Node 3, Weight 0.07546505\n",
            "Iteration 2, Node 4, Weight 0.07513089\n",
            "Iteration 2, Node 5, Weight 0.07540193\n",
            "Iteration 2, Node 6, Weight 0.07525673\n",
            "Iteration 3, Node 0, Weight 0.5660472\n",
            "Iteration 3, Node 1, Weight 0.0720131\n",
            "Iteration 3, Node 2, Weight 0.07230421\n",
            "Iteration 3, Node 3, Weight 0.07244686\n",
            "Iteration 3, Node 4, Weight 0.072303854\n",
            "Iteration 3, Node 5, Weight 0.07254175\n",
            "Iteration 3, Node 6, Weight 0.07234305\n",
            "Iteration 4, Node 0, Weight 0.5326464\n",
            "Iteration 4, Node 1, Weight 0.07791124\n",
            "Iteration 4, Node 2, Weight 0.07790804\n",
            "Iteration 4, Node 3, Weight 0.077823676\n",
            "Iteration 4, Node 4, Weight 0.07792665\n",
            "Iteration 4, Node 5, Weight 0.07791734\n",
            "Iteration 4, Node 6, Weight 0.07786663\n",
            "Iteration 5, Node 0, Weight 0.52424043\n",
            "Iteration 5, Node 1, Weight 0.07921121\n",
            "Iteration 5, Node 2, Weight 0.07915949\n",
            "Iteration 5, Node 3, Weight 0.07946988\n",
            "Iteration 5, Node 4, Weight 0.079314426\n",
            "Iteration 5, Node 5, Weight 0.0791662\n",
            "Iteration 5, Node 6, Weight 0.07943847\n",
            "Iteration 6, Node 0, Weight 0.5267881\n",
            "Iteration 6, Node 1, Weight 0.07878553\n",
            "Iteration 6, Node 2, Weight 0.07882742\n",
            "Iteration 6, Node 3, Weight 0.07874918\n",
            "Iteration 6, Node 4, Weight 0.07898965\n",
            "Iteration 6, Node 5, Weight 0.07886128\n",
            "Iteration 6, Node 6, Weight 0.07899895\n",
            "Iteration 7, Node 0, Weight 0.5422407\n",
            "Iteration 7, Node 1, Weight 0.07612282\n",
            "Iteration 7, Node 2, Weight 0.076320484\n",
            "Iteration 7, Node 3, Weight 0.07644955\n",
            "Iteration 7, Node 4, Weight 0.07645479\n",
            "Iteration 7, Node 5, Weight 0.076369226\n",
            "Iteration 7, Node 6, Weight 0.076042406\n",
            "Iteration 8, Node 0, Weight 0.5299647\n",
            "Iteration 8, Node 1, Weight 0.07808978\n",
            "Iteration 8, Node 2, Weight 0.07828966\n",
            "Iteration 8, Node 3, Weight 0.07840332\n",
            "Iteration 8, Node 4, Weight 0.078405306\n",
            "Iteration 8, Node 5, Weight 0.078601584\n",
            "Iteration 8, Node 6, Weight 0.07824566\n",
            "Iteration 9, Node 0, Weight 0.53469574\n",
            "Iteration 9, Node 1, Weight 0.07766947\n",
            "Iteration 9, Node 2, Weight 0.07738727\n",
            "Iteration 9, Node 3, Weight 0.077605985\n",
            "Iteration 9, Node 4, Weight 0.07749689\n",
            "Iteration 9, Node 5, Weight 0.077217415\n",
            "Iteration 9, Node 6, Weight 0.07792729\n",
            "Unfolding layer 2\n",
            "Iteration 0, Node 0, Weight 0.55746794\n",
            "Iteration 0, Node 1, Weight 0.073726535\n",
            "Iteration 0, Node 2, Weight 0.073529005\n",
            "Iteration 0, Node 3, Weight 0.073937766\n",
            "Iteration 0, Node 4, Weight 0.07424961\n",
            "Iteration 0, Node 5, Weight 0.07355796\n",
            "Iteration 0, Node 6, Weight 0.07353118\n",
            "Iteration 1, Node 0, Weight 0.5372904\n",
            "Iteration 1, Node 1, Weight 0.07732901\n",
            "Iteration 1, Node 2, Weight 0.07691664\n",
            "Iteration 1, Node 3, Weight 0.076958455\n",
            "Iteration 1, Node 4, Weight 0.077161536\n",
            "Iteration 1, Node 5, Weight 0.07687348\n",
            "Iteration 1, Node 6, Weight 0.077470385\n",
            "Iteration 2, Node 0, Weight 0.54864407\n",
            "Iteration 2, Node 1, Weight 0.074879006\n",
            "Iteration 2, Node 2, Weight 0.07487005\n",
            "Iteration 2, Node 3, Weight 0.07558935\n",
            "Iteration 2, Node 4, Weight 0.07518433\n",
            "Iteration 2, Node 5, Weight 0.07550576\n",
            "Iteration 2, Node 6, Weight 0.07532743\n",
            "Iteration 3, Node 0, Weight 0.5657529\n",
            "Iteration 3, Node 1, Weight 0.07199889\n",
            "Iteration 3, Node 2, Weight 0.07235056\n",
            "Iteration 3, Node 3, Weight 0.07252117\n",
            "Iteration 3, Node 4, Weight 0.07234913\n",
            "Iteration 3, Node 5, Weight 0.07263346\n",
            "Iteration 3, Node 6, Weight 0.07239385\n",
            "Iteration 4, Node 0, Weight 0.532389\n",
            "Iteration 4, Node 1, Weight 0.077960126\n",
            "Iteration 4, Node 2, Weight 0.07795509\n",
            "Iteration 4, Node 3, Weight 0.07785337\n",
            "Iteration 4, Node 4, Weight 0.077976264\n",
            "Iteration 4, Node 5, Weight 0.077963896\n",
            "Iteration 4, Node 6, Weight 0.077902354\n",
            "Iteration 5, Node 0, Weight 0.52377933\n",
            "Iteration 5, Node 1, Weight 0.07927994\n",
            "Iteration 5, Node 2, Weight 0.07921552\n",
            "Iteration 5, Node 3, Weight 0.07958383\n",
            "Iteration 5, Node 4, Weight 0.07939273\n",
            "Iteration 5, Node 5, Weight 0.07921259\n",
            "Iteration 5, Node 6, Weight 0.079536065\n",
            "Iteration 6, Node 0, Weight 0.5263712\n",
            "Iteration 6, Node 1, Weight 0.07884465\n",
            "Iteration 6, Node 2, Weight 0.07889263\n",
            "Iteration 6, Node 3, Weight 0.07879691\n",
            "Iteration 6, Node 4, Weight 0.07908246\n",
            "Iteration 6, Node 5, Weight 0.07892515\n",
            "Iteration 6, Node 6, Weight 0.07908707\n",
            "Iteration 7, Node 0, Weight 0.5419963\n",
            "Iteration 7, Node 1, Weight 0.07612783\n",
            "Iteration 7, Node 2, Weight 0.07636723\n",
            "Iteration 7, Node 3, Weight 0.07652203\n",
            "Iteration 7, Node 4, Weight 0.076527424\n",
            "Iteration 7, Node 5, Weight 0.07642445\n",
            "Iteration 7, Node 6, Weight 0.07603475\n",
            "Iteration 8, Node 0, Weight 0.5295642\n",
            "Iteration 8, Node 1, Weight 0.07811826\n",
            "Iteration 8, Node 2, Weight 0.078354836\n",
            "Iteration 8, Node 3, Weight 0.07848621\n",
            "Iteration 8, Node 4, Weight 0.07848332\n",
            "Iteration 8, Node 5, Weight 0.07871235\n",
            "Iteration 8, Node 6, Weight 0.07828084\n",
            "Iteration 9, Node 0, Weight 0.5344179\n",
            "Iteration 9, Node 1, Weight 0.07773248\n",
            "Iteration 9, Node 2, Weight 0.07739657\n",
            "Iteration 9, Node 3, Weight 0.07766201\n",
            "Iteration 9, Node 4, Weight 0.077533156\n",
            "Iteration 9, Node 5, Weight 0.0772023\n",
            "Iteration 9, Node 6, Weight 0.07805548\n",
            "Unfolding layer 3\n",
            "Iteration 0, Node 0, Weight 0.55718\n",
            "Iteration 0, Node 1, Weight 0.07377119\n",
            "Iteration 0, Node 2, Weight 0.07354687\n",
            "Iteration 0, Node 3, Weight 0.07401298\n",
            "Iteration 0, Node 4, Weight 0.074365824\n",
            "Iteration 0, Node 5, Weight 0.07357602\n",
            "Iteration 0, Node 6, Weight 0.0735472\n",
            "Iteration 1, Node 0, Weight 0.5370905\n",
            "Iteration 1, Node 1, Weight 0.077384815\n",
            "Iteration 1, Node 2, Weight 0.07691695\n",
            "Iteration 1, Node 3, Weight 0.07696849\n",
            "Iteration 1, Node 4, Weight 0.0772028\n",
            "Iteration 1, Node 5, Weight 0.076877065\n",
            "Iteration 1, Node 6, Weight 0.07755951\n",
            "Iteration 2, Node 0, Weight 0.54829204\n",
            "Iteration 2, Node 1, Weight 0.074892506\n",
            "Iteration 2, Node 2, Weight 0.07488317\n",
            "Iteration 2, Node 3, Weight 0.07570125\n",
            "Iteration 2, Node 4, Weight 0.0752365\n",
            "Iteration 2, Node 5, Weight 0.07560041\n",
            "Iteration 2, Node 6, Weight 0.075394146\n",
            "Iteration 3, Node 0, Weight 0.56545913\n",
            "Iteration 3, Node 1, Weight 0.07199451\n",
            "Iteration 3, Node 2, Weight 0.07239726\n",
            "Iteration 3, Node 3, Weight 0.07259144\n",
            "Iteration 3, Node 4, Weight 0.072394945\n",
            "Iteration 3, Node 5, Weight 0.0727184\n",
            "Iteration 3, Node 6, Weight 0.07244431\n",
            "Iteration 4, Node 0, Weight 0.5321309\n",
            "Iteration 4, Node 1, Weight 0.078008175\n",
            "Iteration 4, Node 2, Weight 0.078001596\n",
            "Iteration 4, Node 3, Weight 0.077885255\n",
            "Iteration 4, Node 4, Weight 0.078024924\n",
            "Iteration 4, Node 5, Weight 0.07800997\n",
            "Iteration 4, Node 6, Weight 0.07793931\n",
            "Iteration 5, Node 0, Weight 0.52335\n",
            "Iteration 5, Node 1, Weight 0.07934463\n",
            "Iteration 5, Node 2, Weight 0.07926953\n",
            "Iteration 5, Node 3, Weight 0.079686664\n",
            "Iteration 5, Node 4, Weight 0.07946552\n",
            "Iteration 5, Node 5, Weight 0.07925846\n",
            "Iteration 5, Node 6, Weight 0.07962509\n",
            "Iteration 6, Node 0, Weight 0.52597815\n",
            "Iteration 6, Node 1, Weight 0.07890141\n",
            "Iteration 6, Node 2, Weight 0.078954525\n",
            "Iteration 6, Node 3, Weight 0.078844056\n",
            "Iteration 6, Node 4, Weight 0.079167634\n",
            "Iteration 6, Node 5, Weight 0.07898593\n",
            "Iteration 6, Node 6, Weight 0.079168275\n",
            "Iteration 7, Node 0, Weight 0.5417483\n",
            "Iteration 7, Node 1, Weight 0.07613903\n",
            "Iteration 7, Node 2, Weight 0.07641363\n",
            "Iteration 7, Node 3, Weight 0.07659016\n",
            "Iteration 7, Node 4, Weight 0.07659565\n",
            "Iteration 7, Node 5, Weight 0.076478\n",
            "Iteration 7, Node 6, Weight 0.07603525\n",
            "Iteration 8, Node 0, Weight 0.5291856\n",
            "Iteration 8, Node 1, Weight 0.078149125\n",
            "Iteration 8, Node 2, Weight 0.07841664\n",
            "Iteration 8, Node 3, Weight 0.07856293\n",
            "Iteration 8, Node 4, Weight 0.07855587\n",
            "Iteration 8, Node 5, Weight 0.0788125\n",
            "Iteration 8, Node 6, Weight 0.07831732\n",
            "Iteration 9, Node 0, Weight 0.53414184\n",
            "Iteration 9, Node 1, Weight 0.07779261\n",
            "Iteration 9, Node 2, Weight 0.07741139\n",
            "Iteration 9, Node 3, Weight 0.07771627\n",
            "Iteration 9, Node 4, Weight 0.07757075\n",
            "Iteration 9, Node 5, Weight 0.077196494\n",
            "Iteration 9, Node 6, Weight 0.07817064\n",
            "Unfolding layer 4\n",
            "Iteration 0, Node 0, Weight 0.55690026\n",
            "Iteration 0, Node 1, Weight 0.07381491\n",
            "Iteration 0, Node 2, Weight 0.07356753\n",
            "Iteration 0, Node 3, Weight 0.07408298\n",
            "Iteration 0, Node 4, Weight 0.07447108\n",
            "Iteration 0, Node 5, Weight 0.07359688\n",
            "Iteration 0, Node 6, Weight 0.07356628\n",
            "Iteration 1, Node 0, Weight 0.5368893\n",
            "Iteration 1, Node 1, Weight 0.07743763\n",
            "Iteration 1, Node 2, Weight 0.076921985\n",
            "Iteration 1, Node 3, Weight 0.07698191\n",
            "Iteration 1, Node 4, Weight 0.07724313\n",
            "Iteration 1, Node 5, Weight 0.07688496\n",
            "Iteration 1, Node 6, Weight 0.07764102\n",
            "Iteration 2, Node 0, Weight 0.5479579\n",
            "Iteration 2, Node 1, Weight 0.074909374\n",
            "Iteration 2, Node 2, Weight 0.0748997\n",
            "Iteration 2, Node 3, Weight 0.07580269\n",
            "Iteration 2, Node 4, Weight 0.0752866\n",
            "Iteration 2, Node 5, Weight 0.075687006\n",
            "Iteration 2, Node 6, Weight 0.075456746\n",
            "Iteration 3, Node 0, Weight 0.56517285\n",
            "Iteration 3, Node 1, Weight 0.071996346\n",
            "Iteration 3, Node 2, Weight 0.07244303\n",
            "Iteration 3, Node 3, Weight 0.07265748\n",
            "Iteration 3, Node 4, Weight 0.07243995\n",
            "Iteration 3, Node 5, Weight 0.072797045\n",
            "Iteration 3, Node 6, Weight 0.07249329\n",
            "Iteration 4, Node 0, Weight 0.5318787\n",
            "Iteration 4, Node 1, Weight 0.07805449\n",
            "Iteration 4, Node 2, Weight 0.078046605\n",
            "Iteration 4, Node 3, Weight 0.07791768\n",
            "Iteration 4, Node 4, Weight 0.078071766\n",
            "Iteration 4, Node 5, Weight 0.078054614\n",
            "Iteration 4, Node 6, Weight 0.077976085\n",
            "Iteration 5, Node 0, Weight 0.5229511\n",
            "Iteration 5, Node 1, Weight 0.07940524\n",
            "Iteration 5, Node 2, Weight 0.079320945\n",
            "Iteration 5, Node 3, Weight 0.079780065\n",
            "Iteration 5, Node 4, Weight 0.07953306\n",
            "Iteration 5, Node 5, Weight 0.079302885\n",
            "Iteration 5, Node 6, Weight 0.07970662\n",
            "Iteration 6, Node 0, Weight 0.5256098\n",
            "Iteration 6, Node 1, Weight 0.0789553\n",
            "Iteration 6, Node 2, Weight 0.07901284\n",
            "Iteration 6, Node 3, Weight 0.07888971\n",
            "Iteration 6, Node 4, Weight 0.07924595\n",
            "Iteration 6, Node 5, Weight 0.07904328\n",
            "Iteration 6, Node 6, Weight 0.079243176\n",
            "Iteration 7, Node 0, Weight 0.54150414\n",
            "Iteration 7, Node 1, Weight 0.07615379\n",
            "Iteration 7, Node 2, Weight 0.076458685\n",
            "Iteration 7, Node 3, Weight 0.076653905\n",
            "Iteration 7, Node 4, Weight 0.0766595\n",
            "Iteration 7, Node 5, Weight 0.07652919\n",
            "Iteration 7, Node 6, Weight 0.07604082\n",
            "Iteration 8, Node 0, Weight 0.52882993\n",
            "Iteration 8, Node 1, Weight 0.07818071\n",
            "Iteration 8, Node 2, Weight 0.0784748\n",
            "Iteration 8, Node 3, Weight 0.078633904\n",
            "Iteration 8, Node 4, Weight 0.07862328\n",
            "Iteration 8, Node 5, Weight 0.07890364\n",
            "Iteration 8, Node 6, Weight 0.07835374\n",
            "Iteration 9, Node 0, Weight 0.5338735\n",
            "Iteration 9, Node 1, Weight 0.077849455\n",
            "Iteration 9, Node 2, Weight 0.07742926\n",
            "Iteration 9, Node 3, Weight 0.07776808\n",
            "Iteration 9, Node 4, Weight 0.077608205\n",
            "Iteration 9, Node 5, Weight 0.07719662\n",
            "Iteration 9, Node 6, Weight 0.07827484\n",
            "Unfolding layer 5\n",
            "Iteration 0, Node 0, Weight 0.5566323\n",
            "Iteration 0, Node 1, Weight 0.073857084\n",
            "Iteration 0, Node 2, Weight 0.07358962\n",
            "Iteration 0, Node 3, Weight 0.07414805\n",
            "Iteration 0, Node 4, Weight 0.07456686\n",
            "Iteration 0, Node 5, Weight 0.07361911\n",
            "Iteration 0, Node 6, Weight 0.07358699\n",
            "Iteration 1, Node 0, Weight 0.53669214\n",
            "Iteration 1, Node 1, Weight 0.07748734\n",
            "Iteration 1, Node 2, Weight 0.07693004\n",
            "Iteration 1, Node 3, Weight 0.076997265\n",
            "Iteration 1, Node 4, Weight 0.077281944\n",
            "Iteration 1, Node 5, Weight 0.0768955\n",
            "Iteration 1, Node 6, Weight 0.07771577\n",
            "Iteration 2, Node 0, Weight 0.54764307\n",
            "Iteration 2, Node 1, Weight 0.07492802\n",
            "Iteration 2, Node 2, Weight 0.07491806\n",
            "Iteration 2, Node 3, Weight 0.075895\n",
            "Iteration 2, Node 4, Weight 0.07533419\n",
            "Iteration 2, Node 5, Weight 0.07576639\n",
            "Iteration 2, Node 6, Weight 0.07551524\n",
            "Iteration 3, Node 0, Weight 0.5648981\n",
            "Iteration 3, Node 1, Weight 0.07200216\n",
            "Iteration 3, Node 2, Weight 0.072487146\n",
            "Iteration 3, Node 3, Weight 0.072719246\n",
            "Iteration 3, Node 4, Weight 0.07248338\n",
            "Iteration 3, Node 5, Weight 0.07286982\n",
            "Iteration 3, Node 6, Weight 0.072540216\n",
            "Iteration 4, Node 0, Weight 0.53163624\n",
            "Iteration 4, Node 1, Weight 0.07809867\n",
            "Iteration 4, Node 2, Weight 0.07808962\n",
            "Iteration 4, Node 3, Weight 0.07794974\n",
            "Iteration 4, Node 4, Weight 0.078116395\n",
            "Iteration 4, Node 5, Weight 0.0780973\n",
            "Iteration 4, Node 6, Weight 0.07801196\n",
            "Iteration 5, Node 0, Weight 0.52258104\n",
            "Iteration 5, Node 1, Weight 0.07946181\n",
            "Iteration 5, Node 2, Weight 0.07936951\n",
            "Iteration 5, Node 3, Weight 0.07986517\n",
            "Iteration 5, Node 4, Weight 0.079595655\n",
            "Iteration 5, Node 5, Weight 0.07934533\n",
            "Iteration 5, Node 6, Weight 0.0797814\n",
            "Iteration 6, Node 0, Weight 0.5252656\n",
            "Iteration 6, Node 1, Weight 0.07900611\n",
            "Iteration 6, Node 2, Weight 0.0790675\n",
            "Iteration 6, Node 3, Weight 0.07893334\n",
            "Iteration 6, Node 4, Weight 0.07931805\n",
            "Iteration 6, Node 5, Weight 0.07909709\n",
            "Iteration 6, Node 6, Weight 0.07931227\n",
            "Iteration 7, Node 0, Weight 0.541268\n",
            "Iteration 7, Node 1, Weight 0.076170534\n",
            "Iteration 7, Node 2, Weight 0.076501824\n",
            "Iteration 7, Node 3, Weight 0.07671334\n",
            "Iteration 7, Node 4, Weight 0.07671899\n",
            "Iteration 7, Node 5, Weight 0.07657767\n",
            "Iteration 7, Node 6, Weight 0.076049566\n",
            "Iteration 8, Node 0, Weight 0.5284973\n",
            "Iteration 8, Node 1, Weight 0.078212045\n",
            "Iteration 8, Node 2, Weight 0.078529246\n",
            "Iteration 8, Node 3, Weight 0.07869954\n",
            "Iteration 8, Node 4, Weight 0.078685805\n",
            "Iteration 8, Node 5, Weight 0.07898679\n",
            "Iteration 8, Node 6, Weight 0.07838925\n",
            "Iteration 9, Node 0, Weight 0.5336163\n",
            "Iteration 9, Node 1, Weight 0.07790292\n",
            "Iteration 9, Node 2, Weight 0.077448726\n",
            "Iteration 9, Node 3, Weight 0.07781713\n",
            "Iteration 9, Node 4, Weight 0.077644765\n",
            "Iteration 9, Node 5, Weight 0.07720062\n",
            "Iteration 9, Node 6, Weight 0.078369595\n",
            "Unfolding layer 6\n",
            "Iteration 0, Node 0, Weight 0.5563777\n",
            "Iteration 0, Node 1, Weight 0.07389731\n",
            "Iteration 0, Node 2, Weight 0.07361216\n",
            "Iteration 0, Node 3, Weight 0.07420842\n",
            "Iteration 0, Node 4, Weight 0.07465425\n",
            "Iteration 0, Node 5, Weight 0.0736418\n",
            "Iteration 0, Node 6, Weight 0.07360832\n",
            "Iteration 1, Node 0, Weight 0.5365016\n",
            "Iteration 1, Node 1, Weight 0.07753393\n",
            "Iteration 1, Node 2, Weight 0.07693996\n",
            "Iteration 1, Node 3, Weight 0.0770136\n",
            "Iteration 1, Node 4, Weight 0.07731896\n",
            "Iteration 1, Node 5, Weight 0.07690759\n",
            "Iteration 1, Node 6, Weight 0.07778437\n",
            "Iteration 2, Node 0, Weight 0.5473477\n",
            "Iteration 2, Node 1, Weight 0.074947506\n",
            "Iteration 2, Node 2, Weight 0.07493731\n",
            "Iteration 2, Node 3, Weight 0.07597929\n",
            "Iteration 2, Node 4, Weight 0.07537915\n",
            "Iteration 2, Node 5, Weight 0.0758393\n",
            "Iteration 2, Node 6, Weight 0.07556976\n",
            "Iteration 3, Node 0, Weight 0.56463647\n",
            "Iteration 3, Node 1, Weight 0.072010584\n",
            "Iteration 3, Node 2, Weight 0.07252924\n",
            "Iteration 3, Node 3, Weight 0.072776884\n",
            "Iteration 3, Node 4, Weight 0.07252488\n",
            "Iteration 3, Node 5, Weight 0.07293711\n",
            "Iteration 3, Node 6, Weight 0.07258479\n",
            "Iteration 4, Node 0, Weight 0.5314054\n",
            "Iteration 4, Node 1, Weight 0.07814046\n",
            "Iteration 4, Node 2, Weight 0.078130394\n",
            "Iteration 4, Node 3, Weight 0.0779809\n",
            "Iteration 4, Node 4, Weight 0.0781586\n",
            "Iteration 4, Node 5, Weight 0.07813782\n",
            "Iteration 4, Node 6, Weight 0.07804643\n",
            "Iteration 5, Node 0, Weight 0.52223814\n",
            "Iteration 5, Node 1, Weight 0.07951448\n",
            "Iteration 5, Node 2, Weight 0.07941513\n",
            "Iteration 5, Node 3, Weight 0.07994295\n",
            "Iteration 5, Node 4, Weight 0.07965366\n",
            "Iteration 5, Node 5, Weight 0.07938559\n",
            "Iteration 5, Node 6, Weight 0.07985008\n",
            "Iteration 6, Node 0, Weight 0.5249451\n",
            "Iteration 6, Node 1, Weight 0.07905381\n",
            "Iteration 6, Node 2, Weight 0.07911855\n",
            "Iteration 6, Node 3, Weight 0.0789747\n",
            "Iteration 6, Node 4, Weight 0.079384424\n",
            "Iteration 6, Node 5, Weight 0.07914742\n",
            "Iteration 6, Node 6, Weight 0.07937605\n",
            "Iteration 7, Node 0, Weight 0.54104227\n",
            "Iteration 7, Node 1, Weight 0.07618829\n",
            "Iteration 7, Node 2, Weight 0.0765428\n",
            "Iteration 7, Node 3, Weight 0.076768644\n",
            "Iteration 7, Node 4, Weight 0.07677439\n",
            "Iteration 7, Node 5, Weight 0.076623395\n",
            "Iteration 7, Node 6, Weight 0.076060265\n",
            "Iteration 8, Node 0, Weight 0.5281871\n",
            "Iteration 8, Node 1, Weight 0.078242555\n",
            "Iteration 8, Node 2, Weight 0.07858011\n",
            "Iteration 8, Node 3, Weight 0.0787602\n",
            "Iteration 8, Node 4, Weight 0.078743726\n",
            "Iteration 8, Node 5, Weight 0.07906288\n",
            "Iteration 8, Node 6, Weight 0.07842345\n",
            "Iteration 9, Node 0, Weight 0.5333718\n",
            "Iteration 9, Node 1, Weight 0.07795297\n",
            "Iteration 9, Node 2, Weight 0.07746885\n",
            "Iteration 9, Node 3, Weight 0.077863306\n",
            "Iteration 9, Node 4, Weight 0.07767992\n",
            "Iteration 9, Node 5, Weight 0.07720715\n",
            "Iteration 9, Node 6, Weight 0.078455985\n",
            "Unfolding layer 7\n",
            "Iteration 0, Node 0, Weight 0.55613756\n",
            "Iteration 0, Node 1, Weight 0.0739354\n",
            "Iteration 0, Node 2, Weight 0.0736346\n",
            "Iteration 0, Node 3, Weight 0.07426437\n",
            "Iteration 0, Node 4, Weight 0.07473413\n",
            "Iteration 0, Node 5, Weight 0.07366435\n",
            "Iteration 0, Node 6, Weight 0.07362968\n",
            "Iteration 1, Node 0, Weight 0.5363193\n",
            "Iteration 1, Node 1, Weight 0.07757747\n",
            "Iteration 1, Node 2, Weight 0.07695097\n",
            "Iteration 1, Node 3, Weight 0.07703031\n",
            "Iteration 1, Node 4, Weight 0.07735397\n",
            "Iteration 1, Node 5, Weight 0.07692054\n",
            "Iteration 1, Node 6, Weight 0.07784744\n",
            "Iteration 2, Node 0, Weight 0.5470717\n",
            "Iteration 2, Node 1, Weight 0.07496717\n",
            "Iteration 2, Node 2, Weight 0.07495673\n",
            "Iteration 2, Node 3, Weight 0.07605633\n",
            "Iteration 2, Node 4, Weight 0.07542136\n",
            "Iteration 2, Node 5, Weight 0.07590628\n",
            "Iteration 2, Node 6, Weight 0.075620465\n",
            "Iteration 3, Node 0, Weight 0.5643894\n",
            "Iteration 3, Node 1, Weight 0.0720206\n",
            "Iteration 3, Node 2, Weight 0.0725691\n",
            "Iteration 3, Node 3, Weight 0.072830506\n",
            "Iteration 3, Node 4, Weight 0.07256424\n",
            "Iteration 3, Node 5, Weight 0.07299932\n",
            "Iteration 3, Node 6, Weight 0.07262683\n",
            "Iteration 4, Node 0, Weight 0.531187\n",
            "Iteration 4, Node 1, Weight 0.07817981\n",
            "Iteration 4, Node 2, Weight 0.07816885\n",
            "Iteration 4, Node 3, Weight 0.0780108\n",
            "Iteration 4, Node 4, Weight 0.078198284\n",
            "Iteration 4, Node 5, Weight 0.07817601\n",
            "Iteration 4, Node 6, Weight 0.0780793\n",
            "Iteration 5, Node 0, Weight 0.52192056\n",
            "Iteration 5, Node 1, Weight 0.079563424\n",
            "Iteration 5, Node 2, Weight 0.07945781\n",
            "Iteration 5, Node 3, Weight 0.08001415\n",
            "Iteration 5, Node 4, Weight 0.0797073\n",
            "Iteration 5, Node 5, Weight 0.07942354\n",
            "Iteration 5, Node 6, Weight 0.07991322\n",
            "Iteration 6, Node 0, Weight 0.524647\n",
            "Iteration 6, Node 1, Weight 0.079098396\n",
            "Iteration 6, Node 2, Weight 0.079166114\n",
            "Iteration 6, Node 3, Weight 0.07901366\n",
            "Iteration 6, Node 4, Weight 0.079445586\n",
            "Iteration 6, Node 5, Weight 0.07919433\n",
            "Iteration 6, Node 6, Weight 0.07943487\n",
            "Iteration 7, Node 0, Weight 0.5408279\n",
            "Iteration 7, Node 1, Weight 0.07620638\n",
            "Iteration 7, Node 2, Weight 0.07658147\n",
            "Iteration 7, Node 3, Weight 0.07682003\n",
            "Iteration 7, Node 4, Weight 0.07682583\n",
            "Iteration 7, Node 5, Weight 0.07666622\n",
            "Iteration 7, Node 6, Weight 0.07607211\n",
            "Iteration 8, Node 0, Weight 0.52789855\n",
            "Iteration 8, Node 1, Weight 0.07827189\n",
            "Iteration 8, Node 2, Weight 0.07862746\n",
            "Iteration 8, Node 3, Weight 0.078816235\n",
            "Iteration 8, Node 4, Weight 0.07879734\n",
            "Iteration 8, Node 5, Weight 0.07913255\n",
            "Iteration 8, Node 6, Weight 0.07845603\n",
            "Iteration 9, Node 0, Weight 0.5331411\n",
            "Iteration 9, Node 1, Weight 0.0779997\n",
            "Iteration 9, Node 2, Weight 0.077489056\n",
            "Iteration 9, Node 3, Weight 0.07790657\n",
            "Iteration 9, Node 4, Weight 0.07771345\n",
            "Iteration 9, Node 5, Weight 0.07721527\n",
            "Iteration 9, Node 6, Weight 0.07853492\n",
            "Unfolding layer 8\n",
            "Iteration 0, Node 0, Weight 0.55591184\n",
            "Iteration 0, Node 1, Weight 0.07397128\n",
            "Iteration 0, Node 2, Weight 0.07365652\n",
            "Iteration 0, Node 3, Weight 0.07431615\n",
            "Iteration 0, Node 4, Weight 0.07480724\n",
            "Iteration 0, Node 5, Weight 0.07368638\n",
            "Iteration 0, Node 6, Weight 0.07365063\n",
            "Iteration 1, Node 0, Weight 0.5361463\n",
            "Iteration 1, Node 1, Weight 0.07761802\n",
            "Iteration 1, Node 2, Weight 0.07696255\n",
            "Iteration 1, Node 3, Weight 0.07704697\n",
            "Iteration 1, Node 4, Weight 0.07738696\n",
            "Iteration 1, Node 5, Weight 0.076933846\n",
            "Iteration 1, Node 6, Weight 0.077905424\n",
            "Iteration 2, Node 0, Weight 0.5468143\n",
            "Iteration 2, Node 1, Weight 0.0749866\n",
            "Iteration 2, Node 2, Weight 0.07497596\n",
            "Iteration 2, Node 3, Weight 0.07612687\n",
            "Iteration 2, Node 4, Weight 0.07546087\n",
            "Iteration 2, Node 5, Weight 0.07596783\n",
            "Iteration 2, Node 6, Weight 0.07566751\n",
            "Iteration 3, Node 0, Weight 0.56415695\n",
            "Iteration 3, Node 1, Weight 0.07203159\n",
            "Iteration 3, Node 2, Weight 0.07260667\n",
            "Iteration 3, Node 3, Weight 0.07288034\n",
            "Iteration 3, Node 4, Weight 0.072601326\n",
            "Iteration 3, Node 5, Weight 0.07305677\n",
            "Iteration 3, Node 6, Weight 0.07266635\n",
            "Iteration 4, Node 0, Weight 0.5309814\n",
            "Iteration 4, Node 1, Weight 0.07821668\n",
            "Iteration 4, Node 2, Weight 0.07820491\n",
            "Iteration 4, Node 3, Weight 0.07803925\n",
            "Iteration 4, Node 4, Weight 0.07823548\n",
            "Iteration 4, Node 5, Weight 0.07821186\n",
            "Iteration 4, Node 6, Weight 0.0781104\n",
            "Iteration 5, Node 0, Weight 0.52162683\n",
            "Iteration 5, Node 1, Weight 0.079608835\n",
            "Iteration 5, Node 2, Weight 0.07949767\n",
            "Iteration 5, Node 3, Weight 0.08007937\n",
            "Iteration 5, Node 4, Weight 0.07975689\n",
            "Iteration 5, Node 5, Weight 0.07945916\n",
            "Iteration 5, Node 6, Weight 0.07997126\n",
            "Iteration 6, Node 0, Weight 0.52437043\n",
            "Iteration 6, Node 1, Weight 0.07913998\n",
            "Iteration 6, Node 2, Weight 0.07921036\n",
            "Iteration 6, Node 3, Weight 0.079050265\n",
            "Iteration 6, Node 4, Weight 0.07950191\n",
            "Iteration 6, Node 5, Weight 0.07923799\n",
            "Iteration 6, Node 6, Weight 0.07948913\n",
            "Iteration 7, Node 0, Weight 0.5406259\n",
            "Iteration 7, Node 1, Weight 0.076224364\n",
            "Iteration 7, Node 2, Weight 0.076617815\n",
            "Iteration 7, Node 3, Weight 0.07686766\n",
            "Iteration 7, Node 4, Weight 0.07687354\n",
            "Iteration 7, Node 5, Weight 0.076706275\n",
            "Iteration 7, Node 6, Weight 0.07608452\n",
            "Iteration 8, Node 0, Weight 0.5276305\n",
            "Iteration 8, Node 1, Weight 0.07829984\n",
            "Iteration 8, Node 2, Weight 0.07867145\n",
            "Iteration 8, Node 3, Weight 0.07886797\n",
            "Iteration 8, Node 4, Weight 0.07884692\n",
            "Iteration 8, Node 5, Weight 0.07919643\n",
            "Iteration 8, Node 6, Weight 0.07848688\n",
            "Iteration 9, Node 0, Weight 0.53292406\n",
            "Iteration 9, Node 1, Weight 0.07804319\n",
            "Iteration 9, Node 2, Weight 0.07750891\n",
            "Iteration 9, Node 3, Weight 0.077947006\n",
            "Iteration 9, Node 4, Weight 0.07774519\n",
            "Iteration 9, Node 5, Weight 0.07722439\n",
            "Iteration 9, Node 6, Weight 0.07860713\n",
            "Unfolding layer 9\n",
            "Iteration 0, Node 0, Weight 0.5557005\n",
            "Iteration 0, Node 1, Weight 0.074004956\n",
            "Iteration 0, Node 2, Weight 0.073677726\n",
            "Iteration 0, Node 3, Weight 0.07436405\n",
            "Iteration 0, Node 4, Weight 0.074874215\n",
            "Iteration 0, Node 5, Weight 0.07370767\n",
            "Iteration 0, Node 6, Weight 0.073670976\n",
            "Iteration 1, Node 0, Weight 0.5359829\n",
            "Iteration 1, Node 1, Weight 0.07765573\n",
            "Iteration 1, Node 2, Weight 0.0769743\n",
            "Iteration 1, Node 3, Weight 0.07706327\n",
            "Iteration 1, Node 4, Weight 0.07741789\n",
            "Iteration 1, Node 5, Weight 0.07694717\n",
            "Iteration 1, Node 6, Weight 0.07795872\n",
            "Iteration 2, Node 0, Weight 0.5465749\n",
            "Iteration 2, Node 1, Weight 0.0750055\n",
            "Iteration 2, Node 2, Weight 0.0749947\n",
            "Iteration 2, Node 3, Weight 0.07619151\n",
            "Iteration 2, Node 4, Weight 0.07549773\n",
            "Iteration 2, Node 5, Weight 0.07602444\n",
            "Iteration 2, Node 6, Weight 0.07571114\n",
            "Iteration 3, Node 0, Weight 0.5639392\n",
            "Iteration 3, Node 1, Weight 0.07204303\n",
            "Iteration 3, Node 2, Weight 0.0726419\n",
            "Iteration 3, Node 3, Weight 0.07292655\n",
            "Iteration 3, Node 4, Weight 0.07263614\n",
            "Iteration 3, Node 5, Weight 0.07310983\n",
            "Iteration 3, Node 6, Weight 0.07270333\n",
            "Iteration 4, Node 0, Weight 0.53078866\n",
            "Iteration 4, Node 1, Weight 0.07825113\n",
            "Iteration 4, Node 2, Weight 0.07823864\n",
            "Iteration 4, Node 3, Weight 0.07806619\n",
            "Iteration 4, Node 4, Weight 0.07827021\n",
            "Iteration 4, Node 5, Weight 0.078245394\n",
            "Iteration 4, Node 6, Weight 0.0781397\n",
            "Iteration 5, Node 0, Weight 0.5213554\n",
            "Iteration 5, Node 1, Weight 0.07965091\n",
            "Iteration 5, Node 2, Weight 0.079534784\n",
            "Iteration 5, Node 3, Weight 0.08013916\n",
            "Iteration 5, Node 4, Weight 0.07980272\n",
            "Iteration 5, Node 5, Weight 0.07949247\n",
            "Iteration 5, Node 6, Weight 0.08002462\n",
            "Iteration 6, Node 0, Weight 0.524114\n",
            "Iteration 6, Node 1, Weight 0.079178646\n",
            "Iteration 6, Node 2, Weight 0.079251416\n",
            "Iteration 6, Node 3, Weight 0.07908446\n",
            "Iteration 6, Node 4, Weight 0.07955378\n",
            "Iteration 6, Node 5, Weight 0.07927851\n",
            "Iteration 6, Node 6, Weight 0.07953914\n",
            "Iteration 7, Node 0, Weight 0.5404359\n",
            "Iteration 7, Node 1, Weight 0.07624196\n",
            "Iteration 7, Node 2, Weight 0.07665182\n",
            "Iteration 7, Node 3, Weight 0.07691181\n",
            "Iteration 7, Node 4, Weight 0.07691774\n",
            "Iteration 7, Node 5, Weight 0.07674363\n",
            "Iteration 7, Node 6, Weight 0.07609714\n",
            "Iteration 8, Node 0, Weight 0.52738184\n",
            "Iteration 8, Node 1, Weight 0.07832634\n",
            "Iteration 8, Node 2, Weight 0.07871231\n",
            "Iteration 8, Node 3, Weight 0.078915715\n",
            "Iteration 8, Node 4, Weight 0.078892745\n",
            "Iteration 8, Node 5, Weight 0.07925505\n",
            "Iteration 8, Node 6, Weight 0.07851596\n",
            "Iteration 9, Node 0, Weight 0.532721\n",
            "Iteration 9, Node 1, Weight 0.07808366\n",
            "Iteration 9, Node 2, Weight 0.077528216\n",
            "Iteration 9, Node 3, Weight 0.07798472\n",
            "Iteration 9, Node 4, Weight 0.07777512\n",
            "Iteration 9, Node 5, Weight 0.07723407\n",
            "Iteration 9, Node 6, Weight 0.07867328\n",
            "Deep unfolding done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "#def fedlearning(weights,method,params):\n",
        "    # numpy data\n",
        "    if dataset == 'mnist':\n",
        "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    elif dataset == 'fashion':\n",
        "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    ind = y_test == 9\n",
        "    x_test, y_test = x_test[~ind], y_test[~ind]\n",
        "    ind = y_test == 8\n",
        "    x_test, y_test = x_test[~ind], y_test[~ind]\n",
        "    ind = y_train == 9\n",
        "    x_train, y_train = x_train[~ind], y_train[~ind]\n",
        "    ind = y_train == 8\n",
        "    x_train, y_train = x_train[~ind], y_train[~ind]\n",
        "\n",
        "    x_train = x_train / 255.0\n",
        "    if encoding_mode == 'vanilla':\n",
        "        mean = 0\n",
        "    elif encoding_mode == 'mean':\n",
        "        mean = jnp.mean(x_train, axis=0)\n",
        "    elif encoding_mode == 'half':\n",
        "        mean = 0.5\n",
        "    x_train = x_train - mean\n",
        "    x_train = tf.image.resize(x_train[..., tf.newaxis], (int(2**(n/2)), int(2**(n/2)))).numpy()[..., 0].reshape(-1, 2**n)\n",
        "    x_train = x_train / jnp.sqrt(jnp.sum(x_train**2, axis=-1, keepdims=True))\n",
        "\n",
        "    x_test = x_test / 255.0\n",
        "    x_test = x_test - mean\n",
        "    x_test = tf.image.resize(x_test[..., tf.newaxis], (int(2**(n/2)), int(2**(n/2)))).numpy()[..., 0].reshape(-1, 2**n)\n",
        "    x_test = x_test / jnp.sqrt(jnp.sum(x_test**2, axis=-1, keepdims=True))\n",
        "    y_test = jax.nn.one_hot(y_test, n_node)\n",
        "\n",
        "    world_train_loss = []\n",
        "    world_test_loss = []\n",
        "    world_train_acc = []\n",
        "    world_test_acc = []\n",
        "    paramslist_all = []\n",
        "    xlist_all = []\n",
        "    ylist_all = []\n",
        "    opt_statenodeall=[]\n",
        "    for t in tqdm(range(T)):\n",
        "        print(\"Iteration \", t)\n",
        "        params_list = []\n",
        "        opt_state_list = []\n",
        "        data_list = []\n",
        "        iter_list = []\n",
        "        params_lists=[]\n",
        "        xnodelists=[]\n",
        "        ynodelists=[]\n",
        "        for node in range(n_node-1):\n",
        "            x_train_node, y_train_node = filter(x_train, y_train, [0, node+1])\n",
        "            # x_train_node, y_train_node = x_train, jax.nn.one_hot(y_train, n_node)\n",
        "            data = tf.data.Dataset.from_tensor_slices((x_train_node, y_train_node)).batch(128)\n",
        "            data_list.append(data)\n",
        "            iter_list.append(iter(data))\n",
        "\n",
        "            key, subkey = jax.random.split(key)\n",
        "            params = jax.random.normal(subkey, (3 * k, n))\n",
        "            #params=params\n",
        "            opt = optax.adam(learning_rate=1e-2)\n",
        "            opt_state = opt.init(params)\n",
        "            params_list.append(params)\n",
        "            opt_state_list.append(opt_state)\n",
        "\n",
        "        loss_list = []\n",
        "        acc_list = []\n",
        "        params_listnode=[]\n",
        "        xnode=[]\n",
        "        ynode=[]\n",
        "        opt_statenode=[]\n",
        "        weighted_paramlist=[]\n",
        "        for node in range(n_node-1):\n",
        "            #for b in range(100):\n",
        "            #print(\"node\", node)\n",
        "\n",
        "            try:\n",
        "              x, y = next(iter_list[node])\n",
        "            except StopIteration:\n",
        "              iter_list[node] = iter(data_list[node])\n",
        "              x, y = next(iter_list[node])\n",
        "            x = x.numpy()\n",
        "            y = y.numpy()\n",
        "            loss_val, grad_val = compute_loss(params_list[node], x, y, k)\n",
        "            #print(\"loss lenth\",node, \"is\", len(loss_val))\n",
        "            #print(\"grad lenth\",node, \"is\", len(grad_val))\n",
        "            #print(\"para length\",node, \"is\", len(params_list[node]))\n",
        "            updates, opt_state_list[node] = opt.update(grad_val, opt_state_list[node], params_list[node])\n",
        "            params_list[node] = optax.apply_updates(params_list[node], updates)\n",
        "                #print(loss_val)\n",
        "            #if method==\"dunQFL\":\n",
        "            #params_list[node] = params_list[node]*normalized_weights_Per_it[t][node]\n",
        "            #if method==\"QFL\":\n",
        "              #params_list[node]=params_list[node]\n",
        "\n",
        "\n",
        "            #print(weighted_paramlist[node])\n",
        "            params_listnode.append(params_list[node])\n",
        "            opt_statenode.append(opt_state_list[node])\n",
        "            xnode.append(x)\n",
        "            ynode.append(y)\n",
        "\n",
        "\n",
        "            #avg_params = jnp.mean(jnp.stack(params_list, axis=0), axis=0)\n",
        "            #instead above line\n",
        "            ########################################\n",
        "\n",
        "    #def calculate_weighted_average_params(params_list, normalized_weights_Per_it):\n",
        "    # Convert the list of parameters to a JAX array\n",
        "            params_array = jnp.stack(params_list, axis=0)\n",
        "\n",
        "            # Expand dimensions of normalized_weights_Per_it to match params_array\n",
        "            weights_expanded = jnp.expand_dims(normalized_weights_Per_it, axis=-1)\n",
        "\n",
        "            # Calculate the weighted sum along the iteration axis\n",
        "            weighted_sum_params = jnp.sum(params_array * weights_expanded, axis=0)\n",
        "\n",
        "            # Calculate the total weight along the iteration axis\n",
        "            total_weights = jnp.sum(weights_expanded, axis=0)\n",
        "\n",
        "            # Calculate the average parameters\n",
        "            avg_params = weighted_sum_params / total_weights\n",
        "            #########################################\n",
        "\n",
        "            for node in range(n_node-1):\n",
        "                params_list[node] = avg_params\n",
        "\n",
        "                #if b % 25 == 0:\n",
        "            avg_loss = jnp.mean(compute_loss(avg_params, x_test[:1024], y_test[:1024], k)[0])\n",
        "            loss_list.append(avg_loss)\n",
        "            acc_list.append(compute_accuracy(avg_params, x_test[:1024], y_test[:1024], k).mean())\n",
        "            tqdm.write(f\"Iteration {t}, loss {avg_loss}, accuracy {acc_list[-1]}\")\n",
        "\n",
        "        paramslist_all.append(params_listnode)\n",
        "        xlist_all.append(xnode)\n",
        "        ylist_all.append(ynode)\n",
        "        opt_statenodeall.append(opt_statenode)\n",
        "        test_acc = jnp.mean(pred(avg_params, x_test[:1024], k).argmax(axis=-1) == y_test[:1024].argmax(axis=-1))\n",
        "        test_loss = -jnp.mean(jnp.log(pred(avg_params, x_test[:1024], k)) * y_test[:1024])\n",
        "\n",
        "        world_train_loss.append(loss_list)\n",
        "        world_test_loss.append(test_loss)\n",
        "        world_train_acc.append(acc_list)\n",
        "        world_test_acc.append(test_acc)\n",
        "        #tqdm.write(f\"world {world}: test loss {test_loss}, test accuracy {test_acc}\")\n",
        "        #print(len(paramslist_all[0]))\n",
        "\n",
        "    #return paramslist_all, xlist_all, ylist_all,opt_statenodeall\n",
        "\n",
        "    #avg_test_loss = jnp.mean(jnp.array(world_test_loss), axis=0)\n",
        "    #avg_test_acc = jnp.mean(jnp.array(world_test_acc), axis=0)\n",
        "    #std_test_loss = jnp.std(jnp.array(world_test_loss), axis=0)\n",
        "    #std_test_acc = jnp.std(jnp.array(world_test_acc), axis=0)\n",
        "    #print(f'test loss: {avg_test_loss}+-{std_test_loss}, test acc: {avg_test_acc}+-{std_test_acc}')"
      ],
      "metadata": {
        "id": "NsBoabcPIrCw",
        "outputId": "2b38fbd3-6e66-4f82-a113-8a95d8bd6ede",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:26<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, loss 2.378636360168457, accuracy 0.119140625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:36<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, loss 2.3321735858917236, accuracy 0.09765625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:46<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, loss 2.232487678527832, accuracy 0.1015625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:57<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, loss 2.3044190406799316, accuracy 0.1162109375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [01:07<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, loss 2.3684639930725098, accuracy 0.1083984375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [01:17<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, loss 2.412191152572632, accuracy 0.087890625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [01:27<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, loss 2.408784866333008, accuracy 0.09375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [01:40<15:04, 100.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [01:50<15:04, 100.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss 2.139474391937256, accuracy 0.158203125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [02:01<15:04, 100.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss 2.1716184616088867, accuracy 0.15625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [02:11<15:04, 100.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss 2.1790506839752197, accuracy 0.1337890625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [02:20<15:04, 100.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss 2.125211238861084, accuracy 0.1630859375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [02:30<15:04, 100.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss 2.1392173767089844, accuracy 0.15625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [02:40<15:04, 100.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss 2.1928858757019043, accuracy 0.154296875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [02:50<15:04, 100.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss 2.2975943088531494, accuracy 0.125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [03:07<12:18, 92.33s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 2/10 [03:17<12:18, 92.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss 2.184457540512085, accuracy 0.1142578125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 2/10 [03:27<12:18, 92.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss 2.3922648429870605, accuracy 0.146484375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 2/10 [03:36<12:18, 92.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss 2.404451847076416, accuracy 0.125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 2/10 [03:47<12:18, 92.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss 2.4720406532287598, accuracy 0.0869140625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 2/10 [03:57<12:18, 92.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss 2.5601141452789307, accuracy 0.076171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 2/10 [04:07<12:18, 92.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss 2.677370548248291, accuracy 0.0576171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 2/10 [04:16<12:18, 92.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss 2.8052163124084473, accuracy 0.0576171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [04:30<10:16, 88.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 3/10 [04:40<10:16, 88.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3, loss 2.1984939575195312, accuracy 0.1572265625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 3/10 [04:50<10:16, 88.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3, loss 2.2096493244171143, accuracy 0.158203125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 3/10 [05:00<10:16, 88.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3, loss 2.201834201812744, accuracy 0.1357421875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 3/10 [05:10<10:16, 88.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3, loss 2.1129510402679443, accuracy 0.171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 3/10 [05:20<10:16, 88.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3, loss 2.065094470977783, accuracy 0.142578125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 3/10 [05:32<10:16, 88.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3, loss 2.142866373062134, accuracy 0.1298828125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 3/10 [05:42<10:16, 88.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3, loss 2.23368501663208, accuracy 0.1181640625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [05:55<08:42, 87.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 4/10 [06:06<08:42, 87.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss 2.213921070098877, accuracy 0.1416015625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 4/10 [06:15<08:42, 87.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss 2.113245725631714, accuracy 0.1611328125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 4/10 [06:25<08:42, 87.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss 2.2411975860595703, accuracy 0.140625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 4/10 [06:36<08:42, 87.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss 2.279510974884033, accuracy 0.138671875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 4/10 [06:46<08:42, 87.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss 2.205343246459961, accuracy 0.1435546875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 4/10 [06:56<08:42, 87.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss 2.2385106086730957, accuracy 0.1396484375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 4/10 [07:05<08:42, 87.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss 2.3707942962646484, accuracy 0.134765625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [07:18<07:08, 85.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 5/10 [07:29<07:08, 85.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5, loss 2.1921727657318115, accuracy 0.115234375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 5/10 [07:39<07:08, 85.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5, loss 2.211344003677368, accuracy 0.1103515625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 5/10 [07:49<07:08, 85.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5, loss 2.2274608612060547, accuracy 0.1240234375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 5/10 [07:58<07:08, 85.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5, loss 2.286057233810425, accuracy 0.107421875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 5/10 [08:08<07:08, 85.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5, loss 2.3344569206237793, accuracy 0.1005859375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 5/10 [08:18<07:08, 85.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5, loss 2.3513309955596924, accuracy 0.0849609375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 5/10 [08:28<07:08, 85.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5, loss 2.3846497535705566, accuracy 0.0634765625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [08:42<05:39, 84.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 6/10 [08:52<05:39, 84.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6, loss 2.2418227195739746, accuracy 0.0966796875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 6/10 [09:01<05:39, 84.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6, loss 2.1449809074401855, accuracy 0.169921875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 6/10 [09:11<05:39, 84.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6, loss 2.234755277633667, accuracy 0.095703125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 6/10 [09:22<05:39, 84.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6, loss 2.3311679363250732, accuracy 0.0791015625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 6/10 [09:32<05:39, 84.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6, loss 2.4094185829162598, accuracy 0.11328125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 6/10 [09:42<05:39, 84.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6, loss 2.4561171531677246, accuracy 0.166015625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 6/10 [09:52<05:39, 84.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6, loss 2.5084753036499023, accuracy 0.1533203125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [10:14<04:22, 87.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 7/10 [10:26<04:22, 87.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7, loss 2.2512340545654297, accuracy 0.169921875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 7/10 [10:36<04:22, 87.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7, loss 2.122661828994751, accuracy 0.1904296875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 7/10 [10:46<04:22, 87.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7, loss 2.101494312286377, accuracy 0.1865234375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 7/10 [10:56<04:22, 87.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7, loss 2.187617063522339, accuracy 0.171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 7/10 [11:06<04:22, 87.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7, loss 2.2762887477874756, accuracy 0.115234375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 7/10 [11:16<04:22, 87.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7, loss 2.319890022277832, accuracy 0.1181640625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 7/10 [11:25<04:22, 87.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7, loss 2.3328769207000732, accuracy 0.138671875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [11:39<02:53, 86.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 8/10 [11:50<02:53, 86.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss 2.2201318740844727, accuracy 0.13671875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 8/10 [12:00<02:53, 86.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss 2.27400541305542, accuracy 0.0791015625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 8/10 [12:10<02:53, 86.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss 2.299978733062744, accuracy 0.1083984375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 8/10 [12:20<02:53, 86.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss 2.3256585597991943, accuracy 0.134765625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 8/10 [12:30<02:53, 86.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss 2.34875226020813, accuracy 0.1416015625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 8/10 [12:40<02:53, 86.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss 2.299895763397217, accuracy 0.1318359375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 8/10 [12:51<02:53, 86.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss 2.2877297401428223, accuracy 0.125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [13:04<01:26, 86.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 9/10 [13:15<01:26, 86.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9, loss 2.224853038787842, accuracy 0.150390625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 9/10 [13:24<01:26, 86.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9, loss 2.141108512878418, accuracy 0.1630859375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 9/10 [13:36<01:26, 86.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9, loss 2.1350924968719482, accuracy 0.1689453125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 9/10 [13:46<01:26, 86.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9, loss 2.0880513191223145, accuracy 0.2001953125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 9/10 [13:57<01:26, 86.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9, loss 2.1244096755981445, accuracy 0.1904296875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 9/10 [14:07<01:26, 86.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9, loss 2.2447288036346436, accuracy 0.1513671875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 9/10 [14:16<01:26, 86.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9, loss 2.365236759185791, accuracy 0.103515625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [14:30<00:00, 87.08s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: when I calculate avg_params = jnp.mean(jnp.stack(params_list, axis=0), axis=0), use normalized_weights_Per_it[t][node] to give node contrbution\n",
        "\n",
        "avg_params = jnp.mean(jnp.stack(params_list[], axis=0), axis=0)*normalized_weights_Per_it[t][node]\n"
      ],
      "metadata": {
        "id": "6DkbwGStzjHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: world_train_loss.append(loss_list)         world_test_loss.append(test_loss)         world_train_acc.append(acc_list)         world_test_acc.append(test_acc) save into separate .csv\n",
        "\n",
        "import csv\n",
        "\n",
        "# Write the world_train_loss list to a CSV file\n",
        "with open(f'DQFLworld_train_loss.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow(['Iteration', 'Node', 'Loss'])\n",
        "\n",
        "    # Write the data rows\n",
        "    for i in range(len(world_train_loss)):\n",
        "        for j in range(len(world_train_loss[i])):\n",
        "            writer.writerow([i, j, world_train_loss[i][j]])\n",
        "\n",
        "# Write the world_test_loss list to a CSV file\n",
        "with open(f'DQFLworld_test_loss.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow(['Iteration', 'Loss'])\n",
        "\n",
        "    # Write the data rows\n",
        "    for i in range(len(world_test_loss)):\n",
        "        writer.writerow([i, world_test_loss[i]])\n",
        "\n",
        "# Write the world_train_acc list to a CSV file\n",
        "with open(f'DQFLworld_train_acc.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow(['Iteration', 'Node', 'Accuracy'])\n",
        "\n",
        "    # Write the data rows\n",
        "    for i in range(len(world_train_acc)):\n",
        "        for j in range(len(world_train_acc[i])):\n",
        "            writer.writerow([i, j, world_train_acc[i][j]])\n",
        "\n",
        "# Write the world_test_acc list to a CSV file\n",
        "with open(f'DQFLworld_test_acc.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow(['Iteration', 'Accuracy'])\n",
        "\n",
        "    # Write the data rows\n",
        "    for i in range(len(world_test_acc)):\n",
        "        writer.writerow([i, world_test_acc[i]])\n"
      ],
      "metadata": {
        "id": "afomOPLrTZl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#method= \"dunQFL\"\n",
        "method=\"QFL\"\n",
        "\n",
        "#if method== \"dunFL\":\n",
        "key, subkey = jax.random.split(key)\n",
        "params = jax.random.normal(subkey, (3 * k, n))\n",
        "\n",
        "fedlearning(normalized_weights_Per_it,method,params)"
      ],
      "metadata": {
        "id": "pyKMmPqdKtd3",
        "outputId": "9d08c247-0de4-4f0f-8128-277cab46b3e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world 0, loss 0.41524749994277954, accuracy 0.451171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:01<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world 0, loss 0.4123117923736572, accuracy 0.4697265625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 1/5 [00:02<00:09,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [00:02<00:09,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world 1, loss 0.41524749994277954, accuracy 0.451171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [00:03<00:09,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world 1, loss 0.4123117923736572, accuracy 0.4697265625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:03<00:05,  1.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 2/5 [00:04<00:05,  1.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world 2, loss 0.41524749994277954, accuracy 0.451171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 2/5 [00:04<00:05,  1.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world 2, loss 0.4123117923736572, accuracy 0.4697265625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [00:05<00:03,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 3/5 [00:06<00:03,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world 3, loss 0.41524749994277954, accuracy 0.451171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 3/5 [00:07<00:03,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world 3, loss 0.4123117923736572, accuracy 0.4697265625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [00:08<00:02,  2.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 4/5 [00:08<00:02,  2.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world 4, loss 0.41524749994277954, accuracy 0.451171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 4/5 [00:08<00:02,  2.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world 4, loss 0.4123117923736572, accuracy 0.4697265625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:09<00:00,  1.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 0.13743732869625092+-1.4901161193847656e-08, test acc: 0.4697265625+-0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: avg_params = jnp.mean(jnp.stack(params_list, axis=0), axis=0) add weights to calculate mean\n",
        "\n",
        "avg_params = jnp.mean(jnp.stack(params_list, axis=0), axis=0, weights=normalized_weights_node)\n"
      ],
      "metadata": {
        "id": "7oGH3gqJIi5i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}