{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanikairoshi/QFL-and-Deep-Unfolded-QFL/blob/main/DQFL_RiemannianAverage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdPhLlT9q3Np"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Us3DV6Sfq6Mw"
      },
      "outputs": [],
      "source": [
        "\n",
        "%%capture\n",
        "!pip install genomic-benchmarks\n",
        "!pip install qiskit qiskit_machine_learning qiskit_algorithms\n",
        "!pip install qiskit-aer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "er-_TE10rI2c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install qiskit\n",
        "!pip install qiskit_machine_learning\n",
        "!pip install qiskit-aer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geomstats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klPFthgu5E_J",
        "outputId": "012f9dc1-0178-45d8-ad56-2a9f1d94ed27"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting geomstats\n",
            "  Downloading geomstats-2.8.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: joblib>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from geomstats) (1.4.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from geomstats) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from geomstats) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from geomstats) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.10/dist-packages (from geomstats) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from geomstats) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->geomstats) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->geomstats) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->geomstats) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->geomstats) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->geomstats) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->geomstats) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->geomstats) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->geomstats) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->geomstats) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->geomstats) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->geomstats) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->geomstats) (1.16.0)\n",
            "Downloading geomstats-2.8.0-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: geomstats\n",
            "Successfully installed geomstats-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m5M_eDObB6OI"
      },
      "outputs": [],
      "source": [
        "#-------Split data for federated Setting--------#\n",
        "num_epochs = 50\n",
        "max_train_iterations = 100\n",
        "samples_per_epoch=100\n",
        "#backend = Aer.get_backend('aer_simulator')\n",
        "word_size = 40\n",
        "\n",
        "# Configuration variables\n",
        "num_clients = 3\n",
        "num_federated_layers = 10\n",
        "num_deep_unfolding_iterations = 2\n",
        "initial_learning_rate = 1e-4\n",
        "meta_learning_rate=1e-4\n",
        "initial_perturbation = 0.01\n",
        "momentum = 0.95\n",
        "gradient_moving_avg = 0\n",
        "\n",
        "# Define federated learning with accuracy tracking\n",
        "num_features = 5\n",
        "global_model_weights, global_model_accuracy = {}, []\n",
        "clients_train_accuracies, clients_test_accuracies = [], []\n",
        "\n",
        "# Define the federated learning parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from geomstats.geometry.euclidean import Euclidean\n",
        "from geomstats.learning.frechet_mean import FrechetMean\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "def riemannian_weighted_aggregation(\n",
        "    epoch_weights,\n",
        "    client_data_sizes=None,\n",
        "    client_accuracies=None,\n",
        "    weight_mechanism=\"fixed\",\n",
        "    **kwargs\n",
        "):\n",
        "    \"\"\"\n",
        "    Perform Riemannian weighted aggregation of QNN model weights.\n",
        "\n",
        "    Parameters:\n",
        "    - epoch_weights: List of client weights for the epoch\n",
        "    - client_data_sizes: List of sizes of data owned by each client\n",
        "    - client_accuracies: List of accuracies achieved by each client\n",
        "    - weight_mechanism: The weighting mechanism to use (\"data\", \"accuracy\", \"fixed\")\n",
        "    - kwargs: Additional parameters for specific weighting mechanisms\n",
        "\n",
        "    Returns:\n",
        "    - aggregated_weights: Aggregated model weights using Riemannian mean\n",
        "    \"\"\"\n",
        "    num_clients = len(epoch_weights)\n",
        "\n",
        "    # Determine weights based on mechanism\n",
        "    if weight_mechanism == \"data\":\n",
        "        if not client_data_sizes:\n",
        "            raise ValueError(\"Client data sizes must be provided for data-proportional weighting.\")\n",
        "        weights = [size / sum(client_data_sizes) for size in client_data_sizes]\n",
        "\n",
        "    elif weight_mechanism == \"accuracy\":\n",
        "        if not client_accuracies:\n",
        "            raise ValueError(\"Client accuracies must be provided for accuracy-based weighting.\")\n",
        "        weights = [acc / sum(client_accuracies) for acc in client_accuracies]\n",
        "\n",
        "    elif weight_mechanism == \"fixed\":\n",
        "        best_client_index = kwargs.get(\"best_client_index\", 0)\n",
        "        weights = [1.0 if i == best_client_index else 0.0 for i in range(num_clients)]\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown weighting mechanism: {weight_mechanism}\")\n",
        "\n",
        "    # Normalize weights\n",
        "    weights = np.array(weights) / sum(weights)\n",
        "\n",
        "    # Compute Riemannian average using FrechetMean\n",
        "    geometry = Euclidean(epoch_weights[0].size)  # Assume Euclidean space for QNN weights\n",
        "    mean = FrechetMean(geometry)\n",
        "    aggregated_weights = mean.fit(epoch_weights, weights=weights)\n",
        "\n",
        "    return aggregated_weights\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "5pGVfolb5K2f",
        "outputId": "ef7e3691-75a7-43a0-999c-8561be3fac5a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef riemannian_weighted_aggregation(\\n    epoch_weights,\\n    client_data_sizes=None,\\n    client_accuracies=None,\\n    weight_mechanism=\"fixed\",\\n    **kwargs\\n):\\n    \"\"\"\\n    Perform Riemannian weighted aggregation of QNN model weights.\\n\\n    Parameters:\\n    - epoch_weights: List of client weights for the epoch\\n    - client_data_sizes: List of sizes of data owned by each client\\n    - client_accuracies: List of accuracies achieved by each client\\n    - weight_mechanism: The weighting mechanism to use (\"data\", \"accuracy\", \"fixed\")\\n    - kwargs: Additional parameters for specific weighting mechanisms\\n\\n    Returns:\\n    - aggregated_weights: Aggregated model weights using Riemannian mean\\n    \"\"\"\\n    num_clients = len(epoch_weights)\\n\\n    # Determine weights based on mechanism\\n    if weight_mechanism == \"data\":\\n        if not client_data_sizes:\\n            raise ValueError(\"Client data sizes must be provided for data-proportional weighting.\")\\n        weights = [size / sum(client_data_sizes) for size in client_data_sizes]\\n\\n    elif weight_mechanism == \"accuracy\":\\n        if not client_accuracies:\\n            raise ValueError(\"Client accuracies must be provided for accuracy-based weighting.\")\\n        weights = [acc / sum(client_accuracies) for acc in client_accuracies]\\n\\n    elif weight_mechanism == \"fixed\":\\n        best_client_index = kwargs.get(\"best_client_index\", 0)\\n        weights = [1.0 if i == best_client_index else 0.0 for i in range(num_clients)]\\n\\n    else:\\n        raise ValueError(f\"Unknown weighting mechanism: {weight_mechanism}\")\\n\\n    # Normalize weights\\n    weights = np.array(weights) / sum(weights)\\n\\n    # Compute Riemannian average using FrechetMean\\n    geometry = Euclidean(epoch_weights[0].size)  # Assume Euclidean space for QNN weights\\n    mean = FrechetMean(geometry)\\n    aggregated_weights = mean.fit(epoch_weights, weights=weights)\\n\\n    return aggregated_weights\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "drzDdxhUYqkt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpAw5S3imZQW",
        "outputId": "3b35bdc3-48be-4734-ee1f-52613675cc12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/genomic_benchmarks/utils/datasets.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n",
            "INFO:qiskit.passmanager.base_tasks:Pass: UnrollCustomDefinitions - 0.14496 (ms)\n",
            "INFO:qiskit.passmanager.base_tasks:Pass: BasisTranslator - 0.64254 (ms)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1JW0-eTB-rJXvFcglqBo3pFZi1kyIWC3X\n",
            "From (redirected): https://drive.google.com/uc?id=1JW0-eTB-rJXvFcglqBo3pFZi1kyIWC3X&confirm=t&uuid=5f60c4bc-8f68-46a5-b1ad-f3f1211dab13\n",
            "To: /root/.genomic_benchmarks/demo_human_or_worm.zip\n",
            "100%|██████████| 28.9M/28.9M [00:00<00:00, 44.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nuber of samples in the test set: 25000\n",
            "Nuber of samples in the test set: 75000\n",
            "First sample int the data_set variable: \n",
            "('CCTTTATCTGGCTGCTTTAATTTCCCAGACATTTACATCGTCTGCAGCTATATTTCGTTGGATACCCCTTCCCTCCTAAAAAATGCCTTTTGAGGAAGAGGTGATCGAGGTCAGGATTACTTCTAAGAGTCAGTGTTAAAAACTCACTATTTTCCTTGTGTCTATCACTCTGCAGATAAGAGATGGGCCTTGCCTTCAAG', 0)\n",
            "\n",
            "First 5 samples in the word_combinations dict.\n",
            "CCTTTATCTGGCTGCTTTAATTTCCCAGACATTTACATCG 1\n",
            "CTTTATCTGGCTGCTTTAATTTCCCAGACATTTACATCGT 2\n",
            "TTTATCTGGCTGCTTTAATTTCCCAGACATTTACATCGTC 3\n",
            "TTATCTGGCTGCTTTAATTTCCCAGACATTTACATCGTCT 4\n",
            "TATCTGGCTGCTTTAATTTCCCAGACATTTACATCGTCTG 5\n",
            "First 5 samples of encoded data:\n",
            "First 5 samples of encoded shuffled data:\n",
            "First 5 samples of scaled encoded shuffled data:\n",
            "Length of np_train_data: 15000\n",
            "Length of np_test_data: 5000\n",
            "Client 0 Test Data Length: 1666\n",
            "Client 1 Test Data Length: 1666\n",
            "Client 2 Test Data Length: 1666\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import DemoHumanOrWorm\n",
        "import time\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit_algorithms.optimizers import COBYLA\n",
        "from qiskit_machine_learning.algorithms.classifiers import VQC\n",
        "from qiskit.primitives import BackendSampler\n",
        "from functools import partial\n",
        "from qiskit_aer import Aer\n",
        "\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit.primitives import BackendSampler\n",
        "from qiskit_algorithms.optimizers import SPSA\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from qiskit_algorithms.utils import algorithm_globals # Import algorithm_globals\n",
        "\n",
        "# Set random seed for reproducibility using algorithm_globals\n",
        "algorithm_globals.random_seed = 42  # Set seed globally\n",
        "\n",
        "\n",
        "\n",
        "test_set = DemoHumanOrWorm(split='test', version=0)\n",
        "train_set = DemoHumanOrWorm(split='train', version=0)\n",
        "\n",
        "data_set = train_set\n",
        "# data_set = train_set + test_set\n",
        "len(data_set)\n",
        "\n",
        "\n",
        "print(f\"Nuber of samples in the test set: {len(test_set)}\")\n",
        "print(f\"Nuber of samples in the test set: {len(train_set)}\")\n",
        "\n",
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import DemoHumanOrWorm\n",
        "\n",
        "test_set = DemoHumanOrWorm(split='test', version=0)\n",
        "train_set = DemoHumanOrWorm(split='train', version=0)\n",
        "\n",
        "data_set = train_set\n",
        "# data_set = train_set + test_set\n",
        "len(data_set)\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "word_combinations = defaultdict(int)\n",
        "iteration = 1\n",
        "for text, _ in data_set:\n",
        "    for i in range(len(text)):\n",
        "        word = text[i:i+word_size]\n",
        "        if word_combinations.get(word) is None:\n",
        "          word_combinations[word] = iteration\n",
        "          iteration += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"First sample int the data_set variable: \")\n",
        "print(data_set[0])\n",
        "\n",
        "print(\"\\nFirst 5 samples in the word_combinations dict.\")\n",
        "for key, value in list(word_combinations.items())[:5]:\n",
        "    print(key, value)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "# Preprocess the training set\n",
        "np_data_set = []\n",
        "for i in range(len(data_set)):\n",
        "    sequence, label = data_set[i]\n",
        "    sequence = sequence.strip()  # Remove any leading/trailing whitespace\n",
        "    words = [sequence[i:i + word_size] for i in range(0, len(sequence), word_size)]  # Split the sequence into 4-letter words\n",
        "    int_sequence = np.array([word_combinations[word] for word in words])\n",
        "    data_point = {'sequence': int_sequence, 'label': label}\n",
        "    np_data_set.append(data_point)\n",
        "\n",
        "\n",
        "print(\"First 5 samples of encoded data:\")\n",
        "np_data_set[:5]\n",
        "\n",
        "\n",
        "np.random.shuffle(np_data_set)\n",
        "print(\"First 5 samples of encoded shuffled data:\")\n",
        "np_data_set[:5]\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "sequences = np.array([item['sequence'] for item in np_data_set])\n",
        "sequences = np.vstack(sequences)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "sequences_scaled = scaler.fit_transform(sequences)\n",
        "\n",
        "for i, item in enumerate(np_data_set):\n",
        "    item['sequence'] = sequences_scaled[i]\n",
        "\n",
        "print(\"First 5 samples of scaled encoded shuffled data:\")\n",
        "np_data_set[:5]\n",
        "\n",
        "\n",
        "np_train_data = np_data_set[:15000]\n",
        "np_test_data = np_data_set[-5000:]\n",
        "\n",
        "print(f\"Length of np_train_data: {len(np_train_data)}\")\n",
        "print(f\"Length of np_test_data: {len(np_test_data)}\")\n",
        "\n",
        "test_sequences = [data_point[\"sequence\"] for data_point in np_test_data]\n",
        "test_labels = [data_point[\"label\"] for data_point in np_test_data]\n",
        "test_sequences = np.array(test_sequences)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "\n",
        "#---------------------------------------\n",
        "\n",
        "\n",
        "class Client:\n",
        "   def __init__(self, data, test_data):  # Add test_data to __init__\n",
        "        self.data = data\n",
        "        self.test_data = test_data  # Store test_data as an attribute\n",
        "        self.models = []\n",
        "        self.train_scores = []\n",
        "        self.test_scores = []\n",
        "        self.primary_model = None\n",
        "\n",
        "def split_dataset(num_clients, num_epochs, samples_per_epoch):\n",
        "    clients = []\n",
        "    # Split test data across clients\n",
        "    test_samples_per_client = len(np_test_data) // num_clients\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        client_data = []\n",
        "        for j in range(num_epochs):\n",
        "            start_idx = (i * num_epochs * samples_per_epoch) + (j * samples_per_epoch)\n",
        "            end_idx = (i * num_epochs * samples_per_epoch) + ((j + 1) * samples_per_epoch)\n",
        "            client_data.append(np_train_data[start_idx:end_idx])\n",
        "\n",
        "        # Assign a subset of the test data to each client\n",
        "        test_start_idx = i * test_samples_per_client\n",
        "        test_end_idx = (i + 1) * test_samples_per_client\n",
        "        client_test_data = np_test_data[test_start_idx:test_end_idx]\n",
        "\n",
        "        # Create Client instance with both train and test data\n",
        "        clients.append(Client(client_data, client_test_data))\n",
        "\n",
        "    return clients\n",
        "\n",
        "clients = split_dataset(num_clients, num_epochs, samples_per_epoch)\n",
        "\n",
        "# Verify test data distribution across clients\n",
        "for index, client in enumerate(clients):\n",
        "    print(f\"Client {index} Test Data Length: {len(client.test_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIg_mUrp3_2J"
      },
      "source": [
        "Data Load and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3OTrftC6uZd_"
      },
      "outputs": [],
      "source": [
        "def split_dataset_for_epochs(num_clients, num_epochs, train_data, test_data, samples_per_epoch):\n",
        "    \"\"\"\n",
        "    Split the dataset across multiple epochs and clients.\n",
        "\n",
        "    Args:\n",
        "        num_clients (int): Number of clients.\n",
        "        num_epochs (int): Number of epochs.\n",
        "        train_data (list): List of training data points.\n",
        "        test_data (list): List of test data points.\n",
        "        samples_per_epoch (int): Number of samples per epoch.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of Client objects with assigned data for each epoch.\n",
        "    \"\"\"\n",
        "    clients = []\n",
        "\n",
        "    # Split the training data across epochs and clients\n",
        "    train_samples_per_client = len(train_data) // num_clients\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        client_data_for_epochs = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            start_idx = (i * num_epochs * samples_per_epoch) + (epoch * samples_per_epoch)\n",
        "            end_idx = (i * num_epochs * samples_per_epoch) + ((epoch + 1) * samples_per_epoch)\n",
        "            client_data_for_epochs.append(train_data[start_idx:end_idx])\n",
        "\n",
        "        # Assign test data to each client\n",
        "        test_samples_per_client = len(test_data) // num_clients\n",
        "        test_start_idx = i * test_samples_per_client\n",
        "        test_end_idx = (i + 1) * test_samples_per_client\n",
        "        client_test_data = test_data[test_start_idx:test_end_idx]\n",
        "\n",
        "        # Create a Client instance with epoch-specific data\n",
        "        clients.append(Client(client_data_for_epochs, client_test_data))\n",
        "\n",
        "    return clients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vHHwtTUF65Gp"
      },
      "outputs": [],
      "source": [
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit.primitives import BackendSampler\n",
        "from qiskit_algorithms.optimizers import SPSA\n",
        "from qiskit_algorithms.utils import algorithm_globals\n",
        "from qiskit_algorithms.optimizers import COBYLA\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import log_loss\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Callback function to capture the loss values\n",
        "objective_func_vals = []  # Global list to store loss values\n",
        "learning_rates = []\n",
        "perturbations = []\n",
        "# Data structure for tracking per-client, per-layer objective function values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "o0CaYcz9FQw7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os  # For handling directories\n",
        "\n",
        "# Define the directory to save the plots\n",
        "output_dir = \"federated_round_plots\"\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "# Initialize a global variable to track the round number\n",
        "current_round = 1\n",
        "\n",
        "# Callback for visualization, gradient smoothing, and learning rate adjustment in deep unfolding\n",
        "def deep_unfolding_learning_rate_adjustment(parameters, obj_func_eval, gradients=None,round_number=0):\n",
        "    global gradient_moving_avg, learning_rates, perturbations,current_round\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Save the objective function value for visualization\n",
        "    objective_func_vals.append(obj_func_eval)\n",
        "\n",
        "    # If gradients are provided, smooth the gradient using momentum\n",
        "    if gradients is not None:\n",
        "        gradient_moving_avg = momentum * gradient_moving_avg + (1 - momentum) * gradients  # Apply moving average\n",
        "        delta_lr = 0.05 * gradient_moving_avg  # Adjust learning rate based on the smoothed gradient\n",
        "        delta_perturbation = 0.1 * gradient_moving_avg  # Adjust perturbation based on the same gradient\n",
        "    else:\n",
        "        delta_lr = 0  # No gradient info available in this iteration\n",
        "        delta_perturbation = 0\n",
        "\n",
        "    # Update learning rate and perturbation\n",
        "    if len(learning_rates) > 0:\n",
        "        new_lr = max(0.001, learning_rates[-1] + delta_lr)  # Ensure learning rate is positive and non-zero\n",
        "        new_perturbation = max(0.001, perturbations[-1] + delta_perturbation)  # Ensure perturbation is positive\n",
        "    else:\n",
        "        new_lr = initial_learning_rate\n",
        "        new_perturbation = initial_perturbation\n",
        "\n",
        "    learning_rates.append(new_lr)\n",
        "    perturbations.append(new_perturbation)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Visualization of learning rate and perturbation\n",
        "    plt.figure(figsize=(10, 12))  # Adjust figure size for better spacing\n",
        "\n",
        "    # Plot Objective Function Value\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(range(len(objective_func_vals)), objective_func_vals, label=\"Objective Function Value\", color='blue')\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Objective Function Value\")\n",
        "    plt.title(\"Objective Function Over Iterations\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid(True)  # Add grid for better readability\n",
        "\n",
        "    # Plot Learning Rate\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(range(len(learning_rates)), learning_rates, label=\"Learning Rate\", color='green')\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Learning Rate\")\n",
        "    plt.title(\"Learning Rate Over Iterations\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot Perturbation\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.plot(range(len(perturbations)), perturbations, label=\"Perturbation\", color='red')\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Perturbation\")\n",
        "    plt.title(\"Perturbation Over Iterations\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout(pad=3.0)  # Add padding between subplots\n",
        "    # Save the plot after each federated round\n",
        "    #plot_filename = os.path.join(output_dir, f\"federated_round_{current_round}.png\")\n",
        "    #plt.savefig(plot_filename)  # Save the figure\n",
        "    #plt.show()\n",
        "    plt.close()  # Close the plot to free memory\n",
        "\n",
        "    # Increment the round number for the next call\n",
        "    current_round += 1\n",
        "\n",
        "\n",
        "# Define the SPSA callback to capture gradients and update learning rate and perturbation dynamically\n",
        "def spsa_callback(nfev, parameters, obj_func_eval, stepsize, accept):\n",
        "    # Assuming `stepsize` contains gradient information or its approximation\n",
        "    gradients = stepsize\n",
        "    deep_unfolding_learning_rate_adjustment(parameters, obj_func_eval, gradients)\n",
        "\n",
        "# Custom SPSA optimizer with learnable learning rate and perturbation\n",
        "class LearnableLRPerturbationSPSA(SPSA):\n",
        "    def __init__(self, initial_lr=1e-4, initial_perturbation=0.01, lr_alpha=0.1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.lr = initial_lr  # Initial learning rate\n",
        "        self.perturbation = initial_perturbation  # Initial perturbation\n",
        "        self.lr_alpha = lr_alpha  # Learning rate and perturbation update speed\n",
        "\n",
        "    def _update_learning_rate_and_perturbation(self, gradient, obj_func_eval):\n",
        "        \"\"\"\n",
        "        Update both learning rate and perturbation based on gradient and objective function evaluation.\n",
        "        The learning rate increases if the objective function improves and decreases otherwise.\n",
        "        \"\"\"\n",
        "        # Use the gradient sign to determine if we should increase or decrease\n",
        "        grad_lr = np.sign(np.mean(gradient))  # Average gradient sign across parameters\n",
        "\n",
        "        if grad_lr > 0:  # Objective function is improving\n",
        "            self.lr += self.lr_alpha * abs(grad_lr)  # Increase learning rate\n",
        "            self.perturbation += self.lr_alpha * abs(grad_lr)  # Increase perturbation\n",
        "        else:  # Objective function is getting worse\n",
        "            self.lr -= self.lr_alpha * abs(grad_lr)  # Decrease learning rate\n",
        "            self.perturbation -= self.lr_alpha * abs(grad_lr)  # Decrease perturbation\n",
        "\n",
        "        # Ensure both learning rate and perturbation are positive\n",
        "        self.lr = max(0.001, self.lr)\n",
        "        self.perturbation = max(0.001, self.perturbation)\n",
        "\n",
        "    def step(self, gradient, obj_func_eval):\n",
        "        \"\"\"\n",
        "        Perform optimization step for both parameters, learning rate, and perturbation.\n",
        "        Use the objective function evaluation to dynamically adjust learning rate and perturbation.\n",
        "        \"\"\"\n",
        "        self._update_learning_rate_and_perturbation(gradient, obj_func_eval)\n",
        "        return super().step(gradient)  # Perform SPSA step for parameters\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the optimizer state (learning rates, perturbations, and gradient moving averages) for the next round.\n",
        "        \"\"\"\n",
        "        self.lr = initial_learning_rate\n",
        "        self.perturbation = initial_perturbation\n",
        "        self.gradient_moving_avg = 0  # Reset the moving average of the gradient\n",
        "        learning_rates.clear()  # Reset the learning rates history\n",
        "        perturbations.clear()  # Reset the perturbations history\n",
        "        objective_func_vals.clear()  # Clear the objective function history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X46XXHW1s4tR"
      },
      "outputs": [],
      "source": [
        "# Create optimizer with learnable learning rate and perturbation\n",
        "spsa_optimizer = LearnableLRPerturbationSPSA(\n",
        "      maxiter=20, learning_rate=initial_learning_rate, perturbation=initial_perturbation, callback=spsa_callback, lr_alpha=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-3Rhf0Ft7CI2"
      },
      "outputs": [],
      "source": [
        "\n",
        "#======================================================\n",
        "# Initialize QNN model\n",
        "def initialize_model(num_features,initial_params):\n",
        "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2)\n",
        "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "    qc = feature_map.compose(ansatz)\n",
        "\n",
        "    # Create optimizer with learnable learning rate and perturbation\n",
        "    spsa_optimizer = LearnableLRPerturbationSPSA(\n",
        "      maxiter=20, learning_rate=initial_learning_rate, perturbation=initial_perturbation, callback=spsa_callback, lr_alpha=0.1\n",
        ")\n",
        "    def parity(x):\n",
        "        return \"{:b}\".format(x).count(\"1\") % 2\n",
        "\n",
        "    sampler_qnn = SamplerQNN(\n",
        "        circuit=qc,\n",
        "        interpret=parity,\n",
        "        output_shape=2,\n",
        "        input_params=feature_map.parameters,\n",
        "        weight_params=ansatz.parameters\n",
        "    )\n",
        "\n",
        "\n",
        "    # Define the neural network classifier\n",
        "    qnn_classifier = NeuralNetworkClassifier(\n",
        "      neural_network=sampler_qnn,\n",
        "      optimizer=spsa_optimizer,\n",
        "      loss='squared_error',\n",
        "      initial_point=initial_params,  # Initialize with the starting parameters\n",
        ")\n",
        "\n",
        "\n",
        "    return qnn_classifier\n",
        "\n",
        "#=====================================================\n",
        "import os\n",
        "import csv\n",
        "\n",
        "# Define CSV file path\n",
        "csv_file = \"training_results.csv\"\n",
        "\n",
        "# Define headers\n",
        "headers = [\"Federated Round\", \"Client Number\", \"Iteration\", \"Objective Function Value\", \"Training Accuracy\", \"Test Accuracy\", \"Learning Rate\", \"Perturbation\"]\n",
        "\n",
        "# Check if the file exists and is empty before writing headers\n",
        "def initialize_csv():\n",
        "    if not os.path.exists(csv_file) or os.path.getsize(csv_file) == 0:\n",
        "        with open(csv_file, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow(headers)\n",
        "\n",
        "# Initialize the CSV file (only write headers if the file is empty or doesn't exist)\n",
        "initialize_csv()\n",
        "\n",
        "# Example of saving results for each federated round and client\n",
        "def save_results(federated_round, client_id, iteration, obj_func_val, train_acc, test_acc, learning_rate, perturbation):\n",
        "    with open(csv_file, mode='a', newline='') as file:  # Open file in append mode\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([federated_round, client_id, iteration, obj_func_val, train_acc, test_acc, learning_rate, perturbation])\n",
        "#=====================================================\n",
        "# Federated learning loop per client\n",
        "def train_qnn_model(client_data, client_test_data, model=None, client_id=None, layer=None):\n",
        "\n",
        "    global learning_rates, perturbations, objective_func_vals\n",
        "    print(\"Client Data Structure:\")  # Add this line to print the structure\n",
        "    print(client_data)                # This line prints the actual data\n",
        "    print(type(client_data))           # This line prints the data type\n",
        "    num_features = client_data[0][\"sequence\"].shape[0]\n",
        "\n",
        "    #initial_params = np.random.rand(RealAmplitudes(client_data.shape[1], reps=4).num_parameters)  # Initialize params\n",
        "    initial_params = np.random.rand(RealAmplitudes(len(client_data[0][\"sequence\"]), reps=3).num_parameters)\n",
        "\n",
        "    if model is None:\n",
        "        model = initialize_model(num_features, initial_params)\n",
        "\n",
        "    train_sequences = np.array([data_point[\"sequence\"] for data_point in client_data])\n",
        "    train_labels = np.array([data_point[\"label\"] for data_point in client_data])\n",
        "    test_sequences = np.array([data_point[\"sequence\"] for data_point in client_test_data])\n",
        "    test_labels = np.array([data_point[\"label\"] for data_point in client_test_data])\n",
        "\n",
        "    train_accuracies, test_accuracies, total_time = [], [], 0\n",
        "\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    # Deep Unfolding with multiple iterations\n",
        "    # Continue training with learned weights and adjust learning rate based on performance and gradients.\n",
        "    total_time = 0\n",
        "    current_params = initial_params  # Start with the initial parameters\n",
        "\n",
        "    for i in range(num_deep_unfolding_iterations):\n",
        "        print(\"\\n\")\n",
        "        print(f\"Deep Unfolding Iteration {i+1}/{num_deep_unfolding_iterations}\")\n",
        "        start_time = time.time()\n",
        "        model.fit(train_sequences, train_labels)\n",
        "        end_time = time.time()\n",
        "        total_time += end_time - start_time\n",
        "\n",
        "        # After training, retrieve the updated parameters from the optimizer\n",
        "        current_params = model.weights\n",
        "        print(f\"Trained parameters after iteration {i+1}: {current_params}\")\n",
        "\n",
        "        # Store final weights and learning rate for next round\n",
        "        final_learning_rate = learning_rates[-1]\n",
        "        final_perturbation = perturbations[-1]\n",
        "\n",
        "        # Evaluate the model performance\n",
        "        train_accuracy = model.score(train_sequences, train_labels)\n",
        "        test_accuracy = model.score(test_sequences, test_labels)\n",
        "\n",
        "        # Store accuracies for future reference\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        # Write the results to the CSV file\n",
        "        save_results(layer, client_id, i+1, objective_func_vals[-1], train_accuracy * 100, test_accuracy * 100, final_learning_rate, final_perturbation)\n",
        "\n",
        "        #with open(csv_file, mode='a', newline='') as file:\n",
        "          #writer = csv.writer(file)\n",
        "         #writer.writerow([i+1, objective_func_vals[-1], train_accuracy * 100, test_accuracy * 100, final_learning_rate, final_perturbation])\n",
        "\n",
        "        # Update the learning rate for the next iteration based on gradients from SPSA\n",
        "        spsa_optimizer.learning_rate = learning_rates[-1]\n",
        "        model.initial_point = current_params\n",
        "\n",
        "        # Log performance\n",
        "        print(f\"Iteration {i+1} - Learning Rate: {final_learning_rate:.6f}\")\n",
        "        print(f\"Iteration {i+1} - Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "        print(f\"Iteration {i+1} - Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "    return model, train_accuracy, train_accuracy, total_time\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zpK0oXUHzPtm"
      },
      "outputs": [],
      "source": [
        "# Get accuracy of model\n",
        "def get_accuracy(globalmodel_epoch, num_features, test_sequences, test_labels):\n",
        "    '''\n",
        "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
        "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "    param_dict = {param: weight for param, weight in zip(ansatz.parameters, weights)}\n",
        "    ansatz = ansatz.assign_parameters(param_dict)\n",
        "    qc = feature_map.compose(ansatz)\n",
        "\n",
        "    def parity(x):\n",
        "        return \"{:b}\".format(x).count(\"1\") % 2\n",
        "\n",
        "    sampler_qnn = SamplerQNN(\n",
        "        circuit=qc,\n",
        "        interpret=parity,\n",
        "        output_shape=2,\n",
        "        input_params=list(feature_map.parameters),\n",
        "        weight_params=list(ansatz.parameters)\n",
        "    )\n",
        "\n",
        "    qnn_classifier = NeuralNetworkClassifier(\n",
        "        sampler_qnn,\n",
        "        optimizer=spsa_optimizer,\n",
        "        #callback=callback_graph\n",
        "    )\n",
        "    '''\n",
        "    dummy_sequences = test_sequences[:1]  # Use one sample\n",
        "    dummy_labels = test_labels[:1]        # Corresponding label\n",
        "\n",
        "    # Fit the model once with dummy data\n",
        "    globalmodel_epoch.fit(dummy_sequences, dummy_labels)\n",
        "\n",
        "    # Return the accuracy on the subset\n",
        "   # Directly evaluate the model on the test data\n",
        "    accuracy = globalmodel_epoch.score(test_sequences, test_labels)\n",
        "\n",
        "    return accuracy\n",
        "    #return qnn_classifier.score(test_sequences, test_labels)\n",
        "\n",
        "    #if predictions.ndim > 1 and predictions.shape[1] == 2:\n",
        "        #predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    #accuracy = np.mean(predictions == test_labels)\n",
        "    #return accuracy\n",
        "\n",
        "# Function to extract numerical values of parameters\n",
        "def extract_param_values(model):\n",
        "    #param_values = []\n",
        "    # Loop through each parameter in the circuit and get its bound value\n",
        "    # Retrieve the circuit from the neural network\n",
        "    circuit = model.neural_network.circuit\n",
        "\n",
        "    # Extract the parameter values bound to the circuit\n",
        "    # Use enumerate to get both index and parameter\n",
        "    param_values = {param: circuit.parameters[i] for i, param in enumerate(circuit.parameters)}\n",
        "    return param_values\n",
        "#def set_param_values(model, param_values):\n",
        "    # Retrieve the circuit from the neural network\n",
        "    #circuit = model.neural_network.circuit\n",
        "\n",
        "    # Use assign_parameters to update the parameter values\n",
        "    #circuit.assign_parameters(param_values, inplace=True)\n",
        "# Function to set numerical values of parameters back into the circuit\n",
        "'''\n",
        "def set_param_values(model, param_values):\n",
        "    \"\"\"\n",
        "    Assign the averaged parameter values back to the circuit.\n",
        "\n",
        "    Args:\n",
        "        model: The QNN model to update.\n",
        "        param_values: The averaged parameter values.\n",
        "    \"\"\"\n",
        "    # Convert LinearMean object to a NumPy array\n",
        "    if isinstance(param_values, FrechetMean):  # Check if param_values is a LinearMean object\n",
        "        param_values = param_values.estimate_  # Access the estimate attribute for iterable weights\n",
        "\n",
        "    # Assign the averaged parameter values back to the circuit\n",
        "    parameter_dict = {param: value for param, value in zip(model.neural_network.circuit.parameters, param_values)}\n",
        "    model.neural_network.circuit.assign_parameters(parameter_dict)\n",
        "'''\n",
        "def set_param_values(model, param_values):\n",
        "    # Ensure model.neural_network has a quantum circuit\n",
        "    if hasattr(model.neural_network, \"_circuit\"):\n",
        "        circuit = model.neural_network._circuit\n",
        "    elif hasattr(model.neural_network, \"circuit\"):\n",
        "        circuit = model.neural_network.circuit\n",
        "    else:\n",
        "        raise AttributeError(\"The neural network does not have a circuit attribute.\")\n",
        "\n",
        "    # Assign parameters to the circuit\n",
        "    parameter_dict = {param: value for param, value in zip(circuit.parameters, param_values)}\n",
        "    circuit.assign_parameters(parameter_dict)\n",
        "\n",
        "# Manually average the numerical values of the parameters across clients\n",
        "def manual_average_weights(epoch_weights):\n",
        "    # Initialize a list to store the summed weights (initialize with zeros)\n",
        "    num_weights = len(epoch_weights[0])  # Number of weights in the model\n",
        "    num_clients = len(epoch_weights)  # Number of clients\n",
        "\n",
        "    # Initialize sum of weights to zero (assuming NumPy array or list of weights)\n",
        "    summed_weights = np.zeros(num_weights)\n",
        "\n",
        "    # Sum the weights from all clients\n",
        "    for client_weights in epoch_weights:\n",
        "        summed_weights += np.array(client_weights)\n",
        "\n",
        "    # Compute the average by dividing the summed weights by the number of clients\n",
        "    average_weights = summed_weights / num_clients\n",
        "\n",
        "    return average_weights\n",
        "\n",
        "def create_model_with_weights(weights):\n",
        "    initial_params = np.random.rand(RealAmplitudes(num_features, reps=3).num_parameters)\n",
        "    model = initialize_model(num_features,initial_params)\n",
        "    set_param_values(model, weights)  # Assign global weights to the model\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MuhZZtnnzmV5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to save accuracies to CSV\n",
        "def save_accuracies_to_csv(global_accuracies, clients_train_accuracies, clients_test_accuracies, filename='accuracies.csv'):\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write the header row\n",
        "        header = ['Epoch', 'Global Accuracy']\n",
        "        for i in range(len(clients_train_accuracies[0])):  # Assuming all clients have the same number of records\n",
        "            header.append(f'Client {i} Train Accuracy')\n",
        "            header.append(f'Client {i} Test Accuracy')\n",
        "        writer.writerow(header)\n",
        "\n",
        "        # Write the accuracy data for each epoch\n",
        "        for epoch in range(len(global_accuracies)):\n",
        "            row = [epoch, global_accuracies[epoch]]  # Start with epoch and global accuracy\n",
        "            for client_index in range(len(clients_train_accuracies[epoch])):\n",
        "                row.append(clients_train_accuracies[epoch][client_index])  # Add train accuracy for client\n",
        "                row.append(clients_test_accuracies[epoch][client_index])   # Add test accuracy for client\n",
        "            writer.writerow(row)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(np_train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJwPLFM66Z9s",
        "outputId": "a842a8c3-fbf2-4ebc-a84f-6e8501424860"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import logm, expm\n",
        "\n",
        "def riemannian_weighted_aggregation(epoch_weights, weights=None):\n",
        "    \"\"\"\n",
        "    Perform Riemannian weighted aggregation of client weights and return a NumPy array.\n",
        "    Args:\n",
        "        epoch_weights: List of NumPy arrays (or lists) containing client weights.\n",
        "        weights: List of scalar weights for each client (optional).\n",
        "    Returns:\n",
        "        Aggregated weights as a NumPy array.\n",
        "    \"\"\"\n",
        "    num_clients = len(epoch_weights)\n",
        "\n",
        "    if weights is None:\n",
        "        # Use equal weights if not provided\n",
        "        weights = np.ones(num_clients) / num_clients\n",
        "\n",
        "    # Ensure weights are normalized\n",
        "    weights = np.array(weights) / np.sum(weights)\n",
        "\n",
        "    # Convert all weights to NumPy arrays\n",
        "    # and DO NOT attempt to reshape them into square matrices\n",
        "    epoch_weights = [np.array(w) for w in epoch_weights]\n",
        "\n",
        "    # Initialize the Riemannian center (mean) as the first set of weights\n",
        "    riemannian_mean = epoch_weights[0]\n",
        "\n",
        "\n",
        "    # Iteratively compute the Riemannian weighted mean (assuming Euclidean geometry)\n",
        "    # Since the weights are not square matrices, we'll perform element-wise averaging\n",
        "    for _ in range(10):  # Fixed number of iterations for convergence\n",
        "        tangent_sum = np.zeros_like(riemannian_mean)\n",
        "\n",
        "        for w, weight in zip(epoch_weights, weights):\n",
        "            tangent_vector = w - riemannian_mean  # Tangent vector in Euclidean space\n",
        "            tangent_sum += weight * tangent_vector\n",
        "\n",
        "        riemannian_mean = riemannian_mean + tangent_sum\n",
        "\n",
        "    # Return the result as a NumPy array\n",
        "    return np.array(riemannian_mean)"
      ],
      "metadata": {
        "id": "2spAriSJYwmc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTt4DwZj7Is9"
      },
      "source": [
        "Federated Learning Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "BmYmJR4_7Hux",
        "outputId": "7b96720f-c006-486a-87c6-22aa83a249eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:qiskit_algorithms.optimizers.spsa:SPSA: Finished in 4.019387722015381\n",
            "Training Progress: 100%|██████████| 10/10 [1:13:45<00:00, 442.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global Model Accuracy in Epoch 9: 0.55\n",
            "----------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'csv_file_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-fcb6e6f54a51>\u001b[0m in \u001b[0;36m<cell line: 116>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mreset_callback_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy data saved to\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'csv_file_path' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "clients = split_dataset_for_epochs(num_clients, num_epochs, np_train_data, np_test_data, samples_per_epoch)\n",
        "\n",
        "# Display information about the data assigned to each client, including epoch-wise splits\n",
        "for idx, client in enumerate(clients):\n",
        "    print(f\"Client {idx + 1}:\")\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"  Epoch {epoch + 1}: Train data samples: {len(client.data[epoch])}\")\n",
        "    print(f\"  Test data samples: {len(client.test_data)}\")\n",
        "\n",
        "# Display information about the data assigned to each client\n",
        "for idx, client in enumerate(clients):\n",
        "    print(f\"Client {idx + 1}:\")\n",
        "    print(f\"  Train data samples: {len(client.data)}\")\n",
        "    print(f\"  Test data samples: {len(client.test_data)}\")\n",
        "\n",
        "    # Accessing the number of features in a sequence\n",
        "    if client.data:\n",
        "        num_features=client.data[0][0]['sequence'].shape[0]  # Access first data point of epoch 0\n",
        "        #num_features = client.data[0]['sequence'].shape[0]\n",
        "        print(f\"  Number of features in a sequence: {num_features}\")\n",
        "\n",
        "def reset_state():\n",
        "    # Reset the objective value, learning rate, and perturbation after each client\n",
        "    global objective_func_vals, learning_rates, perturbations\n",
        "    objective_func_vals = []  # Reset objective values\n",
        "    learning_rates = []  # Reset learning rates\n",
        "    perturbations = []  # Reset perturbations\n",
        "# Function to reset callback graph state after each round\n",
        "def reset_callback_graph():\n",
        "    global gradient_moving_avg, learning_rates, perturbations\n",
        "\n",
        "    # Reset the state variables to start fresh for the next round\n",
        "    gradient_moving_avg = np.zeros_like(gradient_moving_avg)  # Reset gradient moving average\n",
        "    learning_rates = [initial_learning_rate]  # Reset learning rates list to initial value\n",
        "    perturbations = [initial_perturbation]  # Reset perturbations list to initial value\n",
        "\n",
        "\n",
        "# Wrap the epoch loop with tqdm\n",
        "for epoch in tqdm(range(num_federated_layers), desc=\"Training Progress\"):\n",
        "#for epoch in range(num_federated_layers):\n",
        "    global_model_weights[epoch] = []\n",
        "    epoch_weights, epoch_train_accuracies, epoch_test_accuracies = [], [], []\n",
        "    best_client_index = -1\n",
        "    best_client_accuracy = -1\n",
        "    best_client_weights = None\n",
        "    index=0\n",
        "    print(\"\\n\")\n",
        "    print(f\"Fed_Epoch: {epoch}\")\n",
        "    for index, client in enumerate(clients):\n",
        "        print(\"\\n\")\n",
        "        print(f\"Fed_Epoch {epoch}, Client {index + 1}:\")\n",
        "\n",
        "        try:\n",
        "            # Ensure you're using the correct index for data\n",
        "            current_data = client.data[epoch]  # This assumes data is structured in epochs\n",
        "            print(f\"Training data for epoch {epoch}: {len(current_data)}\")\n",
        "        except IndexError:\n",
        "            print(f\"No data available for epoch {epoch} for Client {index + 1}\")\n",
        "        except KeyError as e:\n",
        "            print(f\"KeyError: {e} when accessing data for epoch {epoch} in Client {index + 1}\")\n",
        "        # Get the last objective value for this client, if available\n",
        "\n",
        "        model, train_score, test_score, train_time = train_qnn_model(\n",
        "            client.data[epoch],\n",
        "            client.test_data,\n",
        "            client_id=index,\n",
        "            layer=epoch,\n",
        "        )\n",
        "\n",
        "        epoch_train_accuracies.append(train_score)\n",
        "        epoch_test_accuracies.append(test_score)\n",
        "        epoch_weights.append(model.weights)\n",
        "        # Record the final objective value for this layer\n",
        "        #client_data[index]['federated_layers'][epoch]['objective_values'].append(objective_func_vals[-1])\n",
        "    #Check if this client haas the best accuracy so far\n",
        "        if train_score > best_client_accuracy:\n",
        "            best_client_accuracy = train_score\n",
        "            best_client_index = index\n",
        "            best_client_weights = model.weights #Assuming `model.weights` holds the model's parameters\n",
        "    # Perform Riemannian aggregation\n",
        "    #aggregated_weights = riemannian_weighted_aggregation(\n",
        "        #epoch_weights=epoch_weights,\n",
        "        #client_data_sizes=[len(client.data[epoch]) for client in clients],\n",
        "        #client_accuracies=epoch_test_accuracies,\n",
        "        #weight_mechanism=\"fixed\"\n",
        "    #)\n",
        "    # Perform Riemannian aggregation\n",
        "    aggregated_weights = riemannian_weighted_aggregation(epoch_weights)#, weights=client_weights\n",
        "\n",
        "\n",
        "    # Update global model\n",
        "    global_model_weights[epoch] = aggregated_weights\n",
        "    new_model_with_global_weights = create_model_with_weights(aggregated_weights)\n",
        "\n",
        "    #new_model_with_global_weights = create_model_with_weights(global_model_weights[epoch])\n",
        "\n",
        "    for index, client in enumerate(clients):\n",
        "        client.primary_model = new_model_with_global_weights\n",
        "\n",
        "    global_accuracy = get_accuracy(new_model_with_global_weights, num_features, test_sequences, test_labels)\n",
        "    global_model_accuracy.append(global_accuracy)\n",
        "\n",
        "    clients_train_accuracies.append(epoch_train_accuracies)\n",
        "    clients_test_accuracies.append(epoch_test_accuracies)\n",
        "\n",
        "    print(f\"Global Model Accuracy in Epoch {epoch}: {global_accuracy:.2f}\")\n",
        "    print(\"----------------------------------------------------------\")\n",
        "    # Save results for the current iteration of the client in the federated round\n",
        "\n",
        "\n",
        "    # Save accuracies to CSV after each epoch (or at the end of all epochs)\n",
        "    save_accuracies_to_csv(global_model_accuracy, clients_train_accuracies, clients_test_accuracies, filename='DQFL_accuracies_5di_3client.csv')\n",
        "    # After each round, reset callback state to prepare for the next round\n",
        "    reset_callback_graph()\n",
        "\n",
        "print(\"Accuracy data saved to\", csv_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zs_rlJT5XJv"
      },
      "source": [
        "Split data as iid and non-iid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pgm1g3VHfXC"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#Introduce custome cross entropy function\n",
        "import numpy as np\n",
        "\n",
        "# Callback for updating learning rate dynamically with deep unfolding principles\n",
        "def deep_unfolding_learning_rate_adjustment(obj_func_eval, gradients=None, client_id=None, layer=None):\n",
        "    global gradient_moving_avg, learning_rates, perturbations,meta_alpha, meta_epsilon, momentum\n",
        "\n",
        "    # Initialize moving average for gradients\n",
        "    if gradients is not None:\n",
        "        if not isinstance(gradient_moving_avg, np.ndarray) or gradient_moving_avg.size == 0:\n",
        "            gradient_moving_avg = gradients\n",
        "        else:\n",
        "            # Update moving average of gradients (Momentum)\n",
        "            gradient_moving_avg = momentum * gradient_moving_avg + (1 - momentum) * gradients\n",
        "        # Calculate the average gradient\n",
        "        avg_gradient = np.mean(gradient_moving_avg)\n",
        "\n",
        "        # Normalize delta_lr by L2 norm of the gradient\n",
        "        norm_gradient = np.linalg.norm(gradients)\n",
        "\n",
        "        '''\n",
        "        # Normalization to prevent instability\n",
        "        norm_gradient = gradients / (np.linalg.norm(gradients) + 1e-8)\n",
        "        avg_gradient = np.mean(norm_gradient)\n",
        "        '''\n",
        "        # Trainable scaling for deep unfolding (meta-parameter)\n",
        "        meta_alpha = 0.01  # This can be learned via a hypernetwork or meta-learning\n",
        "        meta_epsilon = 1e-6  # Small offset to ensure numerical stability\n",
        "        # Gradually adjust learning rate based on gradient signs and magnitude\n",
        "        # This formula gradually adds or subtracts from the learning rate instead of multiplication\n",
        "        delta_lr = meta_alpha * np.sign(avg_gradient) * np.sqrt(np.abs(avg_gradient) + meta_epsilon) / (norm_gradient + 1e-6)\n",
        "\n",
        "        #delta_lr = meta_alpha * np.sign(avg_gradient) * np.sqrt(np.abs(avg_gradient) + meta_epsilon)\n",
        "    # Apply gradual adjustment (either addition or subtraction based on the direction of the gradient)\n",
        "        if avg_gradient > 0:\n",
        "            delta_lr = delta_lr - 0.001  # Decrease if gradient is positive (potential overfitting)\n",
        "        else:\n",
        "            delta_lr = delta_lr + 0.001  # Increase if gradient is negative (potential underfitting)\n",
        "    else:\n",
        "        delta_lr = 0\n",
        "\n",
        "    # Compute new learning rate with clamping for stability\n",
        "    new_lr = max(0.001, min(5.0, learning_rates[-1] + delta_lr)) if learning_rates else initial_learning_rate\n",
        "\n",
        "    # Update per-client, per-layer information if federated\n",
        "    if client_id is not None and layer is not None:\n",
        "        if client_id not in client_data:\n",
        "            client_data[client_id] = {'federated_layers': {}}\n",
        "        if layer not in client_data[client_id]['federated_layers']:\n",
        "            client_data[client_id]['federated_layers'][layer] = {'objective_values': [], 'learning_rates': []}\n",
        "\n",
        "        # Store loss and learning rate for the specific client and layer\n",
        "        client_data[client_id]['federated_layers'][layer]['objective_values'].append(obj_func_eval)\n",
        "        client_data[client_id]['federated_layers'][layer]['learning_rates'].append(new_lr)\n",
        "\n",
        "    # Store global metrics\n",
        "    objective_func_vals.append(obj_func_eval)  # Store the loss value globally\n",
        "    learning_rates.append(new_lr)  # Append the new learning rate to the history\n",
        "\n",
        "    # Update meta-parameters (meta_alpha and meta_epsilon) using gradient descent\n",
        "    #meta_gradients = compute_meta_gradients(gradients, avg_gradient, delta_lr)\n",
        "    #meta_alpha -= meta_learning_rate * meta_gradients['alpha']\n",
        "    #meta_epsilon -= meta_learning_rate * meta_gradients['epsilon']\n",
        "\n",
        "    # Debug output for analysis\n",
        "    # print(f\"Objective Function Value: {obj_func_eval:.6f}, New Learning Rate: {new_lr:.6f}\")\n",
        "\n",
        "    return new_lr\n",
        "\n",
        "\n",
        "def callback_graph(weights, loss):\n",
        "    \"\"\"Callback to log and synchronize loss during training.\"\"\"\n",
        "    #print(f\"Loss = {loss}\")\n",
        "    if len(objective_func_vals) == 0 or loss != objective_func_vals[-1]:\n",
        "        objective_func_vals.append(loss)\n",
        "\n",
        "spsa_optimizer = SPSA(maxiter=50, learning_rate=0.01, perturbation = 0.15, callback=lambda nfev, params, obj_func_eval, stepsize, accept: deep_unfolding_learning_rate_adjustment(obj_func_eval, stepsize))\n",
        "\n",
        "\n",
        "\n",
        "# Define the CSV file path\n",
        "csv_file_path = 'federated_learning_accuracy.csv'\n",
        "\n",
        "# Open the CSV file in write mode and add headers (if starting fresh)\n",
        "with open(csv_file_path, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    # Write the header\n",
        "    writer.writerow(['Epoch', 'Global Accuracy'] + [f'Client {i+1} Final Accuracy' for i in range(num_clients)])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPn8nonXisKp"
      },
      "outputs": [],
      "source": [
        "from qiskit import QuantumCircuit\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit_machine_learning.circuit.library import QNNCircuit\n",
        "\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "\n",
        "num_qubits = 2\n",
        "\n",
        "def parity(x):\n",
        "    return f\"{bin(x)}\".count(\"1\") % 2\n",
        "\n",
        "# Using the QNNCircuit:\n",
        "# Create a parameterized 2 qubit circuit composed of the default ZZFeatureMap feature map\n",
        "# and RealAmplitudes ansatz.\n",
        "qnn_qc = QNNCircuit(num_qubits)\n",
        "\n",
        "qnn = SamplerQNN(\n",
        "    circuit=qnn_qc,\n",
        "    interpret=parity,\n",
        "    output_shape=2\n",
        ")\n",
        "\n",
        "qnn.forward(input_data=[1, 2], weights=[1, 2, 3, 4, 5, 6, 7, 8])\n",
        "\n",
        "# Explicitly specifying the ansatz and feature map:\n",
        "feature_map = ZZFeatureMap(feature_dimension=num_qubits)\n",
        "ansatz = RealAmplitudes(num_qubits=num_qubits)\n",
        "\n",
        "qc = QuantumCircuit(num_qubits)\n",
        "qc.compose(feature_map, inplace=True)\n",
        "qc.compose(ansatz, inplace=True)\n",
        "\n",
        "qnn = SamplerQNN(\n",
        "    circuit=qc,\n",
        "    input_params=feature_map.parameters,\n",
        "    weight_params=ansatz.parameters,\n",
        "    interpret=parity,\n",
        "    output_shape=2\n",
        ")\n",
        "\n",
        "qnn.forward(input_data=[1, 2], weights=[1, 2, 3, 4, 5, 6, 7, 8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV9T-8JBzwMo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "filename = 'accuracies.csv'\n",
        "data = pd.read_csv(filename)\n",
        "\n",
        "# Extract the relevant columns for plotting\n",
        "epochs = data['Epoch']\n",
        "global_accuracy = data['Global Accuracy']\n",
        "client_train_accuracies = data.filter(like='Train Accuracy').values\n",
        "client_test_accuracies = data.filter(like='Test Accuracy').values\n",
        "\n",
        "# Plot Global Accuracy over Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, global_accuracy, label='Global Accuracy', color='blue', marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Global Accuracy')\n",
        "plt.title('Global Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Train Accuracies for all clients over Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(client_train_accuracies.shape[1]):\n",
        "    plt.plot(epochs, client_train_accuracies[:, i], label=f'Client {i} Train Accuracy', marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Train Accuracy')\n",
        "plt.title('Train Accuracies for All Clients Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Accuracies for all clients over Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(client_test_accuracies.shape[1]):\n",
        "    plt.plot(epochs, client_test_accuracies[:, i], label=f'Client {i} Test Accuracy', marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('Test Accuracies for All Clients Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a_q7UqBTcju"
      },
      "source": [
        "new ways to average"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNFJIxH/7Ryz20Z1AtDjNEk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}