{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyMGZ9KYpHyBFn8JpHxkxmCf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanikairoshi/QFL-and-Deep-Unfolded-QFL/blob/main/Qiskit_QFL_with_qnn_genome_my_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdPhLlT9q3Np"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%capture\n",
        "!pip install genomic-benchmarks\n",
        "!pip install qiskit qiskit_machine_learning qiskit_algorithms\n",
        "!pip install qiskit-aer"
      ],
      "metadata": {
        "id": "Us3DV6Sfq6Mw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install qiskit\n",
        "!pip install qiskit_machine_learning\n",
        "!pip install qiskit-aer"
      ],
      "metadata": {
        "id": "er-_TE10rI2c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import DemoHumanOrWorm\n",
        "\n",
        "test_set = DemoHumanOrWorm(split='test', version=0)\n",
        "train_set = DemoHumanOrWorm(split='train', version=0)\n",
        "\n",
        "data_set = train_set\n",
        "# data_set = train_set + test_set\n",
        "len(data_set)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OTrftC6uZd_",
        "outputId": "1c86b685-65eb-426e-8ad5-98b7c1952143"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75000"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Nuber of samples in the test set: {len(test_set)}\")\n",
        "print(f\"Nuber of samples in the test set: {len(train_set)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_I2pWKEubYT",
        "outputId": "1a4d332e-b4ea-4824-ee21-c5cbf2352c80"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nuber of samples in the test set: 25000\n",
            "Nuber of samples in the test set: 75000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import DemoHumanOrWorm\n",
        "\n",
        "test_set = DemoHumanOrWorm(split='test', version=0)\n",
        "train_set = DemoHumanOrWorm(split='train', version=0)\n",
        "\n",
        "data_set = train_set\n",
        "# data_set = train_set + test_set\n",
        "len(data_set)\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "word_size = 40\n",
        "word_combinations = defaultdict(int)\n",
        "iteration = 1\n",
        "for text, _ in data_set:\n",
        "    for i in range(len(text)):\n",
        "        word = text[i:i+word_size]\n",
        "        if word_combinations.get(word) is None:\n",
        "          word_combinations[word] = iteration\n",
        "          iteration += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"First sample int the data_set variable: \")\n",
        "print(data_set[0])\n",
        "\n",
        "print(\"\\nFirst 5 samples in the word_combinations dict.\")\n",
        "for key, value in list(word_combinations.items())[:5]:\n",
        "    print(key, value)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "# Preprocess the training set\n",
        "np_data_set = []\n",
        "for i in range(len(data_set)):\n",
        "    sequence, label = data_set[i]\n",
        "    sequence = sequence.strip()  # Remove any leading/trailing whitespace\n",
        "    words = [sequence[i:i + word_size] for i in range(0, len(sequence), word_size)]  # Split the sequence into 4-letter words\n",
        "    int_sequence = np.array([word_combinations[word] for word in words])\n",
        "    data_point = {'sequence': int_sequence, 'label': label}\n",
        "    np_data_set.append(data_point)\n",
        "\n",
        "\n",
        "print(\"First 5 samples of encoded data:\")\n",
        "np_data_set[:5]\n",
        "\n",
        "\n",
        "np.random.shuffle(np_data_set)\n",
        "print(\"First 5 samples of encoded shuffled data:\")\n",
        "np_data_set[:5]\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "sequences = np.array([item['sequence'] for item in np_data_set])\n",
        "sequences = np.vstack(sequences)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "sequences_scaled = scaler.fit_transform(sequences)\n",
        "\n",
        "for i, item in enumerate(np_data_set):\n",
        "    item['sequence'] = sequences_scaled[i]\n",
        "\n",
        "print(\"First 5 samples of scaled encoded shuffled data:\")\n",
        "np_data_set[:5]\n",
        "\n",
        "\n",
        "np_train_data = np_data_set[:70000]\n",
        "np_test_data = np_data_set[-5000:]\n",
        "\n",
        "print(f\"Length of np_train_data: {len(np_train_data)}\")\n",
        "print(f\"Length of np_test_data: {len(np_test_data)}\")\n",
        "\n",
        "test_sequences = [data_point[\"sequence\"] for data_point in np_test_data]\n",
        "test_labels = [data_point[\"label\"] for data_point in np_test_data]\n",
        "test_sequences = np.array(test_sequences)\n",
        "test_labels = np.array(test_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl4qvHChudnt",
        "outputId": "362631c5-063e-46cb-81a6-f868b206e4be"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First sample int the data_set variable: \n",
            "('ATGTGCAGCTGTCTGTCTCCACATCTTCCCCTCCTGGATGCCTGTTCTCTGCCTTCTCTCTCCATCCATCCATGGAGATCACCTCTACTAAGGTCAGCGATAAACTTCATATCACTTATGTCTGCTGCTTCAGGTCCACCAACTTGGGTCACCCACTAGATTTGTAGTACCCTAAGGAGGATTTAGCACTAGGCTTAACG', 0)\n",
            "\n",
            "First 5 samples in the word_combinations dict.\n",
            "ATGTGCAGCTGTCTGTCTCCACATCTTCCCCTCCTGGATG 1\n",
            "TGTGCAGCTGTCTGTCTCCACATCTTCCCCTCCTGGATGC 2\n",
            "GTGCAGCTGTCTGTCTCCACATCTTCCCCTCCTGGATGCC 3\n",
            "TGCAGCTGTCTGTCTCCACATCTTCCCCTCCTGGATGCCT 4\n",
            "GCAGCTGTCTGTCTCCACATCTTCCCCTCCTGGATGCCTG 5\n",
            "First 5 samples of encoded data:\n",
            "First 5 samples of encoded shuffled data:\n",
            "First 5 samples of scaled encoded shuffled data:\n",
            "Length of np_train_data: 70000\n",
            "Length of np_test_data: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the structure of np_test_data\n",
        "for i, data_point in enumerate(np_test_data[:5]):  # Print the first 5 test data points\n",
        "    print(f\"Test data point {i}: {data_point}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NZX9M8wORaq",
        "outputId": "3a496aa8-8183-4aac-c56f-4f35c49bc0f4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data point 0: {'sequence': array([0.5360579, 0.5360579, 0.5360579, 0.5360579, 0.5360579]), 'label': 1}\n",
            "Test data point 1: {'sequence': array([0.35626312, 0.35626312, 0.35626312, 0.35626312, 0.35626312]), 'label': 0}\n",
            "Test data point 2: {'sequence': array([0.70987801, 0.70987801, 0.70987801, 0.70987801, 0.70987801]), 'label': 1}\n",
            "Test data point 3: {'sequence': array([0.77066302, 0.77066302, 0.77066302, 0.77066302, 0.77066302]), 'label': 1}\n",
            "Test data point 4: {'sequence': array([0.46368032, 0.46368032, 0.46368032, 0.46368032, 0.46368032]), 'label': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit_algorithms.optimizers import COBYLA\n",
        "from qiskit_machine_learning.algorithms.classifiers import VQC\n",
        "from qiskit.primitives import BackendSampler\n",
        "from functools import partial\n",
        "from qiskit_aer import Aer\n",
        "\n",
        "\n",
        "num_clients = 5\n",
        "num_epochs = 100\n",
        "max_train_iterations = 100\n",
        "samples_per_epoch=100\n",
        "backend = Aer.get_backend('aer_simulator')\n",
        "\n",
        "class Client:\n",
        "   def __init__(self, data, test_data):  # Add test_data to __init__\n",
        "        self.data = data\n",
        "        self.test_data = test_data  # Store test_data as an attribute\n",
        "        self.models = []\n",
        "        self.train_scores = []\n",
        "        self.test_scores = []\n",
        "        self.primary_model = None\n",
        "\n",
        "def split_dataset(num_clients, num_epochs, samples_per_epoch):\n",
        "  clients = []\n",
        "  for i in range(num_clients):\n",
        "    client_data = []\n",
        "    for j in range(num_epochs):\n",
        "      start_idx = (i*num_epochs*samples_per_epoch)+(j*samples_per_epoch)\n",
        "      end_idx = (i*num_epochs*samples_per_epoch)+((j+1)*samples_per_epoch)\n",
        "      client_data.append(np_train_data[start_idx:end_idx])\n",
        "    # Pass test_data when creating Client instances\n",
        "    clients.append(Client(client_data, np_test_data))\n",
        "  return clients\n",
        "\n",
        "clients = split_dataset(num_clients, num_epochs, samples_per_epoch)"
      ],
      "metadata": {
        "id": "vM9FXroZu1bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "clients[0].data[0][:3] #display the data for the first client and its first epoch."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oJfeKHWvFO4",
        "outputId": "b890cb8f-a5c4-480b-ee86-d58de53d86fb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'sequence': array([0.81627711, 0.81627711, 0.81627711, 0.81627711, 0.81627711]),\n",
              "  'label': 1},\n",
              " {'sequence': array([0.35218188, 0.35218188, 0.35218188, 0.35218188, 0.35218188]),\n",
              "  'label': 0},\n",
              " {'sequence': array([0.95511434, 0.95511434, 0.95511434, 0.95511434, 0.95511434]),\n",
              "  'label': 1}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "itr = 0\n",
        "def training_callback(weights, obj_func_eval):\n",
        "        global itr\n",
        "        itr += 1\n",
        "        print(f\"{itr}\", end=' | ')\n",
        "def train(data, model = None):\n",
        "  if model is None:\n",
        "    num_features = len(data[0][\"sequence\"])\n",
        "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
        "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "    optimizer = COBYLA(maxiter=max_train_iterations)\n",
        "    vqc_model = VQC(\n",
        "        feature_map=feature_map,\n",
        "        ansatz=ansatz,\n",
        "        optimizer=optimizer,\n",
        "        callback=partial(training_callback),\n",
        "        sampler=BackendSampler(backend=backend),\n",
        "        warm_start=True\n",
        "    )\n",
        "    model = vqc_model\n",
        "\n",
        "  train_sequences = [data_point[\"sequence\"] for data_point in data]\n",
        "  train_labels = [data_point[\"label\"] for data_point in data]\n",
        "\n",
        "  # Convert the lists to NumPy arrays\n",
        "  train_sequences = np.array(train_sequences)\n",
        "  train_labels = np.array(train_labels)\n",
        "\n",
        "  # Print the shapes\n",
        "  print(\"Train Sequences Shape:\", train_sequences.shape)\n",
        "  print(\"Train Labels Shape:\", train_labels.shape)\n",
        "\n",
        "  print(\"Training Started\")\n",
        "  start_time = time.time()\n",
        "  model.fit(train_sequences, train_labels)\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"\\nTraining complete. Time taken: {elapsed_time} seconds.\")\n",
        "\n",
        "  print(f\"SCORING MODEL\")\n",
        "  train_score_q = model.score(train_sequences, train_labels)\n",
        "  test_score_q = model.score(test_sequences[:200], test_labels[:200])\n",
        "  return train_score_q, test_score_q, model\n",
        "\n",
        "def getAccuracy(weights):\n",
        "        num_features = len(test_sequences[0])\n",
        "        feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
        "        ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "        ansatz = ansatz.bind_parameters(weights)\n",
        "        optimizer = COBYLA(maxiter=0)\n",
        "        vqc = VQC(\n",
        "            feature_map=feature_map,\n",
        "            ansatz=ansatz,\n",
        "            optimizer=optimizer,\n",
        "            sampler=BackendSampler(backend=backend)\n",
        "        )\n",
        "        vqc.fit(test_sequences[:25], test_labels[:25])\n",
        "        return vqc.score(test_sequences[:200], test_labels[:200])\n"
      ],
      "metadata": {
        "id": "Xaqhu6LrzJLh"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6_AV8hy8zY_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit.primitives import BackendSampler\n",
        "from qiskit_algorithms.optimizers import SPSA\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Callback function to visualize training progress\n",
        "objective_func_vals = []\n",
        "def callback_graph(weights, obj_func_eval):\n",
        "    clear_output(wait=True)\n",
        "    objective_func_vals.append(obj_func_eval)\n",
        "    plt.title(\"Objective function value against iteration\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Objective function value\")\n",
        "    plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vMGbsnFLzZhB"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit_algorithms.optimizers import COBYLA\n",
        "from qiskit_machine_learning.algorithms.classifiers import VQC\n",
        "from qiskit.primitives import BackendSampler\n",
        "from functools import partial\n",
        "from qiskit_aer import Aer\n",
        "\n",
        "\n",
        "num_clients = 5\n",
        "num_epochs = 50\n",
        "max_train_iterations = 100\n",
        "samples_per_epoch=100\n",
        "backend = Aer.get_backend('aer_simulator')\n",
        "\n",
        "class Client:\n",
        "   def __init__(self, data, test_data):  # Add test_data to __init__\n",
        "        self.data = data\n",
        "        self.test_data = test_data  # Store test_data as an attribute\n",
        "        self.models = []\n",
        "        self.train_scores = []\n",
        "        self.test_scores = []\n",
        "        self.primary_model = None\n",
        "'''\n",
        "def split_dataset(num_clients, num_epochs, samples_per_epoch):\n",
        "  clients = []\n",
        "  for i in range(num_clients):\n",
        "    client_data = []\n",
        "    for j in range(num_epochs):\n",
        "      start_idx = (i*num_epochs*samples_per_epoch)+(j*samples_per_epoch)\n",
        "      end_idx = (i*num_epochs*samples_per_epoch)+((j+1)*samples_per_epoch)\n",
        "      client_data.append(np_train_data[start_idx:end_idx])\n",
        "    # Pass test_data when creating Client instances\n",
        "    clients.append(Client(client_data, np_test_data))\n",
        "  return clients\n",
        "\n",
        "clients = split_dataset(num_clients, num_epochs, samples_per_epoch)\n",
        "'''\n",
        "def split_dataset(num_clients, num_epochs, samples_per_epoch):\n",
        "    clients = []\n",
        "    # Split test data across clients\n",
        "    test_samples_per_client = len(np_test_data) // num_clients\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        client_data = []\n",
        "        for j in range(num_epochs):\n",
        "            start_idx = (i * num_epochs * samples_per_epoch) + (j * samples_per_epoch)\n",
        "            end_idx = (i * num_epochs * samples_per_epoch) + ((j + 1) * samples_per_epoch)\n",
        "            client_data.append(np_train_data[start_idx:end_idx])\n",
        "\n",
        "        # Assign a subset of the test data to each client\n",
        "        test_start_idx = i * test_samples_per_client\n",
        "        test_end_idx = (i + 1) * test_samples_per_client\n",
        "        client_test_data = np_test_data[test_start_idx:test_end_idx]\n",
        "\n",
        "        # Create Client instance with both train and test data\n",
        "        clients.append(Client(client_data, client_test_data))\n",
        "\n",
        "    return clients\n",
        "\n",
        "clients = split_dataset(num_clients, num_epochs, samples_per_epoch)\n"
      ],
      "metadata": {
        "id": "obiD3RBs_SFy"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify test data distribution across clients\n",
        "for index, client in enumerate(clients):\n",
        "    print(f\"Client {index} Test Data Length: {len(client.test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxUIwXDEn3ig",
        "outputId": "0aa5cb51-d961-491d-dabc-b10bb941d08f"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 0 Test Data Length: 1000\n",
            "Client 1 Test Data Length: 1000\n",
            "Client 2 Test Data Length: 1000\n",
            "Client 3 Test Data Length: 1000\n",
            "Client 4 Test Data Length: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to initialize a new QNN model (same architecture as clients' models)\n",
        "def initialize_model(num_features):\n",
        "    # Create the same quantum neural network (QNN) architecture as clients\n",
        "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2)\n",
        "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "\n",
        "    # Combine the feature map and ansatz into a single quantum circuit\n",
        "    qc = feature_map.compose(ansatz)\n",
        "\n",
        "    # Use parity as the interpretation function\n",
        "    def parity(x):\n",
        "        return \"{:b}\".format(x).count(\"1\") % 2  # Binary classification\n",
        "\n",
        "    # Explicitly define input_params and weight_params\n",
        "    input_params = feature_map.parameters  # Input parameters (for encoding)\n",
        "    weight_params = ansatz.parameters      # Trainable parameters (for optimization)\n",
        "\n",
        "    # Define the QNN model using a sampler\n",
        "    sampler_qnn = SamplerQNN(\n",
        "        circuit=qc,\n",
        "        interpret=parity,\n",
        "        output_shape=2,  # Binary classification\n",
        "        input_params=input_params,\n",
        "        weight_params=weight_params\n",
        "    )\n",
        "\n",
        "    # Create a classifier using the neural network\n",
        "    qnn_classifier = NeuralNetworkClassifier(\n",
        "        neural_network=sampler_qnn,\n",
        "        optimizer=SPSA(maxiter=50)  # Use SPSA optimizer\n",
        "    )\n",
        "\n",
        "    return qnn_classifier\n",
        "\n",
        "# Function to create a model with averaged weights\n",
        "def create_model_with_weights(average_weights, num_features):\n",
        "    # Initialize a new QNN model with the same architecture\n",
        "    model = initialize_model(num_features)\n",
        "    # Assign the averaged weights to the model's trainable parameters (ansatz weights)\n",
        "    weight_params = model.neural_network.weight_params  # Get the trainable parameters\n",
        "\n",
        "    # Check if the lengths match, and truncate if necessary\n",
        "    num_weights = min(len(average_weights), len(weight_params))\n",
        "\n",
        "    # Create a dictionary mapping parameters to averaged weights\n",
        "    param_dict = {param: average_weights[i] for i, param in enumerate(weight_params[:num_weights])}\n",
        "\n",
        "\n",
        "    # Assign the averaged weights to the circuit parameters\n",
        "    model.neural_network.circuit.assign_parameters(param_dict)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "SE9YYRwPJDau"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit.primitives import BackendSampler\n",
        "from qiskit_algorithms.optimizers import SPSA\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Callback function to visualize training progress\n",
        "objective_func_vals = []\n",
        "def callback_graph(weights, obj_func_eval):\n",
        "    clear_output(wait=True)\n",
        "    objective_func_vals.append(obj_func_eval)\n",
        "    plt.title(\"Objective function value against iteration\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Objective function value\")\n",
        "    plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
        "    plt.show()\n",
        "\n",
        "# Function to create the QNN model\n",
        "def create_qnn_model(data_train):\n",
        "    num_features = data_train[0][\"sequence\"].shape[0]\n",
        "    # Define the quantum feature map and ansatz\n",
        "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2)\n",
        "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "\n",
        "    # Construct the quantum neural network using a sampler\n",
        "\n",
        "    qc = feature_map.compose(ansatz)  # Build the QNN circuit\n",
        "    print(f\"Number of features (input dimension): {num_features}\")\n",
        "    print(f\"Number of circuit parameters: {qc.num_parameters}\")\n",
        "\n",
        "    # Use parity as the interpretation function\n",
        "    def parity(x):\n",
        "        return \"{:b}\".format(x).count(\"1\") % 2  # Binary classification\n",
        "\n",
        "    # Explicitly define input_params and weight_params\n",
        "    input_params = feature_map.parameters  # Parameters for the feature map (inputs)\n",
        "    weight_params = ansatz.parameters      # Parameters for the ansatz (weights)\n",
        "    #print(f\"Input Parameters: {input_params}\")\n",
        "    #print(f\"Weight Parameters: {weight_params}\")\n",
        "\n",
        "    sampler_qnn = SamplerQNN(\n",
        "        circuit=qc,\n",
        "        interpret=parity,\n",
        "        output_shape=2,  # Output dimension for binary classification\n",
        "        input_params=input_params,       # Pass input parameters\n",
        "        weight_params=weight_params     # Pass weight parameters\n",
        "    )\n",
        "\n",
        "    # Define a classifier using the QNN\n",
        "    qnn_classifier = NeuralNetworkClassifier(\n",
        "        neural_network=sampler_qnn,\n",
        "        optimizer=SPSA(maxiter=50),  # Example with SPSA optimizer\n",
        "        #callback=callback_graph\n",
        "    )\n",
        "\n",
        "    return qnn_classifier\n"
      ],
      "metadata": {
        "id": "j-HD-sL2zgxh"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getAccuracy(weights, num_features, test_sequences, test_labels):\n",
        "    # Rebuild the QNN model with the given weights\n",
        "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
        "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "\n",
        "    # Replace bind_parameters with assign_parameters\n",
        "    # Create a parameter dictionary for assignment\n",
        "    param_dict = {param: weight for param, weight in zip(ansatz.parameters, weights)}\n",
        "    ansatz = ansatz.assign_parameters(param_dict)\n",
        "\n",
        "    # Rebuild the QNN using the updated ansatz\n",
        "    qc = feature_map.compose(ansatz)\n",
        "\n",
        "    # Define the parity function for binary classification\n",
        "    def parity(x):\n",
        "        return \"{:b}\".format(x).count(\"1\") % 2\n",
        "\n",
        "    # Build the SamplerQNN with the updated circuit\n",
        "    sampler_qnn = SamplerQNN(\n",
        "        circuit=qc,\n",
        "        interpret=parity,\n",
        "        output_shape=2,  # Binary classification\n",
        "        input_params=feature_map.parameters,  # Input parameters (from the feature map)\n",
        "        weight_params=ansatz.parameters  # Weight parameters (from the ansatz)\n",
        "    )\n",
        "\n",
        "    # Build the NeuralNetworkClassifier with the QNN\n",
        "    qnn_classifier = NeuralNetworkClassifier(\n",
        "        neural_network=sampler_qnn,\n",
        "        optimizer=COBYLA(maxiter=0)  # No need for further optimization\n",
        "    )\n",
        "\n",
        "    # Train the classifier on a subset of test data (or use full test data if preferred)\n",
        "    qnn_classifier.fit(test_sequences, test_labels)\n",
        "\n",
        "    # Return the accuracy on a larger test set\n",
        "    return qnn_classifier.score(test_sequences, test_labels)\n"
      ],
      "metadata": {
        "id": "IkECBNFLazNA"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "# Lists to store accuracies over epochs\n",
        "global_model_weights = {}\n",
        "global_model_accuracy = []\n",
        "clients_train_accuracies = []  # List to store train accuracies per epoch per client\n",
        "clients_test_accuracies = []   # List to store test accuracies per epoch per client\n",
        "\n",
        "# Function to train the QNN model for one client\n",
        "def train_qnn_model(client_data,client_test_data, model=None):\n",
        "    #num_features = client_data[0][\"sequence\"].shape[0]  # Get the feature dimension\n",
        "    # Debug: Print the client data structure\n",
        "    #for i, client in enumerate(clients):\n",
        "        #print(f\"Client {i} Test Data: {client.test_data}\")\n",
        "        #print(f\"Test Data Length: {len(client.test_data)}\")\n",
        "\n",
        "    if model is None:\n",
        "        # Create a new QNN model if one doesn't exist\n",
        "        model = create_qnn_model(client_data)\n",
        "\n",
        "    # Extract sequences and labels for training\n",
        "    train_sequences = [data_point[\"sequence\"] for data_point in client_data]\n",
        "    train_labels = [data_point[\"label\"] for data_point in client_data]\n",
        "\n",
        "    # Extract sequences and labels for testing\n",
        "    # Handle test data\n",
        "    if isinstance(client_test_data, dict):\n",
        "        # Single data point (dictionary format)\n",
        "        test_sequences = np.array([client_test_data[\"sequence\"]])\n",
        "        test_labels = np.array([client_test_data[\"label\"]])\n",
        "    else:\n",
        "        # List of dictionaries (multiple data points)\n",
        "        test_sequences = [data_point[\"sequence\"] for data_point in client_test_data]\n",
        "        test_labels = [data_point[\"label\"] for data_point in client_test_data]\n",
        "        test_sequences = np.array(test_sequences)\n",
        "        test_labels = np.array(test_labels)\n",
        "    # Debug: Print the first few test sequences and labels for verification\n",
        "    #print(\"First few test sequences:\", test_sequences[:2])\n",
        "    #print(\"First few test labels:\", test_labels[:2])\n",
        "    # Convert lists to NumPy arrays\n",
        "    train_sequences = np.array(train_sequences)\n",
        "    train_labels = np.array(train_labels)\n",
        "\n",
        "\n",
        "    print(\"Training started...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the QNN model\n",
        "    model.fit(train_sequences, train_labels)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Training completed in {elapsed_time} seconds.\")\n",
        "\n",
        "    # Evaluate the model on training and test data\n",
        "    train_score_q = model.score(train_sequences, train_labels)\n",
        "    test_score_q = model.score(test_sequences, test_labels)\n",
        "\n",
        "    return model, train_score_q, test_score_q, elapsed_time\n",
        "\n",
        "# Function to extract numerical values of parameters\n",
        "def extract_param_values(model):\n",
        "    param_values = []\n",
        "    # Loop through each parameter in the circuit and get its bound value\n",
        "    for param in model.neural_network.circuit.parameters:\n",
        "        # Extract the numerical value (assuming they are already bound with values)\n",
        "        bound_value = model.neural_network.circuit._parameters[param]\n",
        "        param_values.append(bound_value)\n",
        "    return np.array(param_values)\n",
        "\n",
        "# Function to set numerical values of parameters back into the circuit\n",
        "def set_param_values(model, param_values):\n",
        "    # Assign the averaged parameter values back to the circuit\n",
        "    parameter_dict = {param: value for param, value in zip(model.neural_network.circuit.parameters, param_values)}\n",
        "    model.neural_network.circuit.assign_parameters(parameter_dict)\n",
        "\n",
        "# Manually average the numerical values of the parameters across clients\n",
        "def manual_average_weights(epoch_weights):\n",
        "    # Initialize a list to store the summed weights (initialize with zeros)\n",
        "    num_weights = len(epoch_weights[0])  # Number of weights in the model\n",
        "    num_clients = len(epoch_weights)  # Number of clients\n",
        "\n",
        "    # Initialize sum of weights to zero (assuming NumPy array or list of weights)\n",
        "    summed_weights = np.zeros(num_weights)\n",
        "\n",
        "    # Sum the weights from all clients\n",
        "    for client_weights in epoch_weights:\n",
        "        summed_weights += np.array(client_weights)\n",
        "\n",
        "    # Compute the average by dividing the summed weights by the number of clients\n",
        "    average_weights = summed_weights / num_clients\n",
        "\n",
        "    return average_weights\n",
        "\n",
        "\n",
        "# Function to save accuracies to CSV\n",
        "def save_accuracies_to_csv(global_accuracies, clients_train_accuracies, clients_test_accuracies, filename='accuracies.csv'):\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write the header row\n",
        "        header = ['Epoch', 'Global Accuracy']\n",
        "        for i in range(len(clients_train_accuracies[0])):  # Assuming all clients have the same number of records\n",
        "            header.append(f'Client {i} Train Accuracy')\n",
        "            header.append(f'Client {i} Test Accuracy')\n",
        "        writer.writerow(header)\n",
        "\n",
        "        # Write the accuracy data for each epoch\n",
        "        for epoch in range(len(global_accuracies)):\n",
        "            row = [epoch, global_accuracies[epoch]]  # Start with epoch and global accuracy\n",
        "            for client_index in range(len(clients_train_accuracies[epoch])):\n",
        "                row.append(clients_train_accuracies[epoch][client_index])  # Add train accuracy for client\n",
        "                row.append(clients_test_accuracies[epoch][client_index])   # Add test accuracy for client\n",
        "            writer.writerow(row)\n",
        "\n",
        "# Federated learning loop\n",
        "num_features =5\n",
        "for epoch in range(num_epochs):\n",
        "    global_model_weights[epoch] = []\n",
        "    epoch_weights = []\n",
        "\n",
        "    epoch_train_accuracies = []  # Store train accuracies for this epoch\n",
        "    epoch_test_accuracies = []   # Store test accuracies for this epoch\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "\n",
        "    # Train each client\n",
        "    for index, client in enumerate(clients):\n",
        "        print(f\"Training Client {index}\")\n",
        "        #print(f\"Client {index} Test Data: {client.test_data}\")  # Print the test data for this client\n",
        "\n",
        "        if client.primary_model is None:\n",
        "            # Pass both training and test data to the training function\n",
        "            model, train_score_q, test_score_q, train_time = train_qnn_model(client.data[epoch], client.test_data)\n",
        "            client.primary_model = model\n",
        "        else:\n",
        "            model, train_score_q, test_score_q, train_time = train_qnn_model(client.data[epoch], client.test_data, model=client.primary_model)\n",
        "\n",
        "        client.models.append(model)\n",
        "        client.train_scores.append(train_score_q)\n",
        "        client.test_scores.append(test_score_q)\n",
        "\n",
        "        print(f\"Client {index} Train Score: {train_score_q}\")\n",
        "        print(f\"Client {index} Test Score: {test_score_q}\")\n",
        "        print(\"\\n\\n\")\n",
        "        print(\"----------------------------------------------------------\")\n",
        "\n",
        "        # Append client accuracies for this epoch\n",
        "        epoch_train_accuracies.append(train_score_q)\n",
        "        epoch_test_accuracies.append(test_score_q)\n",
        "\n",
        "        # Collect model weights\n",
        "        #epoch_weights.append(model.neural_network.circuit.parameters)\n",
        "        # Extract numerical values of parameters (as a NumPy array)''\n",
        "        #param_values = np.array([p for p in model.neural_network.circuit.parameters])\n",
        "        #epoch_weights.append(param_values)\n",
        "\n",
        "        # Collect model weights (modified)\n",
        "        # Extract numerical values of parameters (as a NumPy array)\n",
        "        '''\n",
        "        param_values = np.array([\n",
        "            client.primary_model.neural_network.circuit.parameters[i]  # Access parameters directly using index\n",
        "              for i in range(len(client.primary_model.neural_network.circuit.parameters))\n",
        "        ])\n",
        "        epoch_weights.append(param_values)\n",
        "        '''\n",
        "        # Assuming model.weights returns a NumPy array or list of weights\n",
        "        epoch_weights.append(model.weights)\n",
        "\n",
        "   # Manually compute the average weights\n",
        "    average_weights = manual_average_weights(epoch_weights)\n",
        "\n",
        "    # Update the global model weights\n",
        "    print(\"Global model updated\")\n",
        "    global_model_weights[epoch] = average_weights\n",
        "\n",
        "    # Create a new model with the averaged global weights\n",
        "    print(\"Create new model\")\n",
        "    new_model_with_global_weights = create_model_with_weights(global_model_weights[epoch],num_features)\n",
        "\n",
        "    # Update each client's primary model with the new global weights\n",
        "    for index, client in enumerate(clients):\n",
        "        client.primary_model = new_model_with_global_weights\n",
        "\n",
        "    # Calculate global accuracy using the test data\n",
        "    global_accuracy = getAccuracy(global_model_weights[epoch], num_features, test_sequences, test_labels)\n",
        "    global_model_accuracy.append(global_accuracy)\n",
        "\n",
        "    # Save the clients' train/test accuracies for this epoch\n",
        "    clients_train_accuracies.append(epoch_train_accuracies)\n",
        "    clients_test_accuracies.append(epoch_test_accuracies)\n",
        "\n",
        "    print(f\"Global Model Accuracy In Epoch {epoch}: {global_accuracy:.2f}\")\n",
        "    print(\"----------------------------------------------------------\")\n",
        "\n",
        "    # Save accuracies to CSV after each epoch (or at the end of all epochs)\n",
        "    save_accuracies_to_csv(global_model_accuracy, clients_train_accuracies, clients_test_accuracies, filename='accuracies.csv')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ucS5zEozmN5",
        "outputId": "dc156e86-28ef-44e0-8537-c29570a6fae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 178.9969024658203 seconds.\n",
            "Client 0 Train Score: 0.81\n",
            "Client 0 Test Score: 0.747\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 179.3487753868103 seconds.\n",
            "Client 1 Train Score: 0.88\n",
            "Client 1 Test Score: 0.839\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n",
            "Training completed in 173.55766487121582 seconds.\n",
            "Client 2 Train Score: 0.63\n",
            "Client 2 Test Score: 0.596\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Training started...\n",
            "Training completed in 173.86723971366882 seconds.\n",
            "Client 3 Train Score: 0.64\n",
            "Client 3 Test Score: 0.653\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Training started...\n",
            "Training completed in 175.14222645759583 seconds.\n",
            "Client 4 Train Score: 0.75\n",
            "Client 4 Test Score: 0.664\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Global model updated\n",
            "Create new model\n",
            "Global Model Accuracy In Epoch 0: 0.56\n",
            "----------------------------------------------------------\n",
            "Epoch: 1\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 179.40540957450867 seconds.\n",
            "Client 0 Train Score: 0.77\n",
            "Client 0 Test Score: 0.781\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 177.53029203414917 seconds.\n",
            "Client 1 Train Score: 0.85\n",
            "Client 1 Test Score: 0.861\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n",
            "Training completed in 182.22289156913757 seconds.\n",
            "Client 2 Train Score: 0.8\n",
            "Client 2 Test Score: 0.721\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Training started...\n",
            "Training completed in 173.3632252216339 seconds.\n",
            "Client 3 Train Score: 0.81\n",
            "Client 3 Test Score: 0.77\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Training started...\n",
            "Training completed in 176.1532120704651 seconds.\n",
            "Client 4 Train Score: 0.88\n",
            "Client 4 Test Score: 0.874\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Global model updated\n",
            "Create new model\n",
            "Global Model Accuracy In Epoch 1: 0.22\n",
            "----------------------------------------------------------\n",
            "Epoch: 2\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 178.70065188407898 seconds.\n",
            "Client 0 Train Score: 0.78\n",
            "Client 0 Test Score: 0.661\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 175.41316962242126 seconds.\n",
            "Client 1 Train Score: 0.8\n",
            "Client 1 Test Score: 0.754\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n",
            "Training completed in 179.64730191230774 seconds.\n",
            "Client 2 Train Score: 0.85\n",
            "Client 2 Test Score: 0.837\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Training started...\n",
            "Training completed in 176.6202585697174 seconds.\n",
            "Client 3 Train Score: 0.84\n",
            "Client 3 Test Score: 0.779\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Training started...\n",
            "Training completed in 179.01878452301025 seconds.\n",
            "Client 4 Train Score: 0.78\n",
            "Client 4 Test Score: 0.711\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Global model updated\n",
            "Create new model\n",
            "Global Model Accuracy In Epoch 2: 0.47\n",
            "----------------------------------------------------------\n",
            "Epoch: 3\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 180.1537561416626 seconds.\n",
            "Client 0 Train Score: 0.95\n",
            "Client 0 Test Score: 0.899\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 176.61992859840393 seconds.\n",
            "Client 1 Train Score: 0.85\n",
            "Client 1 Test Score: 0.851\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n",
            "Training completed in 177.76154780387878 seconds.\n",
            "Client 2 Train Score: 0.86\n",
            "Client 2 Test Score: 0.872\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Training started...\n",
            "Training completed in 178.11196994781494 seconds.\n",
            "Client 3 Train Score: 0.9\n",
            "Client 3 Test Score: 0.86\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Training started...\n",
            "Training completed in 177.80254006385803 seconds.\n",
            "Client 4 Train Score: 0.77\n",
            "Client 4 Test Score: 0.816\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Global model updated\n",
            "Create new model\n",
            "Global Model Accuracy In Epoch 3: 0.59\n",
            "----------------------------------------------------------\n",
            "Epoch: 4\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 178.19418907165527 seconds.\n",
            "Client 0 Train Score: 0.81\n",
            "Client 0 Test Score: 0.74\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 178.36261987686157 seconds.\n",
            "Client 1 Train Score: 0.7\n",
            "Client 1 Test Score: 0.726\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n",
            "Training completed in 175.63141751289368 seconds.\n",
            "Client 2 Train Score: 0.83\n",
            "Client 2 Test Score: 0.805\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Training started...\n",
            "Training completed in 176.14010095596313 seconds.\n",
            "Client 3 Train Score: 0.85\n",
            "Client 3 Test Score: 0.783\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Training started...\n",
            "Training completed in 177.1104030609131 seconds.\n",
            "Client 4 Train Score: 0.75\n",
            "Client 4 Test Score: 0.684\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Global model updated\n",
            "Create new model\n",
            "Global Model Accuracy In Epoch 4: 0.42\n",
            "----------------------------------------------------------\n",
            "Epoch: 5\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 180.72200298309326 seconds.\n",
            "Client 0 Train Score: 0.72\n",
            "Client 0 Test Score: 0.775\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 179.88999390602112 seconds.\n",
            "Client 1 Train Score: 0.79\n",
            "Client 1 Test Score: 0.623\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n",
            "Training completed in 180.51845836639404 seconds.\n",
            "Client 2 Train Score: 0.79\n",
            "Client 2 Test Score: 0.728\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Training started...\n",
            "Training completed in 183.07774949073792 seconds.\n",
            "Client 3 Train Score: 0.81\n",
            "Client 3 Test Score: 0.802\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Training started...\n",
            "Training completed in 177.9153847694397 seconds.\n",
            "Client 4 Train Score: 0.85\n",
            "Client 4 Test Score: 0.79\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Global model updated\n",
            "Create new model\n",
            "Global Model Accuracy In Epoch 5: 0.63\n",
            "----------------------------------------------------------\n",
            "Epoch: 6\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 176.53635239601135 seconds.\n",
            "Client 0 Train Score: 0.8\n",
            "Client 0 Test Score: 0.742\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 179.82131457328796 seconds.\n",
            "Client 1 Train Score: 0.93\n",
            "Client 1 Test Score: 0.87\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "new ways to average"
      ],
      "metadata": {
        "id": "9a_q7UqBTcju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_model_weights = {}\n",
        "global_model_accuracy = []\n",
        "\n",
        "# Federated learning loop\n",
        "for epoch in range(num_epochs):\n",
        "    global_model_weights[epoch] = []\n",
        "    epoch_weights = []\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "\n",
        "    # Train each client\n",
        "    for index, client in enumerate(clients):\n",
        "        print(f\"Training Client {index}\")\n",
        "\n",
        "        if client.primary_model is None:\n",
        "            # First time training: no existing model\n",
        "            train_score_q, test_score_q, model = train(data=client.data[epoch])\n",
        "            client.primary_model = model\n",
        "        else:\n",
        "            # Continue training with the existing model\n",
        "            train_score_q, test_score_q, model = train(data=client.data[epoch], model=client.primary_model)\n",
        "\n",
        "        # Save model and scores\n",
        "        client.models.append(model)\n",
        "        client.train_scores.append(train_score_q)\n",
        "        client.test_scores.append(test_score_q)\n",
        "\n",
        "        # Extract and collect model weights\n",
        "        print(f\"Train Score: {train_score_q}\")\n",
        "        print(f\"Test Score: {test_score_q}\")\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "        # Assuming model.weights returns a NumPy array or list of weights\n",
        "        epoch_weights.append(model.weights)\n",
        "\n",
        "    # Average the weights across all clients\n",
        "    average_weights = sum(epoch_weights) / len(epoch_weights)\n",
        "\n",
        "    # Create a new model with the averaged global weights\n",
        "    global_model_weights[epoch] = average_weights\n",
        "    new_model_with_global_weights = create_model_with_weights(global_model_weights[epoch])\n",
        "\n",
        "    # Update each client's primary model with the global averaged weights\n",
        "    for index, client in enumerate(clients):\n",
        "        client.primary_model = new_model_with_global_weights\n",
        "\n",
        "    # Optionally calculate global accuracy (if applicable in your case)\n",
        "    global_accuracy = getAccuracy(global_model_weights[epoch])  # Assuming getAccuracy() works with global weights\n",
        "    global_model_accuracy.append(global_accuracy)\n",
        "\n",
        "    print(f\"Global Model Accuracy In Epoch {epoch}: {global_accuracy:.2f}\")\n",
        "    print(\"----------------------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "Dd3Im7PfTkl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract numerical values of parameters\n",
        "def extract_param_values(model):\n",
        "    param_values = []\n",
        "    # Loop through each parameter in the circuit and get its bound value\n",
        "    for param in model.neural_network.circuit.parameters:\n",
        "        # Extract the numerical value (assuming they are already bound with values)\n",
        "        bound_value = model.neural_network.circuit._parameters[param]\n",
        "        param_values.append(bound_value)\n",
        "    return np.array(param_values)\n",
        "\n",
        "# Function to set numerical values of parameters back into the circuit\n",
        "def set_param_values(model, param_values):\n",
        "    # Assign the averaged parameter values back to the circuit\n",
        "    parameter_dict = {param: value for param, value in zip(model.neural_network.circuit.parameters, param_values)}\n",
        "    model.neural_network.circuit.assign_parameters(parameter_dict)\n",
        "\n",
        "# Federated learning loop\n",
        "for epoch in range(num_epochs):\n",
        "    global_model_weights[epoch] = []\n",
        "    epoch_weights = []\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "\n",
        "    # Train each client\n",
        "    for index, client in enumerate(clients):\n",
        "        print(f\"Training Client {index}\")\n",
        "\n",
        "        if client.primary_model is None:\n",
        "            # Pass both training and test data to the training function\n",
        "            model, train_score_q, test_score_q, train_time = train_qnn_model(client.data[epoch], client.test_data[epoch])\n",
        "            client.primary_model = model\n",
        "        else:\n",
        "            model, train_score_q, test_score_q, train_time = train_qnn_model(client.data[epoch], client.test_data[epoch], model=client.primary_model)\n",
        "\n",
        "        client.models.append(model)\n",
        "        client.train_scores.append(train_score_q)\n",
        "        client.test_scores.append(test_score_q)\n",
        "\n",
        "        print(f\"Client {index} Train Score: {train_score_q}\")\n",
        "        print(f\"Client {index} Test Score: {test_score_q}\")\n",
        "        print(\"  \")\n",
        "        print(\"----------------------------------------------------------\")\n",
        "\n",
        "        # Collect model weights (extract numerical parameter values)\n",
        "        param_values = extract_param_values(client.primary_model)\n",
        "        epoch_weights.append(param_values)\n",
        "\n",
        "    # Average the numerical values of the parameters across clients\n",
        "    average_weights = np.mean(epoch_weights, axis=0)\n",
        "\n",
        "    # Manually update each client's model parameters with the averaged weights\n",
        "    global_model_weights[epoch] = average_weights\n",
        "\n",
        "    for client in clients:\n",
        "        # Manually set the global averaged weights to the model's parameters\n",
        "        set_param_values(client.primary_model, global_model_weights[epoch])\n",
        "\n",
        "    # Calculate global accuracy\n",
        "    global_accuracy = getAccuracy(global_model_weights[epoch])\n",
        "\n",
        "    # Print global accuracy for the epoch\n",
        "    print(f\"Global Model Accuracy In Epoch {epoch}: {global_accuracy:.2f}\")\n",
        "    print(\"----------------------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "skaTQsAsTbbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33333333333333333333333333333333333333333333"
      ],
      "metadata": {
        "id": "MK8swGngN3RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_model_weights = {}\n",
        "global_model_accuracy = []\n",
        "\n",
        "# Function to train the QNN model for one client\n",
        "def train_qnn_model(client_data,client_test_data, model=None):\n",
        "    #num_features = client_data[0][\"sequence\"].shape[0]  # Get the feature dimension\n",
        "    # Debug: Print the client data structure\n",
        "    print(\"Client Test Data Structure:\", client_test_data)\n",
        "    if model is None:\n",
        "        # Create a new QNN model if one doesn't exist\n",
        "        model = create_qnn_model(client_data)\n",
        "\n",
        "    # Extract sequences and labels for training\n",
        "    train_sequences = [data_point[\"sequence\"] for data_point in client_data]\n",
        "    train_labels = [data_point[\"label\"] for data_point in client_data]\n",
        "\n",
        "    # Extract sequences and labels for testing\n",
        "    # Handle test data\n",
        "    if isinstance(client_test_data, dict):\n",
        "        # Single data point (dictionary format)\n",
        "        test_sequences = np.array([client_test_data[\"sequence\"]])\n",
        "        test_labels = np.array([client_test_data[\"label\"]])\n",
        "    else:\n",
        "        # List of dictionaries (multiple data points)\n",
        "        test_sequences = [data_point[\"sequence\"] for data_point in client_test_data]\n",
        "        test_labels = [data_point[\"label\"] for data_point in client_test_data]\n",
        "        test_sequences = np.array(test_sequences)\n",
        "        test_labels = np.array(test_labels)\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    train_sequences = np.array(train_sequences)\n",
        "    train_labels = np.array(train_labels)\n",
        "\n",
        "\n",
        "    print(\"Training started...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the QNN model\n",
        "    model.fit(train_sequences, train_labels)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Training completed in {elapsed_time} seconds.\")\n",
        "\n",
        "    # Evaluate the model on training and test data\n",
        "    train_score_q = model.score(train_sequences, train_labels)\n",
        "    test_score_q = model.score(test_sequences, test_labels)\n",
        "\n",
        "    return model, train_score_q, test_score_q, elapsed_time\n",
        "\n",
        "# Federated learning loop\n",
        "for epoch in range(num_epochs):\n",
        "    global_model_weights[epoch] = []\n",
        "    epoch_weights = []\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "\n",
        "    # Train each client\n",
        "    for index, client in enumerate(clients):\n",
        "        print(f\"Training Client {index}\")\n",
        "\n",
        "        if client.primary_model is None:\n",
        "            # Pass both training and test data to the training function\n",
        "            model, train_score_q, test_score_q, train_time = train_qnn_model(client.data[epoch], client.test_data[epoch])\n",
        "            client.primary_model = model\n",
        "        else:\n",
        "            model, train_score_q, test_score_q, train_time = train_qnn_model(client.data[epoch], client.test_data[epoch], model=client.primary_model)\n",
        "\n",
        "        client.models.append(model)\n",
        "        client.train_scores.append(train_score_q)\n",
        "        client.test_scores.append(test_score_q)\n",
        "\n",
        "        print(f\"Client {index} Train Score: {train_score_q}\")\n",
        "        print(f\"Client {index} Test Score: {test_score_q}\")\n",
        "        print(\"  \")\n",
        "        print(\"----------------------------------------------------------\")\n",
        "\n",
        "        for model in client.models:\n",
        "        # Extract numerical values of parameters (as a NumPy array)\n",
        "            param_values = [p.value for p in model.neural_network.circuit.parameters] # Extract the values\n",
        "            epoch_weights.append(param_values)\n",
        "\n",
        "    # Average the numerical values of the parameters across clients\n",
        "    average_weights = np.mean(epoch_weights, axis=0)\n",
        "\n",
        "    # Manually update each client's model parameters with the averaged weights\n",
        "    global_model_weights[epoch] = average_weights\n",
        "\n",
        "    for client in clients:\n",
        "        # Manually set the global averaged weights to the model's parameters\n",
        "        for i, param in enumerate(client.primary_model.neural_network.circuit.parameters):\n",
        "            client.primary_model.neural_network.circuit._parameters[param] = average_weights[i] # Set the value directly\n",
        "\n",
        "\n",
        "    # Calculate global accuracy\n",
        "    global_accuracy = getAccuracy(global_model_weights[epoch])\n",
        "\n",
        "    # Print global accuracy for the epoch\n",
        "    print(f\"Global Model Accuracy In Epoch {epoch}: {global_accuracy:.2f}\")\n",
        "    print(\"----------------------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "vUX7LWLUIvFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "\n",
        "itr = 0\n",
        "def training_callback(weights, obj_func_eval):\n",
        "        global itr\n",
        "        itr += 1\n",
        "        print(f\"{itr}\", end=' | ')\n",
        "def train(data, model = None):\n",
        "  if model is None:\n",
        "    num_features = len(data[0][\"sequence\"])\n",
        "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
        "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "    optimizer = COBYLA(maxiter=max_train_iterations)\n",
        "    vqc_model = VQC(\n",
        "        feature_map=feature_map,\n",
        "        ansatz=ansatz,\n",
        "        optimizer=optimizer,\n",
        "        callback=partial(training_callback),\n",
        "        sampler=BackendSampler(backend=backend),\n",
        "        warm_start=True\n",
        "    )\n",
        "    model = vqc_model\n",
        "\n",
        "  train_sequences = [data_point[\"sequence\"] for data_point in data]\n",
        "  train_labels = [data_point[\"label\"] for data_point in data]\n",
        "\n",
        "  # Convert the lists to NumPy arrays\n",
        "  train_sequences = np.array(train_sequences)\n",
        "  train_labels = np.array(train_labels)\n",
        "\n",
        "  # Print the shapes\n",
        "  print(\"Train Sequences Shape:\", train_sequences.shape)\n",
        "  print(\"Train Labels Shape:\", train_labels.shape)\n",
        "\n",
        "  print(\"Training Started\")\n",
        "  start_time = time.time()\n",
        "  model.fit(train_sequences, train_labels)\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"\\nTraining complete. Time taken: {elapsed_time} seconds.\")\n",
        "\n",
        "  print(f\"SCORING MODEL\")\n",
        "  train_score_q = model.score(train_sequences, train_labels)\n",
        "  test_score_q = model.score(test_sequences[:200], test_labels[:200])\n",
        "  return train_score_q, test_score_q, model\n",
        "\n",
        "def getAccuracy(weights):\n",
        "        num_features = len(test_sequences[0])\n",
        "        feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
        "        ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "        ansatz = ansatz.bind_parameters(weights)\n",
        "        optimizer = COBYLA(maxiter=0)\n",
        "        vqc = VQC(\n",
        "            feature_map=feature_map,\n",
        "            ansatz=ansatz,\n",
        "            optimizer=optimizer,\n",
        "            sampler=BackendSampler(backend=backend)\n",
        "        )\n",
        "        vqc.fit(test_sequences[:25], test_labels[:25])\n",
        "        return vqc.score(test_sequences[:200], test_labels[:200])\n",
        "\n",
        "def create_model_with_weights(weights):\n",
        "  num_features = len(test_sequences[0])\n",
        "  feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
        "  ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "  optimizer = COBYLA(maxiter=max_train_iterations)\n",
        "  vqc = VQC(\n",
        "      feature_map=feature_map,\n",
        "      ansatz=ansatz,\n",
        "      optimizer=optimizer,\n",
        "      sampler=BackendSampler(backend=backend),\n",
        "      warm_start = True,\n",
        "      initial_point  = weights,\n",
        "      callback=partial(training_callback)\n",
        "  )\n",
        "  return vqc\n",
        "\n",
        "\n",
        "global_model_weights = {}\n",
        "global_model_accuracy = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  global_model_weights[epoch] = []\n",
        "  epoch_weights = []\n",
        "  print(f\"epoch: {epoch}\")\n",
        "  for index, client in enumerate(clients):\n",
        "    print(f\"Index: {index}, Client: {client}\")\n",
        "\n",
        "    if client.primary_model is None:\n",
        "      train_score_q, test_score_q, model = train(data = client.data[epoch])\n",
        "      client.models.append(model)\n",
        "      client.test_scores.append(test_score_q)\n",
        "      client.train_scores.append(train_score_q)\n",
        "      # Print the values\n",
        "      print(\"Train Score:\", train_score_q)\n",
        "      print(\"Test Score:\", test_score_q)\n",
        "      print(\"\\n\\n\")\n",
        "      epoch_weights.append(model.weights)\n",
        "\n",
        "    else:\n",
        "      train_score_q, test_score_q, model = train(data = client.data[epoch], model = client.primary_model)\n",
        "      client.models.append(model)\n",
        "      client.test_scores.append(test_score_q)\n",
        "      client.train_scores.append(train_score_q)\n",
        "      print(\"Train Score:\", train_score_q)\n",
        "      print(\"Test Score:\", test_score_q)\n",
        "      print(\"\\n\\n\")\n",
        "      epoch_weights.append(model.weights)\n",
        "\n",
        "\n",
        "if(epoch != 0):\n",
        "    epoch_weights.append(global_model_weights[epoch-1])\n",
        "average_weights = sum(epoch_weights) / len(epoch_weights)\n",
        "\n",
        "global_model_weights[epoch] = average_weights\n",
        "new_model_with_global_weights = create_model_with_weights(global_model_weights[epoch])\n",
        "for index, client in enumerate(clients):\n",
        "  client.primary_model = new_model_with_global_weights\n",
        "\n",
        "global_accuracy = getAccuracy(global_model_weights[epoch])\n",
        "print(f\"Global Model Accuracy In Epoch {epoch}: {global_accuracy}\")\n",
        "print(\"----------------------------------------------------------\")\n",
        "global_model_accuracy.append(global_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "bqzOZRiKN2FE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}