{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPpbtzRfWB2KLf3hCVyb9bI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanikairoshi/QFL-with-DUN/blob/main/Qiskit_VQC_FL_Fashion_MNIST_Experiemental_Anlysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xVJogN-3d6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f9a03a2-cf9c-43ee-dc11-194d7d44cef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qiskit\n",
            "  Downloading qiskit-0.44.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting qiskit-terra==0.25.1 (from qiskit)\n",
            "  Downloading qiskit_terra-0.25.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rustworkx>=0.13.0 (from qiskit-terra==0.25.1->qiskit)\n",
            "  Downloading rustworkx-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.25.1->qiskit) (1.23.5)\n",
            "Collecting ply>=3.10 (from qiskit-terra==0.25.1->qiskit)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.25.1->qiskit) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.25.1->qiskit) (1.11.2)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.25.1->qiskit) (1.12)\n",
            "Collecting dill>=0.3 (from qiskit-terra==0.25.1->qiskit)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.25.1->qiskit) (2.8.2)\n",
            "Collecting stevedore>=3.0.0 (from qiskit-terra==0.25.1->qiskit)\n",
            "  Downloading stevedore-5.1.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting symengine<0.10,>=0.9 (from qiskit-terra==0.25.1->qiskit)\n",
            "  Downloading symengine-0.9.2-cp310-cp310-manylinux2010_x86_64.whl (37.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.5/37.5 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.25.1->qiskit) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit-terra==0.25.1->qiskit) (1.16.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0 (from stevedore>=3.0.0->qiskit-terra==0.25.1->qiskit)\n",
            "  Downloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit-terra==0.25.1->qiskit) (1.3.0)\n",
            "Installing collected packages: ply, symengine, rustworkx, pbr, dill, stevedore, qiskit-terra, qiskit\n",
            "Successfully installed dill-0.3.7 pbr-5.11.1 ply-3.11 qiskit-0.44.1 qiskit-terra-0.25.1 rustworkx-0.13.1 stevedore-5.1.0 symengine-0.9.2\n",
            "Collecting qiskit.aer\n",
            "  Downloading qiskit_aer-0.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: qiskit-terra>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from qiskit.aer) (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from qiskit.aer) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from qiskit.aer) (1.11.2)\n",
            "Requirement already satisfied: rustworkx>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.21.0->qiskit.aer) (0.13.1)\n",
            "Requirement already satisfied: ply>=3.10 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.21.0->qiskit.aer) (3.11)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.21.0->qiskit.aer) (5.9.5)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.21.0->qiskit.aer) (1.12)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.21.0->qiskit.aer) (0.3.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.21.0->qiskit.aer) (2.8.2)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.21.0->qiskit.aer) (5.1.0)\n",
            "Requirement already satisfied: symengine<0.10,>=0.9 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.21.0->qiskit.aer) (0.9.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.21.0->qiskit.aer) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit-terra>=0.21.0->qiskit.aer) (1.16.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from stevedore>=3.0.0->qiskit-terra>=0.21.0->qiskit.aer) (5.11.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit-terra>=0.21.0->qiskit.aer) (1.3.0)\n",
            "Installing collected packages: qiskit.aer\n",
            "Successfully installed qiskit.aer-0.12.2\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install qiskit\n",
        "!pip install qiskit.aer\n",
        "#Install required Dependencies\n",
        "!pip install --upgrade seaborn\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install --upgrade matplotlib\n",
        "!pip install --upgrade pandas\n",
        "!pip install --upgrade qiskit\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.manifold import TSNE\n",
        "from qiskit import *\n",
        "import numpy as np\n",
        "from qiskit.utils import algorithm_globals\n",
        "!pip install qiskit_machine_learning\n",
        "\n",
        "import time\n",
        "# installing TensorFLow Version 2.3.1\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qiskit import QuantumCircuit, Aer, execute\n",
        "from qiskit.circuit import Parameter\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from qiskit.utils import algorithm_globals\n",
        "import numpy as np\n",
        "#from qiskit.circuit.library import RealAmplitudes\n",
        "from qiskit.circuit.library import ZZFeatureMap, TwoLocal, RealAmplitudes\n",
        "from qiskit.circuit.library import ZZFeatureMap, ZFeatureMap, PauliFeatureMap\n",
        "#from qiskit_machine_learning.datasets import ad_hoc_data\n",
        "from qiskit_machine_learning.algorithms.classifiers import VQC\n",
        "#from qiskit_machine_learning.optimizers import SPSA\n",
        "from qiskit import BasicAer\n",
        "from qiskit import BasicAer, execute\n",
        "from qiskit.algorithms.optimizers import SPSA\n"
      ],
      "metadata": {
        "id": "ickqAraJIkFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "Ut8cwfCrI5ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the Fashion MNIST dataset from keras\n",
        "from tensorflow.keras.datasets import fashion_mnist as dataset\n",
        "(X_train, y_train), (X_test, y_test) = dataset.load_data()\n",
        "# Load the FeMNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "4BA-CSzeIkOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Reshape the data if needed (e.g., flattening the images)\n",
        "# x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "# x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# Create DataFrames\n",
        "train_df = pd.DataFrame({'label': y_train})\n",
        "test_df = pd.DataFrame({'label': y_test})\n",
        "\n",
        "# Add pixel values to DataFrames\n",
        "for i in range(x_train.shape[1] * x_train.shape[2]):\n",
        "    train_df[f'pixel_{i}'] = x_train[:, i // x_train.shape[2], i % x_train.shape[2]]\n",
        "    test_df[f'pixel_{i}'] = x_test[:, i // x_test.shape[2], i % x_test.shape[2]]\n",
        "\n",
        "# Save DataFrames to CSV\n",
        "train_df.to_csv('fashion_mnist_train.csv', index=False)\n",
        "test_df.to_csv('fashion_mnist_test.csv', index=False)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "9grxI9T5IvTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 28 # width and length are equal\n",
        "data_path= \"/content/\"\n",
        "train_data = np.loadtxt(data_path + \"fashion_mnist_train.csv\", delimiter=\",\",skiprows=1)\n",
        "test_data = np.loadtxt(data_path + \"fashion_mnist_test.csv\", delimiter=\",\",skiprows=1)"
      ],
      "metadata": {
        "id": "dXnLXvfdJ-Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = train_data[:, 1:][1].reshape((image_size, image_size))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9XveFtqEIkZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting features and labels from the dataset and truncating the dataset to 10,000 datapoints\n",
        "train_data_features = train_data[:10000, 1:]\n",
        "train_data_labels = train_data[:10000, :1].reshape(10000,)\n",
        "\n",
        "# Using SVD to reduce dimensions to 10\n",
        "tsvd = TruncatedSVD(n_components=10)\n",
        "X_SVD = tsvd.fit_transform(train_data_features)\n",
        "\n",
        "# Use t-SNE technique to reduce dimensions to 2\n",
        "np.random.seed(0)\n",
        "tsne = TSNE(n_components=2)\n",
        "train_data_features_reduced = tsne.fit_transform(X_SVD)"
      ],
      "metadata": {
        "id": "uIyVmZTNK0Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to help plot the 2-D dataset\n",
        "def plotdataset(X, Y, c1, c2, N):\n",
        "    lbl1 = f'Component {c1}'\n",
        "    lbl2 = f'Component {c2}'\n",
        "    df = pd.DataFrame({lbl1:X[:N,c1], lbl2:X[:N,c2], 'label':Y[:N]})\n",
        "    sns.lmplot(data=df, x=lbl1, y=lbl2, fit_reg=False, hue='label', scatter_kws={'alpha':0.5})\n",
        "plotdataset(train_data_features_reduced, train_data_labels, 0, 1, N=2000)"
      ],
      "metadata": {
        "id": "I900jjLeK5SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_datapoints_array = [] #an array of the data points containing value 0\n",
        "one_datapoints_array = []# an array of the data points containing value 1\n",
        "for i in range(10000):\n",
        "    if train_data_labels[i] == 0:                   # extracting zeros\n",
        "        zero_datapoints_array.append(train_data_features_reduced[i])\n",
        "\n",
        "for i in range(10000):\n",
        "    if train_data_labels[i] == 1:                   # extracting ones\n",
        "        one_datapoints_array.append(train_data_features_reduced[i])\n",
        "\n",
        "zero_datapoints_array = np.array(zero_datapoints_array)\n",
        "one_datapoints_array = np.array(one_datapoints_array)\n",
        "\n",
        "def normalize(arr, max_val, n):\n",
        "    a = np.divide(arr, max_val)\n",
        "    return a + n\n",
        "zero_datapoints_normalized = normalize(zero_datapoints_array, 100, 1)\n",
        "one_datapoints_normalized = normalize(one_datapoints_array, 100, 1)"
      ],
      "metadata": {
        "id": "2Q59UKkLqH_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_size = 200\n",
        "test_size = 50\n",
        "dp_size_zero = 5\n",
        "dp_size_one = 5\n",
        "\n",
        "zero_train = zero_datapoints_normalized[:train_size]\n",
        "one_train = one_datapoints_normalized[:train_size]\n",
        "\n",
        "zero_test = zero_datapoints_normalized[train_size + 1:train_size + test_size + 1]\n",
        "one_test = one_datapoints_normalized[train_size + 1:train_size + test_size + 1]\n",
        "\n",
        "training_input = {'A':zero_train, 'B':one_train}\n",
        "test_input = {'A':zero_test, 'B':one_test}\n",
        "\n",
        "# datapoints is our validation set\n",
        "datapoints = []\n",
        "dp_zero = zero_datapoints_normalized[train_size + test_size + 2:train_size + test_size + 2 + dp_size_zero]\n",
        "dp_one = one_datapoints_normalized[train_size + test_size + 2:train_size + test_size + 2 + dp_size_one]\n",
        "datapoints.append(np.concatenate((dp_zero, dp_one)))\n",
        "dp_y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
        "datapoints.append(dp_y)\n",
        "\n",
        "class_to_label = {'A': 0, 'B': 1}"
      ],
      "metadata": {
        "id": "jyTb9GbJNP9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming you have the data defined as per your initial code\n",
        "train_size = 200\n",
        "test_size = 50\n",
        "dp_size_zero = 5\n",
        "dp_size_one = 5\n",
        "\n",
        "zero_train = zero_datapoints_normalized[:train_size]\n",
        "one_train = one_datapoints_normalized[:train_size]\n",
        "\n",
        "zero_test = zero_datapoints_normalized[train_size + 1:train_size + test_size + 1]\n",
        "one_test = one_datapoints_normalized[train_size + 1:train_size + test_size + 1]\n",
        "\n",
        "training_input = {'A': zero_train, 'B': one_train}\n",
        "test_input = {'A': zero_test, 'B': one_test}\n",
        "\n",
        "datapoints = []\n",
        "dp_zero = zero_datapoints_normalized[train_size + test_size + 2:train_size + test_size + 2 + dp_size_zero]\n",
        "dp_one = one_datapoints_normalized[train_size + test_size + 2:train_size + test_size + 2 + dp_size_one]\n",
        "datapoints.append(np.concatenate((dp_zero, dp_one)))\n",
        "dp_y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
        "datapoints.append(dp_y)\n",
        "\n",
        "class_to_label = {'A': 0, 'B': 1}\n",
        "\n",
        "# Split data for 5 clients\n",
        "num_clients = 5\n",
        "client_data = {f'Client_{i+1}': {'TRAIN_DATA': [], 'TRAIN_LABELS': [], 'TEST_DATA': [], 'TEST_LABELS': []} for i in range(num_clients)}\n",
        "\n",
        "for i in range(num_clients):\n",
        "    start_idx = i * (train_size // num_clients)\n",
        "    end_idx = (i + 1) * (train_size // num_clients)\n",
        "\n",
        "    client_data[f'Client_{i+1}']['TRAIN_DATA'] = np.concatenate((zero_train[start_idx:end_idx], one_train[start_idx:end_idx]))\n",
        "    client_data[f'Client_{i+1}']['TRAIN_LABELS'] = np.array([class_to_label['A']] * (end_idx - start_idx) + [class_to_label['B']] * (end_idx - start_idx))\n",
        "\n",
        "    start_idx = i * (test_size // num_clients)\n",
        "    end_idx = (i + 1) * (test_size // num_clients)\n",
        "\n",
        "    client_data[f'Client_{i+1}']['TEST_DATA'] = np.concatenate((zero_test[start_idx:end_idx], one_test[start_idx:end_idx]))\n",
        "    client_data[f'Client_{i+1}']['TEST_LABELS'] = np.array([class_to_label['A']] * (end_idx - start_idx) + [class_to_label['B']] * (end_idx - start_idx))\n",
        "\n",
        "# Print client data\n",
        "#for client, data in client_data.items():\n",
        "    #print(f'{client}:')\n",
        "    #print(data)\n",
        "    #print()\n"
      ],
      "metadata": {
        "id": "DMaePu0tNXKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test code below"
      ],
      "metadata": {
        "id": "knqhamKf6DGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "algorithm_globals.random_seed = 3142\n",
        "np.random.seed(algorithm_globals.random_seed)\n",
        "\n",
        "from qiskit.circuit.library import ZZFeatureMap, ZFeatureMap, PauliFeatureMap\n",
        "feature_dim = 2\n",
        "pauli_feature_map = PauliFeatureMap(feature_dimension=feature_dim, reps=1, paulis = ['Z','X','ZY'])\n",
        "pauli_feature_map.draw()\n",
        "\n",
        "# Define the feature map and variational form\n",
        "#FEATURE_MAP = pauli_feature_map\n",
        "#FEATURE_MAP = ZZFeatureMap(feature_dimension=2, reps=2)\n",
        "FEATURE_MAP=ZFeatureMap(feature_dimension=2, reps=2)\n",
        "\n",
        "VAR_FORM = TwoLocal(2, ['ry', 'rz'], 'cz', reps=2)\n",
        "\n",
        "# Combine feature map and variational form to create the circuit\n",
        "AD_HOC_CIRCUIT = FEATURE_MAP.compose(VAR_FORM)\n",
        "AD_HOC_CIRCUIT.measure_all()\n",
        "AD_HOC_CIRCUIT.decompose().draw()\n",
        "\n",
        "class localOptimizerLog:\n",
        "    \"\"\"Log to store optimizer's intermediate results\"\"\"\n",
        "    def __init__(self):\n",
        "        self.evaluations = []\n",
        "        self.parameters = []\n",
        "        self.costs = []\n",
        "    def update(self, evaluation, parameter, cost, _stepsize, _accept):\n",
        "        \"\"\"Save intermediate results. Optimizer passes five values\n",
        "        but we ignore the last two.\"\"\"\n",
        "        self.evaluations.append(evaluation)\n",
        "        self.parameters.append(parameter)\n",
        "        self.costs.append(cost)\n",
        "\n",
        "initial_point = np.random.random(VAR_FORM.num_parameters)\n",
        "\n",
        "#Global optimizer log\n",
        "class GlobalOptimizerLog:\n",
        "    \"\"\"Log to store optimizer's intermediate results\"\"\"\n",
        "    def __init__(self):\n",
        "        self.evaluations = []\n",
        "        self.parameters = []\n",
        "        self.costs = []\n",
        "    def update(self, evaluation, parameter, cost, _stepsize, _accept):\n",
        "        \"\"\"Save intermediate results. Optimizer passes five values\n",
        "        but we ignore the last two.\"\"\"\n",
        "        self.evaluations.append(evaluation)\n",
        "        self.parameters.append(parameter)\n",
        "        self.costs.append(cost)\n",
        "\n",
        "global_optimizer_log = GlobalOptimizerLog()\n",
        "\n",
        "# Initialize global model\n",
        "global_model = VQC(\n",
        "    feature_map=FEATURE_MAP,\n",
        "    ansatz=VAR_FORM,\n",
        "    loss='cross_entropy',\n",
        "    optimizer=SPSA(callback=global_optimizer_log.update),\n",
        "    initial_point=initial_point,\n",
        "    quantum_instance=BasicAer.get_backend('qasm_simulator')\n",
        ")\n",
        "\n",
        "\n",
        "#Local Training with Global Training\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Training settings\n",
        "num_rounds = 2\n",
        "learning_rate = 0.01  # Adjust as needed\n",
        "\n",
        "# Create a list of VQC instances, one for each client\n",
        "clients = []\n",
        "# Initialize an empty list to store client logs\n",
        "client_logs = []\n",
        "# Initialize an empty list to store client parameters\n",
        "client_parameters = []\n",
        "global_parameters=[]\n",
        "global_cost=[]\n",
        "\n",
        "# Array to store global cost\n",
        "GlobalCostperRoundAll=[]\n",
        "\n",
        "# Initialize global parameters\n",
        "global_parameters = np.random.rand(VAR_FORM.num_parameters)  # Initialize with random values or any desired initial values\n",
        "\n",
        "# Federated training loop\n",
        "for round_num in range(num_rounds):\n",
        "    global_cost=[]\n",
        "    client_logs=[]\n",
        "    aggregated_params = np.zeros_like(global_parameters)  # Initialize aggregated parameters\n",
        "    # Train each client's data on their VQC models\n",
        "    for client_id, data in client_data.items():\n",
        "        train_data = data['TRAIN_DATA']\n",
        "        train_labels = data['TRAIN_LABELS']\n",
        "        test_data = data['TEST_DATA']\n",
        "        test_labels = data['TEST_LABELS']\n",
        "        #train_data, train_labels, test_data, test_labels= data\n",
        "\n",
        "        # Set up the optimization\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")  # Suppress warnings\n",
        "        Locallog = localOptimizerLog()\n",
        "        optimizer = SPSA(maxiter=100, callback=Locallog.update)\n",
        "        vqc = VQC(feature_map=FEATURE_MAP,\n",
        "          ansatz=VAR_FORM,\n",
        "          loss='cross_entropy',\n",
        "          optimizer=SPSA(callback=Locallog.update),\n",
        "          initial_point=global_parameters,\n",
        "          quantum_instance=BasicAer.get_backend('qasm_simulator'))\n",
        "\n",
        "        vqc.fit(train_data, train_labels)\n",
        "        clients.append(vqc)\n",
        "        client_logs.append(Locallog)\n",
        "        client_parameters.append(Locallog.parameters[-1])\n",
        "        cost = Locallog.costs[-1]\n",
        "        print(f\"Round {round_num}, Client {client_id}: parameters = {Locallog.parameters[-1]}\")\n",
        "\n",
        "\n",
        "    #aggregated_params = np.zeros_like(global_parameters)  # Initialize aggregated parameters\n",
        "    aggregated_params = np.mean(client_parameters, axis=0)  # Calculate mean of parameters\n",
        "\n",
        "    # Update global parameters for the next round\n",
        "    global_parameters = aggregated_params\n",
        "    # Aggregate client parameters\n",
        "    print(f\"Round {round_num},global: aggregated_gradients = {global_parameters}\")\n",
        "\n",
        "    # Update the global optimizer log with the aggregated parameters\n",
        "    global_optimizer_log.parameters.append(global_parameters)\n",
        "    global_model.fit(train_data, train_labels)\n",
        "    global_cost = global_optimizer_log.costs\n",
        "    print(f\"Round {round_num},global cost: {global_cost}\")\n",
        "\n",
        "print(\"Federated training done\")\n",
        "\n",
        "import csv as csv\n",
        "# Slice the data if needed (e.g., taking the last 100 elements)\n",
        "data_to_save =global_optimizer_log.costs\n",
        "\n",
        "# Define the CSV file path\n",
        "csv_file_path = \"global_optimizer_costsZfeatureMap.csv\"\n",
        "\n",
        "# Write the data to the CSV file\n",
        "with open(csv_file_path, mode='w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow([\"Cost\"])  # Write a header row if needed\n",
        "    for cost in data_to_save:\n",
        "        writer.writerow([cost])\n",
        "\n",
        "print(f\"Data saved to {csv_file_path}\")"
      ],
      "metadata": {
        "id": "kb7gf9QlNwWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#csv_file_pathPauliCost = \"global_optimizer_costsPaulifeatureMap.csv\"\n",
        "#csv_file_pathZZCost = \"global_optimizer_costsZZfeatureMap.csv\"\n",
        "csv_file_pathZCost = \"global_optimizer_costsZfeatureMap.csv\"\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "\n",
        "PauliCost = []\n",
        "ZZCost=[]\n",
        "ZCost=[]\n",
        "\n",
        "print(len(ZCost))\n",
        "\n",
        "\n",
        "#================================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "\n",
        "\n",
        "#===============================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathZCost, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            ZCost.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathZCost}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "#each client cost variations over epochs for whole set code\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 4))\n",
        "# Plot cost for each client over rounds\n",
        "epoch = len(client_logs[0].evaluations)\n",
        "for client_id, log in enumerate(client_logs):\n",
        "    plt.plot(range(epoch), log.costs, label=f'Client {client_id + 1}', linestyle='dashed')\n",
        "#plt.plot(range(epoch), ZZCost[-100:], label=\"ZZFeatureMap\", linewidth=2,color='orange')#LR=0.01, maxiter=100\n",
        "#plt.plot(range(epoch), ZCost[-100:], label=\"global-SPSA-500\", linewidth=1, marker='*',color='blue')#LR=0.01 maxiter=500\n",
        "#plt.plot(range(epoch), PauliCost[-100:], label=\"PauliFeatureMap\", linewidth=2, color='red')\n",
        "#plt.plot(range(epoch), global_optimizer_log.costs, label='Global Cost')\n",
        "plt.plot(range(epoch), ZCost[-100:], label=\"ZFeatureMap\", linewidth=2, color='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('QiskitFL Cost Evolution Different FeatureMaps-TWOLocal Circuit-FeMnist Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cm1Ea4XoOs4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actual code below"
      ],
      "metadata": {
        "id": "ZXRyU7KL6G5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qiskit.circuit.library import ZZFeatureMap, ZFeatureMap, PauliFeatureMap\n",
        "\n",
        "def CircuitStruWithFeatureMap(CircuitStructure, FeatureMap, FileName):\n",
        "  # Set random seed\n",
        "  algorithm_globals.random_seed = 3142\n",
        "  np.random.seed(algorithm_globals.random_seed)\n",
        "\n",
        "\n",
        "  #VAR_FORM = TwoLocal(2, ['ry', 'rz'], 'cz', reps=2)\n",
        "  FEATURE_MAP=FeatureMap\n",
        "  VAR_FORM=CircuitStructure\n",
        "\n",
        "  # Combine feature map and variational form to create the circuit\n",
        "  AD_HOC_CIRCUIT = FEATURE_MAP.compose(VAR_FORM)\n",
        "  AD_HOC_CIRCUIT.measure_all()\n",
        "  AD_HOC_CIRCUIT.decompose().draw()\n",
        "\n",
        "  class localOptimizerLog:\n",
        "    \"\"\"Log to store optimizer's intermediate results\"\"\"\n",
        "    def __init__(self):\n",
        "        self.evaluations = []\n",
        "        self.parameters = []\n",
        "        self.costs = []\n",
        "    def update(self, evaluation, parameter, cost, _stepsize, _accept):\n",
        "        \"\"\"Save intermediate results. Optimizer passes five values\n",
        "        but we ignore the last two.\"\"\"\n",
        "        self.evaluations.append(evaluation)\n",
        "        self.parameters.append(parameter)\n",
        "        self.costs.append(cost)\n",
        "\n",
        "  initial_point = np.random.random(VAR_FORM.num_parameters)\n",
        "\n",
        "#Global optimizer log\n",
        "  class GlobalOptimizerLog:\n",
        "    \"\"\"Log to store optimizer's intermediate results\"\"\"\n",
        "    def __init__(self):\n",
        "        self.evaluations = []\n",
        "        self.parameters = []\n",
        "        self.costs = []\n",
        "    def update(self, evaluation, parameter, cost, _stepsize, _accept):\n",
        "        \"\"\"Save intermediate results. Optimizer passes five values\n",
        "        but we ignore the last two.\"\"\"\n",
        "        self.evaluations.append(evaluation)\n",
        "        self.parameters.append(parameter)\n",
        "        self.costs.append(cost)\n",
        "\n",
        "  global_optimizer_log = GlobalOptimizerLog()\n",
        "\n",
        "  # Initialize global model\n",
        "  global_model = VQC(\n",
        "    feature_map=FEATURE_MAP,\n",
        "    ansatz=VAR_FORM,\n",
        "    loss='cross_entropy',\n",
        "    optimizer=SPSA(callback=global_optimizer_log.update),\n",
        "    initial_point=initial_point,\n",
        "    quantum_instance=BasicAer.get_backend('qasm_simulator')\n",
        "  )\n",
        "\n",
        "\n",
        "  #Local Training with Global Training\n",
        "  import warnings\n",
        "  warnings.filterwarnings(\"ignore\")\n",
        "  # Training settings\n",
        "  num_rounds = 2\n",
        "  learning_rate = 0.01  # Adjust as needed\n",
        "\n",
        "  # Create a list of VQC instances, one for each client\n",
        "  clients = []\n",
        "  # Initialize an empty list to store client logs\n",
        "  client_logs = []\n",
        "  # Initialize an empty list to store client parameters\n",
        "  client_parameters = []\n",
        "  global_parameters=[]\n",
        "  global_cost=[]\n",
        "\n",
        "  # Array to store global cost\n",
        "  GlobalCostperRoundAll=[]\n",
        "\n",
        "  # Initialize global parameters\n",
        "  global_parameters = np.random.rand(VAR_FORM.num_parameters)  # Initialize with random values or any desired initial values\n",
        "\n",
        "  # Federated training loop\n",
        "  for round_num in range(num_rounds):\n",
        "      global_cost=[]\n",
        "      client_logs=[]\n",
        "      aggregated_params = np.zeros_like(global_parameters)  # Initialize aggregated parameters\n",
        "      # Train each client's data on their VQC models\n",
        "      for client_id, data in client_data.items():\n",
        "          train_data = data['TRAIN_DATA']\n",
        "          train_labels = data['TRAIN_LABELS']\n",
        "          test_data = data['TEST_DATA']\n",
        "          test_labels = data['TEST_LABELS']\n",
        "          #train_data, train_labels, test_data, test_labels= data\n",
        "\n",
        "          # Set up the optimization\n",
        "          with warnings.catch_warnings():\n",
        "              warnings.simplefilter(\"ignore\")  # Suppress warnings\n",
        "          Locallog = localOptimizerLog()\n",
        "          optimizer = SPSA(maxiter=100, callback=Locallog.update)\n",
        "          vqc = VQC(feature_map=FEATURE_MAP,\n",
        "            ansatz=VAR_FORM,\n",
        "            loss='cross_entropy',\n",
        "            optimizer=SPSA(callback=Locallog.update),\n",
        "            initial_point=global_parameters,\n",
        "            quantum_instance=BasicAer.get_backend('qasm_simulator'))\n",
        "\n",
        "          vqc.fit(train_data, train_labels)\n",
        "          clients.append(vqc)\n",
        "          client_logs.append(Locallog)\n",
        "          client_parameters.append(Locallog.parameters[-1])\n",
        "          cost = Locallog.costs[-1]\n",
        "          print(f\"Round {round_num}, Client {client_id}: parameters = {Locallog.parameters[-1]}\")\n",
        "\n",
        "\n",
        "      #aggregated_params = np.zeros_like(global_parameters)  # Initialize aggregated parameters\n",
        "      aggregated_params = np.mean(client_parameters, axis=0)  # Calculate mean of parameters\n",
        "\n",
        "      # Update global parameters for the next round\n",
        "      global_parameters = aggregated_params\n",
        "      # Aggregate client parameters\n",
        "      print(f\"Round {round_num},global: aggregated_gradients = {global_parameters}\")\n",
        "\n",
        "      # Update the global optimizer log with the aggregated parameters\n",
        "      global_optimizer_log.parameters.append(global_parameters)\n",
        "      global_model.fit(train_data, train_labels)\n",
        "      global_cost = global_optimizer_log.costs\n",
        "      print(f\"Round {round_num},global cost: {global_cost}\")\n",
        "      global_model.score(test_data,test_labels)\n",
        "\n",
        "  print(\"Federated training done\")\n",
        "\n",
        "  import csv as csv\n",
        "  # Slice the data if needed (e.g., taking the last 100 elements)\n",
        "  data_to_save =global_optimizer_log.costs\n",
        "\n",
        "  # Define the CSV file path\n",
        "  csv_file_path = FileName\n",
        "\n",
        "  # Write the data to the CSV file\n",
        "  with open(csv_file_path, mode='w', newline='') as csv_file:\n",
        "      writer = csv.writer(csv_file)\n",
        "      writer.writerow([\"Cost\"])  # Write a header row if needed\n",
        "      for cost in data_to_save:\n",
        "          writer.writerow([cost])\n",
        "\n",
        "  print(f\"Data saved to {csv_file_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LIWztAHGUpU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test circuit structures with Z fm\n",
        "from qiskit.circuit.library import RealAmplitudes,EfficientSU2\n",
        "import csv as csv\n",
        "feature_dim = 2\n",
        "# Define the feature map and variational form\n",
        "\n",
        "feature_dim = 2\n",
        "num_qubits=2\n",
        "\n",
        "FEATURE_MAPZ=ZFeatureMap(feature_dimension=2, reps=2)\n",
        "\n",
        "#Test 01\n",
        "variational_circ1 = TwoLocal(2, ['ry', 'rz'], 'cz', reps=2)\n",
        "fileName1=\"TwoLocalwithZ.csv\"\n",
        "\n",
        "variational_circ2 = RealAmplitudes(num_qubits, entanglement='full', reps=3)\n",
        "fileName2=\"RealAmplitudeswithZ.csv\"\n",
        "\n",
        "variational_circ3=EfficientSU2(feature_dim, reps=2)\n",
        "fileName3=\"EfficientSU2withz.csv\"\n",
        "\n",
        "\n",
        "CircuitStruWithFeatureMap(variational_circ1,FEATURE_MAPZ,fileName1)\n",
        "print(\"Test 01- TwoLocalwithZ completed\")\n",
        "print(\"---------\")\n",
        "CircuitStruWithFeatureMap(variational_circ2,FEATURE_MAPZ,fileName2)\n",
        "print(\"Test 02- RealAmplitudeswithZ completed\")\n",
        "print(\"---------\")\n",
        "CircuitStruWithFeatureMap(variational_circ3,FEATURE_MAPZ,fileName3)\n",
        "print(\"Test 03- EfficientSU2withPauli completed\")\n",
        "print(\"---------\")\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "\n",
        "TwoLocalwithZ = []\n",
        "RealAmplitudeswithZ=[]\n",
        "EfficientSU2withz=[]\n",
        "\n",
        "\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(fileName1, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            TwoLocalwithZ.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{fileName1}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "#================================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(fileName2, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            RealAmplitudeswithZ.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{fileName2}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "#===============================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(fileName3, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            EfficientSU2withz.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{fileName3}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "#each client cost variations over epochs for whole set code\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 4))\n",
        "# Plot cost for each client over rounds\n",
        "epoch = 100\n",
        "#for client_id, log in enumerate(client_logs):\n",
        "    #plt.plot(range(epoch), log.costs, label=f'Client {client_id + 1}', linestyle='dashed')\n",
        "plt.plot(range(epoch), RealAmplitudeswithZ[-100:], label=\"RealAmplitudes\", linewidth=2,color='orange')#LR=0.01, maxiter=100\n",
        "#plt.plot(range(epoch), ZCost[-100:], label=\"global-SPSA-500\", linewidth=1, marker='*',color='blue')#LR=0.01 maxiter=500\n",
        "plt.plot(range(epoch), TwoLocalwithZ[-100:], label=\"TwoLocal\", linewidth=2, color='red')\n",
        "#plt.plot(range(epoch), global_optimizer_log.costs, label='Global Cost')\n",
        "plt.plot(range(epoch), EfficientSU2withz[-100:], label=\"EfficientSU2\", linewidth=2, color='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('QiskitFL TWOLocal Circuit Structure-FeMnist Data', fontsize=10)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SU5bherC6OcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#each client cost variations over epochs for whole set code\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 4))\n",
        "# Plot cost for each client over rounds\n",
        "epoch = 100\n",
        "#for client_id, log in enumerate(client_logs):\n",
        "    #plt.plot(range(epoch), log.costs, label=f'Client {client_id + 1}', linestyle='dashed')\n",
        "plt.plot(range(epoch), TwoLocalwithZ[-100:], label=\"TwoLocal\", linewidth=1, color='blue')\n",
        "\n",
        "plt.plot(range(epoch), RealAmplitudeswithZ[-100:], label=\"RealAmplitudes\", linewidth=1,color='red')#LR=0.01, maxiter=100\n",
        "#plt.plot(range(epoch), ZCost[-100:], label=\"global-SPSA-500\", linewidth=1, marker='*',color='blue')#LR=0.01 maxiter=500\n",
        "#plt.plot(range(epoch), global_optimizer_log.costs, label='Global Cost')\n",
        "plt.plot(range(epoch), EfficientSU2withz[-100:], label=\"EfficientSU2\", linewidth=1 ,color='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('QiskitFL TWOLocal Circuit Structure-FeMnist Data', fontsize=10)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "baobPqQROBB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from qiskit.circuit.library import RealAmplitudes,EfficientSU2\n",
        "import csv as csv\n",
        "feature_dim = 2\n",
        "# Define the feature map and variational form\n",
        "\n",
        "feature_dim = 2\n",
        "FEATURE_MAPpauli = PauliFeatureMap(feature_dimension=feature_dim, reps=1, paulis = ['Z','X','ZY'])\n",
        "#pauli_feature_map.draw()\n",
        "FEATURE_MAPZZ = ZZFeatureMap(feature_dimension=2, reps=2)\n",
        "FEATURE_MAPZ=ZFeatureMap(feature_dimension=2, reps=2)\n",
        "\n",
        "#Test 01\n",
        "variational_circ1 = TwoLocal(2, ['ry', 'rz'], 'cz', reps=2)\n",
        "fileNamePauli1=\"TwoLocalwithPauli.csv\"\n",
        "fileNameZ1=\"TwoLocalwithZ.csv\"\n",
        "fileNameZZ1='TwoLocalwithZZ.csv'\n",
        "\n",
        "\n",
        "CircuitStruWithFeatureMap(variational_circ1,FEATURE_MAPZ,fileNameZ1)\n",
        "print(\"Test 01- part 1 z feature map completed\")\n",
        "print(\"---------\")\n",
        "CircuitStruWithFeatureMap(variational_circ1,FEATURE_MAPZZ,fileNameZZ1)\n",
        "print(\"Test 01- part 1 zz feature map completed\")\n",
        "print(\"---------\")\n",
        "CircuitStruWithFeatureMap(variational_circ1,FEATURE_MAPpauli,fileNamePauli1)\n",
        "print(\"Test 01- part 1 pauli feature map completed\")\n",
        "print(\"---------\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zVnExL73Uum5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "csv_file_pathPauliCost = \"TwoLocalwithPauli.csv\"\n",
        "csv_file_pathZZCost = \"TwoLocalwithZZ.csv\"\n",
        "csv_file_pathZCost = \"TwoLocalwithZ.csv\"\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "\n",
        "PauliCost = []\n",
        "ZZCost=[]\n",
        "ZCost=[]\n",
        "\n",
        "print(len(ZCost))\n",
        "\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathPauliCost, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            PauliCost.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathPauliCost}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "#================================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathZZCost, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            ZZCost.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathZZCost}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "#===============================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathZCost, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            ZCost.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathZCost}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "#each client cost variations over epochs for whole set code\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 4))\n",
        "# Plot cost for each client over rounds\n",
        "epoch = 100\n",
        "#for client_id, log in enumerate(client_logs):\n",
        "    #plt.plot(range(epoch), log.costs, label=f'Client {client_id + 1}', linestyle='dashed')\n",
        "plt.plot(range(epoch), ZZCost[-100:], label=\"ZZFeatureMap\", linewidth=2,color='orange')#LR=0.01, maxiter=100\n",
        "#plt.plot(range(epoch), ZCost[-100:], label=\"global-SPSA-500\", linewidth=1, marker='*',color='blue')#LR=0.01 maxiter=500\n",
        "plt.plot(range(epoch), PauliCost[-100:], label=\"PauliFeatureMap\", linewidth=2, color='red')\n",
        "#plt.plot(range(epoch), global_optimizer_log.costs, label='Global Cost')\n",
        "plt.plot(range(epoch), ZCost[-100:], label=\"ZFeatureMap\", linewidth=2, color='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('QiskitFL TWOLocal Circuit Structure-FeMnist Data', fontsize=10)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "x549S1oE_bdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Test 02\n",
        "num_qubits=2\n",
        "variational_circ2 = RealAmplitudes(num_qubits, entanglement='full', reps=3)\n",
        "fileNamePauli2=\"RealAmplitudeswithPauli.csv\"\n",
        "fileNameZ2=\"RealAmplitudeswithZ.csv\"\n",
        "fileNameZZ2='RealAmplitudeswithZZ.csv'\n",
        "\n",
        "CircuitStruWithFeatureMap(variational_circ2,FEATURE_MAPZ,fileNameZ2)\n",
        "print(\"Test 02- part 1 z feature map completed\")\n",
        "print(\"---------\")\n",
        "CircuitStruWithFeatureMap(variational_circ2,FEATURE_MAPZZ,fileNameZZ2)\n",
        "print(\"Test 02- part 1 zz feature map completed\")\n",
        "print(\"---------\")\n",
        "CircuitStruWithFeatureMap(variational_circ2,FEATURE_MAPpauli,fileNamePauli2)\n",
        "print(\"Test 02- part 1 pauli feature map completed\")\n",
        "print(\"---------\")\n",
        "\n"
      ],
      "metadata": {
        "id": "55rnjPloAdZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_pathPauliCost = \"RealAmplitudeswithPauli.csv\"\n",
        "csv_file_pathZZCost = \"RealAmplitudeswithZZ.csv\"\n",
        "csv_file_pathZCost = \"RealAmplitudeswithZ.csv\"\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "\n",
        "PauliCost = []\n",
        "ZZCost=[]\n",
        "ZCost=[]\n",
        "\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathPauliCost, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            PauliCost.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathPauliCost}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "#================================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathZZCost, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            ZZCost.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathZZCost}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "#===============================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathZCost, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            ZCost.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathZCost}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "#each client cost variations over epochs for whole set code\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 4))\n",
        "# Plot cost for each client over rounds\n",
        "epoch = 100\n",
        "#for client_id, log in enumerate(client_logs):\n",
        "    #plt.plot(range(epoch), log.costs, label=f'Client {client_id + 1}', linestyle='dashed')\n",
        "plt.plot(range(epoch), ZZCost[-100:], label=\"ZZFeatureMap\", linewidth=2,color='orange')#LR=0.01, maxiter=100\n",
        "#plt.plot(range(epoch), ZCost[-100:], label=\"global-SPSA-500\", linewidth=1, marker='*',color='blue')#LR=0.01 maxiter=500\n",
        "plt.plot(range(epoch), PauliCost[-100:], label=\"PauliFeatureMap\", linewidth=2, color='red')\n",
        "#plt.plot(range(epoch), global_optimizer_log.costs, label='Global Cost')\n",
        "plt.plot(range(epoch), ZCost[-100:], label=\"ZFeatureMap\", linewidth=2, color='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('QiskitFL RealiAmplitude Circuit Structure-FeMnist Data', fontsize=10)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "esGM1J_FUUdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Test 03\n",
        "variational_circ3=EfficientSU2(feature_dim, reps=2)\n",
        "fileNamePauli3=\"EfficientSU2withPauli.csv\"\n",
        "fileNameZ3=\"EfficientSU2withZ.csv\"\n",
        "fileNameZZ3='EfficientSU2withZZ.csv'\n",
        "\n",
        "CircuitStruWithFeatureMap(variational_circ3,FEATURE_MAPZ,fileNameZ3)\n",
        "print(\"Test 03- part 1 z feature map completed\")\n",
        "print(\"---------\")\n",
        "CircuitStruWithFeatureMap(variational_circ3,FEATURE_MAPZZ,fileNameZZ3)\n",
        "print(\"Test 03- part 1 zz feature map completed\")\n",
        "print(\"---------\")\n",
        "CircuitStruWithFeatureMap(variational_circ3,FEATURE_MAPpauli,fileNamePauli3)\n",
        "print(\"Test 03- part 1 pauli feature map completed\")\n",
        "print(\"---------\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6a_3-02hAf-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_pathPauliCost = \"EfficientSU2withPauli.csv\"\n",
        "csv_file_pathZZCost = \"EfficientSU2withZZ.csv\"\n",
        "csv_file_pathZCost = \"EfficientSU2withZ.csv\"\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "\n",
        "PauliCost = []\n",
        "ZZCost=[]\n",
        "ZCost=[]\n",
        "\n",
        "print(len(ZCost))\n",
        "\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathPauliCost, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            PauliCost.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathPauliCost}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "#================================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathZZCost, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            ZZCost.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathZZCost}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "#===============================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathZCost, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            ZCost.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathZCost}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "#each client cost variations over epochs for whole set code\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 4))\n",
        "# Plot cost for each client over rounds\n",
        "epoch = 100\n",
        "#for client_id, log in enumerate(client_logs):\n",
        "    #plt.plot(range(epoch), log.costs, label=f'Client {client_id + 1}', linestyle='dashed')\n",
        "plt.plot(range(epoch), ZZCost[-100:], label=\"ZZFeatureMap\", linewidth=2,color='orange')#LR=0.01, maxiter=100\n",
        "#plt.plot(range(epoch), ZCost[-100:], label=\"global-SPSA-500\", linewidth=1, marker='*',color='blue')#LR=0.01 maxiter=500\n",
        "plt.plot(range(epoch), PauliCost[-100:], label=\"PauliFeatureMap\", linewidth=2, color='red')\n",
        "#plt.plot(range(epoch), global_optimizer_log.costs, label='Global Cost')\n",
        "plt.plot(range(epoch), ZCost[-100:], label=\"ZFeatureMap\", linewidth=2, color='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('QiskitFL RealiAmplitude Circuit Structure-FeMnist Data', fontsize=10)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RS9eVn0mVLqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "csv_file_pathZTwoLocal = \"TwoLocalwithZ.csv\"\n",
        "csv_file_pathZReal = \"RealAmplitudeswithZ.csv\"\n",
        "csv_file_pathZEff = \"EfficientSU2withZ.csv\"\n",
        "\n",
        "csv_file_pathPauliTwoLocal =  \"TwoLocalwithPauli.csv\"\n",
        "csv_file_pathPauliReal = \"RealAmplitudeswithPauli.csv\"\n",
        "csv_file_pathPauliEff =\"EfficientSU2withPauli.csv\"\n",
        "\n",
        "csv_file_pathZZTwoLocal = \"TwoLocalwithZZ.csv\"\n",
        "csv_file_pathZZReal = \"RealAmplitudeswithZZ.csv\"\n",
        "csv_file_pathZZEff = \"EfficientSU2withZZ.csv\"\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "# Initialize an empty list to store the data\n",
        "\n",
        "ZTwoLocal = []\n",
        "ZReal=[] #ZZCost\n",
        "ZEff=[] #Zcost\n",
        "\n",
        "PauliTwoLocal = []\n",
        "PauliReal=[] #ZZCost\n",
        "PauliEff=[] #Zcost\n",
        "\n",
        "ZZTwoLocal = []\n",
        "ZZReal=[] #ZZCost\n",
        "ZZEff=[] #Zcost\n",
        "\n",
        "#***************************ZZfeatureMap*******************************\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathZTwoLocal, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            ZTwoLocal.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathZTwoLocal}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "#================================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathZReal, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            ZReal.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathZReal}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "#===============================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathZEff, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            ZEff.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathZEff}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "#==================================================================\n",
        "\n",
        "\n",
        "#***************************PaulifeatureMap*******************************\n",
        "\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathPauliTwoLocal, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            PauliTwoLocal.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathPauliTwoLocal}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "#================================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathPauliReal, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            PauliReal.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathPauliReal}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "#===============================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathPauliEff, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            PauliEff.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathPauliEff}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "#***************************ZZfeatureMap*******************************\n",
        "\n",
        "try:\n",
        "    with open(csv_file_pathZZTwoLocal, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            ZZTwoLocal.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathZZTwoLocal}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "#================================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathZZReal, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            ZZReal.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathZZReal}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "#===============================================\n",
        "# Read data from the CSV file and append it to the list\n",
        "try:\n",
        "    with open(csv_file_pathZZEff, mode='r', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        next(reader)  # Skip the header row if present\n",
        "        for row in reader:\n",
        "            # Assuming the CSV file has one column\n",
        "            ZZEff.append(float(row[0]))  # Convert to float if needed\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{csv_file_pathZZEff}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "#==================================================================\n",
        "\n",
        "#each client cost variations over epochs for whole set code\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 4))\n",
        "# Plot cost for each client over rounds\n",
        "epoch = 100\n",
        "#for client_id, log in enumerate(client_logs):\n",
        "    #plt.plot(range(epoch), log.costs, label=f'Client {client_id + 1}', linestyle='dashed')\n",
        "\n",
        "plt.plot(range(epoch), ZReal[-100:], label=\"Z_with_Real \", linewidth=2,color='blue',linestyle='dashed')#LR=0.01, maxiter=100\n",
        "#plt.plot(range(epoch), ZCost[-100:], label=\"global-SPSA-500\", linewidth=1, marker='*',color='blue')#LR=0.01 maxiter=500\n",
        "plt.plot(range(epoch), ZTwoLocal[-100:], label=\"Z_with_Two\", linewidth=2, color='green')\n",
        "#plt.plot(range(epoch), global_optimizer_log.costs, label='Global Cost')\n",
        "plt.plot(range(epoch), ZEff[-100:], label=\"Z_with_Eff\", linewidth=2, color='cyan',linestyle='dotted')\n",
        "\n",
        "\n",
        "plt.plot(range(epoch), PauliReal[-100:], label=\"Pauli_with_Real \", linewidth=2,color='orange',linestyle='dashed')#LR=0.01, maxiter=100\n",
        "#plt.plot(range(epoch), ZCost[-100:], label=\"global-SPSA-500\", linewidth=1, marker='*',color='blue')#LR=0.01 maxiter=500\n",
        "plt.plot(range(epoch), PauliTwoLocal[-100:], label=\"Pauli_with_Two\", linewidth=2, color='red')\n",
        "#plt.plot(range(epoch), global_optimizer_log.costs, label='Global Cost')\n",
        "plt.plot(range(epoch), PauliEff[-100:], label=\"Pauli_with_Eff\", linewidth=2, color='yellow',linestyle='dotted')\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(range(epoch), ZZReal[-100:], label=\"ZZ_with_Real\", linewidth=2,color='black',linestyle='dashed')#LR=0.01, maxiter=100\n",
        "#plt.plot(range(epoch), ZCost[-100:], label=\"global-SPSA-500\", linewidth=1, marker='*',color='blue')#LR=0.01 maxiter=500\n",
        "plt.plot(range(epoch), ZZTwoLocal[-100:], label=\"ZZ_with_Two\", linewidth=2, color='purple')\n",
        "#plt.plot(range(epoch), global_optimizer_log.costs, label='Global Cost')\n",
        "plt.plot(range(epoch), ZZEff[-100:], label=\"ZZ_with_Eff\", linewidth=2, color='brown',linestyle='dotted')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('QiskitFL Different Circuit Structures and FeatureMaps-FeMnist Data', fontsize=10)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0jbNK7CqVgWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***********************************Implement Femnist with Qiskit VQC****************\n"
      ],
      "metadata": {
        "id": "L9h_UFYUIkye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def filter_data(x, y):\n",
        "  \"\"\"\n",
        "  Helper Function to filter the dataset\n",
        "  x is image independent feature , y is dependent feature\n",
        "  \"\"\"\n",
        "  #filter the data using labels\n",
        "  keep = (y == 5) | (y == 9)\n",
        "  #value for keep vaiable is true only for y=lable 5 and lable 9\n",
        "  x, y = x[keep], y[keep]\n",
        "\n",
        "  # convert labels to boolean\n",
        "  # y = True if y==5\n",
        "  # y = False if y==9\n",
        "  y = y == 5\n",
        "  return x,y\n",
        "\n",
        "#===========================\n",
        "\n",
        "#importing the Fashion MNIST dataset from keras\n",
        "from tensorflow.keras.datasets import fashion_mnist as dataset\n",
        "(X_train, y_train), (X_test, y_test) = dataset.load_data()\n",
        "print(\"The shape of the X_train is {}\".format( X_train.shape))\n",
        "print(\"The shape of the y_train is {}\".format(y_train.shape))\n",
        "print(\"The shape of the X_test is {}\".format(X_test.shape))\n",
        "print(\"The shape of the y_test is {}\".format(y_test.shape))\n",
        "\n",
        "\n",
        "#Filter the train set\n",
        "X_train, y_train = filter_data(X_train, y_train)\n",
        "\n",
        "#Filter the test_set\n",
        "X_test, y_test = filter_data(X_test, y_test)\n",
        "\n",
        "# Let's have a look at the shapes of train and test data\n",
        "print(\"           \")\n",
        "print(\"=======Filter data for binary classification\")\n",
        "print(\"The shape of the X_train is {}\".format( X_train.shape))\n",
        "print(\"The shape of the y_train is {}\".format(y_train.shape))\n",
        "print(\"The shape of the X_test is {}\".format(X_test.shape))\n",
        "print(\"The shape of the y_test is {}\".format(y_test.shape))\n",
        "\n",
        "#Let's have a look at the first image from our X_train and the\n",
        "# corresponding label from y_train\n",
        "print(\"\")\n",
        "print(\"The First Image has the label {}\".format(y_train[0]))\n",
        "plt.imshow(X_train[0])\n",
        "plt.colorbar()\n",
        "plt.title('Visualization of the Dataset')\n",
        "plt.show()\n",
        "\n",
        "#Normalizing the train and test image data\n",
        "X_train = X_train/255.0\n",
        "X_test = X_test/ 255.0\n",
        "\n",
        "#Let's again have a look at the first image from our X_train and\n",
        "#see if we have successfully normalized the datasets\n",
        "plt.imshow(X_train[0])\n",
        "plt.colorbar()\n",
        "print(\"The First Image after pixel normalization\")\n",
        "plt.title('Visualization of the Dataset')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Before proceeding, we need to reshape our images in the dataset\n",
        "x_train= X_train.reshape(X_train.shape[0], *(28,28,1))#X_train.shape[0] indicate number of images in xtrain, 1 indicate channels here since grayscale 1\n",
        "x_test= X_test.reshape(X_test.shape[0], *(28,28,1))\n",
        "\n",
        "\n",
        "# Downscaling the images\n",
        "X_train= tf.image.resize(x_train, (2,2)).numpy()\n",
        "X_test= tf.image.resize(x_test, (2,2)).numpy()\n",
        "\n",
        "#Let's again have a look at the first image from our resized X_train\n",
        "plt.imshow(X_train[0,:,:,0])\n",
        "plt.colorbar()\n",
        "plt.title('Visualization of the Resized Dataset')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Splitting the training fdataset into train and validation datasets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.15, random_state=0)\n",
        "\n",
        "print(\"The shape of the X_train is {}\".format(X_train.shape))\n",
        "print(\"The shape of the y_train is {}\".format(y_train.shape))\n",
        "print(\"The shape of the X_valid is {}\".format(X_valid.shape))\n",
        "print(\"The shape of the y_valid is {}\".format(y_valid.shape))"
      ],
      "metadata": {
        "id": "Y3Yu-D_z3hSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Normalize the pixel values to range [0, 1]\n",
        "#x_train = x_train.astype('float32') / 255.0\n",
        "#x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Flatten the images (convert 28x28 images to 1D arrays of length 784)\n",
        "#x_train_flattened = x_train.reshape(x_train.shape[0], -1)\n",
        "#x_test_flattened = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "#======================================================================\n",
        "#Federated Data preprocess\n",
        "\n",
        "# Split the data among clients\n",
        "n_clients = 5  # Set the number of clients (adjust as needed)\n",
        "client_data = []\n",
        "\n",
        "# Split the training data among clients using train_test_split\n",
        "for i in range(n_clients):\n",
        "    x_train_client, _, y_train_client, _ = train_test_split(\n",
        "        x_train, y_train, test_size=0.9 / n_clients, random_state=i\n",
        "    )\n",
        "    client_data.append((x_train_client, y_train_client))\n",
        "\n",
        "# Print the shapes of each client's dataset\n",
        "for i, (x_train_client, y_train_client) in enumerate(client_data):\n",
        "    print(f\"Client {i+1} - x_train shape: {x_train_client.shape}, y_train shape: {y_train_client.shape}\")\n",
        "\n",
        "# Load the MNIST dataset\n",
        "#(_, y_train), (_, y_test) = mnist.load_data()\n",
        "\n",
        "# Convert the class labels to one-hot encoded format\n",
        "num_classes = 2 # Number of classes in the MNIST dataset\n",
        "#y_train_onehot = tf.one_hot(y_train, num_classes)\n",
        "#y_test_onehot = tf.one_hot(y_test, num_classes)\n",
        "\n",
        "# Print the shape of one-hot encoded labels\n",
        "#print(\"Shape of y_train_onehot:\", y_train_onehot.shape)  # (60000, 10)\n",
        "#print(\"Shape of y_test_onehot:\", y_test_onehot.shape)"
      ],
      "metadata": {
        "id": "v_ljsYqNAo0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pgxaigETB7F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDPAWB7LYL2w"
      },
      "outputs": [],
      "source": [
        "from qiskit.circuit.library import ZZFeatureMap, ZFeatureMap, PauliFeatureMap\n",
        "\n",
        "def CircuitStruWithFeatureMap(CircuitStructure, FeatureMap, FileName):\n",
        "  # Set random seed\n",
        "  algorithm_globals.random_seed = 3142\n",
        "  np.random.seed(algorithm_globals.random_seed)\n",
        "\n",
        "\n",
        "  #VAR_FORM = TwoLocal(2, ['ry', 'rz'], 'cz', reps=2)\n",
        "  FEATURE_MAP=FeatureMap\n",
        "  VAR_FORM=CircuitStructure\n",
        "\n",
        "  # Combine feature map and variational form to create the circuit\n",
        "  AD_HOC_CIRCUIT = FEATURE_MAP.compose(VAR_FORM)\n",
        "  AD_HOC_CIRCUIT.measure_all()\n",
        "  AD_HOC_CIRCUIT.decompose().draw()\n",
        "\n",
        "  class localOptimizerLog:\n",
        "    \"\"\"Log to store optimizer's intermediate results\"\"\"\n",
        "    def __init__(self):\n",
        "        self.evaluations = []\n",
        "        self.parameters = []\n",
        "        self.costs = []\n",
        "    def update(self, evaluation, parameter, cost, _stepsize, _accept):\n",
        "        \"\"\"Save intermediate results. Optimizer passes five values\n",
        "        but we ignore the last two.\"\"\"\n",
        "        self.evaluations.append(evaluation)\n",
        "        self.parameters.append(parameter)\n",
        "        self.costs.append(cost)\n",
        "\n",
        "  initial_point = np.random.random(VAR_FORM.num_parameters)\n",
        "\n",
        "#Global optimizer log\n",
        "  class GlobalOptimizerLog:\n",
        "    \"\"\"Log to store optimizer's intermediate results\"\"\"\n",
        "    def __init__(self):\n",
        "        self.evaluations = []\n",
        "        self.parameters = []\n",
        "        self.costs = []\n",
        "    def update(self, evaluation, parameter, cost, _stepsize, _accept):\n",
        "        \"\"\"Save intermediate results. Optimizer passes five values\n",
        "        but we ignore the last two.\"\"\"\n",
        "        self.evaluations.append(evaluation)\n",
        "        self.parameters.append(parameter)\n",
        "        self.costs.append(cost)\n",
        "\n",
        "  global_optimizer_log = GlobalOptimizerLog()\n",
        "\n",
        "  # Initialize global model\n",
        "  global_model = VQC(\n",
        "    feature_map=FEATURE_MAP,\n",
        "    ansatz=VAR_FORM,\n",
        "    loss='cross_entropy',\n",
        "    optimizer=SPSA(callback=global_optimizer_log.update),\n",
        "    initial_point=initial_point,\n",
        "    quantum_instance=BasicAer.get_backend('qasm_simulator')\n",
        "  )\n",
        "\n",
        "\n",
        "  #Local Training with Global Training\n",
        "  import warnings\n",
        "  warnings.filterwarnings(\"ignore\")\n",
        "  # Training settings\n",
        "  num_rounds = 50\n",
        "  learning_rate = 0.01  # Adjust as needed\n",
        "\n",
        "  # Create a list of VQC instances, one for each client\n",
        "  clients = []\n",
        "  # Initialize an empty list to store client logs\n",
        "  client_logs = []\n",
        "  # Initialize an empty list to store client parameters\n",
        "  client_parameters = []\n",
        "  global_parameters=[]\n",
        "  global_cost=[]\n",
        "\n",
        "  # Array to store global cost\n",
        "  GlobalCostperRoundAll=[]\n",
        "\n",
        "  # Initialize global parameters\n",
        "  global_parameters = np.random.rand(VAR_FORM.num_parameters)  # Initialize with random values or any desired initial values\n",
        "\n",
        "  # Federated training loop\n",
        "  for round_num in range(num_rounds):\n",
        "      global_cost=[]\n",
        "      client_logs=[]\n",
        "      aggregated_params = np.zeros_like(global_parameters)  # Initialize aggregated parameters\n",
        "      # Train each client's data on their VQC models\n",
        "      for client_id, data in client_data.items():\n",
        "          train_data = data['TRAIN_DATA']\n",
        "          train_labels = data['TRAIN_LABELS']\n",
        "          test_data = data['TEST_DATA']\n",
        "          test_labels = data['TEST_LABELS']\n",
        "          #train_data, train_labels, test_data, test_labels= data\n",
        "\n",
        "          # Set up the optimization\n",
        "          with warnings.catch_warnings():\n",
        "              warnings.simplefilter(\"ignore\")  # Suppress warnings\n",
        "          Locallog = localOptimizerLog()\n",
        "          optimizer = SPSA(maxiter=100, callback=Locallog.update)\n",
        "          vqc = VQC(feature_map=FEATURE_MAP,\n",
        "            ansatz=VAR_FORM,\n",
        "            loss='cross_entropy',\n",
        "            optimizer=SPSA(callback=Locallog.update),\n",
        "            initial_point=global_parameters,\n",
        "            quantum_instance=BasicAer.get_backend('qasm_simulator'))\n",
        "\n",
        "          vqc.fit(train_data, train_labels)\n",
        "          clients.append(vqc)\n",
        "          client_logs.append(Locallog)\n",
        "          client_parameters.append(Locallog.parameters[-1])\n",
        "          cost = Locallog.costs[-1]\n",
        "          print(f\"Round {round_num}, Client {client_id}: parameters = {Locallog.parameters[-1]}\")\n",
        "\n",
        "\n",
        "      #aggregated_params = np.zeros_like(global_parameters)  # Initialize aggregated parameters\n",
        "      aggregated_params = np.mean(client_parameters, axis=0)  # Calculate mean of parameters\n",
        "\n",
        "      # Update global parameters for the next round\n",
        "      global_parameters = aggregated_params\n",
        "      # Aggregate client parameters\n",
        "      print(f\"Round {round_num},global: aggregated_gradients = {global_parameters}\")\n",
        "\n",
        "      # Update the global optimizer log with the aggregated parameters\n",
        "      global_optimizer_log.parameters.append(global_parameters)\n",
        "      global_model.fit(train_data, train_labels)\n",
        "      global_cost = global_optimizer_log.costs\n",
        "      print(f\"Round {round_num},global cost: {global_cost}\")\n",
        "\n",
        "  print(\"Federated training done\")\n",
        "\n",
        "  import csv as csv\n",
        "  # Slice the data if needed (e.g., taking the last 100 elements)\n",
        "  data_to_save =global_optimizer_log.costs\n",
        "\n",
        "  # Define the CSV file path\n",
        "  csv_file_path = FileName\n",
        "\n",
        "  # Write the data to the CSV file\n",
        "  with open(csv_file_path, mode='w', newline='') as csv_file:\n",
        "      writer = csv.writer(csv_file)\n",
        "      writer.writerow([\"Cost\"])  # Write a header row if needed\n",
        "      for cost in data_to_save:\n",
        "          writer.writerow([cost])\n",
        "\n",
        "  print(f\"Data saved to {csv_file_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FLattening the images\n",
        "X_train = X_train.reshape(X_train.shape[0], *(1,4,1))\n",
        "X_valid = X_valid.reshape(X_valid.shape[0], *(1,4,1))\n",
        "X_test = X_test.reshape(X_test.shape[0], *(1,4,1))\n",
        "\n",
        "#Let's have a look on the first example\n",
        "print(X_train[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "wQ9QjxYmE_mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gFb2ir-pOIaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def binary_encode(X,threshold=0.5):\n",
        "  \"\"\"\n",
        "  Encodes the given datset to use binary encoding\n",
        "\n",
        "  Parameters:\n",
        "  X(array) : Image data to be processed for encoding\n",
        "  threshold(float): Threshold for binary encoding, 0.5 by default\n",
        "\n",
        "  Returns:\n",
        "  encoded_images(array): Binary encoded Image Data\n",
        "\n",
        "  \"\"\"\n",
        "  encoded_images = list()\n",
        "  for image in X:\n",
        "    # pixel value is 1 if it's greater than threshold or else zero\n",
        "    encoded_image = [1 if j>threshold else 0 for j in image[0]]\n",
        "    encoded_images.append(encoded_image)\n",
        "  return np.array(encoded_images)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2asw0qeSQYuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " X_train = binary_encode(X_train)\n",
        "print(X_train[0])"
      ],
      "metadata": {
        "id": "iswPVbcOFV2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from qiskit.circuit.library import ZZFeatureMap\n",
        "circuit = ZZFeatureMap(3, reps=1, insert_barriers=True)\n",
        "circuit.decompose().draw()\n",
        "x = [0.1, 0.2, 0.3]\n",
        "encode = circuit.bind_parameters(x)\n",
        "encode.decompose().draw()"
      ],
      "metadata": {
        "id": "qXysSQA7hsMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data encoding"
      ],
      "metadata": {
        "id": "oACYjg68Pl0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from qiskit import QuantumCircuit, Aer, transpile, assemble\n",
        "from qiskit.visualization import plot_bloch_multivector\n",
        "\n",
        "# Load MNIST data (replace this with actual MNIST data loading code)\n",
        "# For this example, let's consider a single grayscale image with 4x4 pixels\n",
        "mnist_image = np.array([\n",
        "    [0.2, 0.5, 0.8, 0.7],\n",
        "    [0.4, 0.6, 0.3, 0.1],\n",
        "    [0.9, 0.3, 0.2, 0.5],\n",
        "    [0.6, 0.4, 0.7, 0.2]\n",
        "])\n",
        "\n",
        "# Quantum circuit parameters\n",
        "n_qubits = 16  # Number of qubits (should match the image dimensions)\n",
        "state_vector_simulator = Aer.get_backend('statevector_simulator')\n",
        "\n",
        "# Normalize pixel values and convert to binary strings\n",
        "binary_strings = []\n",
        "for row in mnist_image:\n",
        "    for pixel in row:\n",
        "        binary_strings.append(format(int(pixel * 255), '08b'))\n",
        "\n",
        "# Create a quantum circuit\n",
        "qc = QuantumCircuit(n_qubits)\n",
        "\n",
        "# Apply amplitude encoding\n",
        "for i, binary_string in enumerate(binary_strings):\n",
        "    for j, bit in enumerate(binary_string):\n",
        "        qubit_index = i * 4 + j\n",
        "        if qubit_index < n_qubits:\n",
        "            if bit == '1':\n",
        "                qc.x(qubit_index)  # Apply an X gate to set the qubit to |1>\n",
        "\n",
        "# Simulate the quantum state\n",
        "job = assemble(transpile(qc, state_vector_simulator))\n",
        "result = state_vector_simulator.run(job).result()\n",
        "statevector = result.get_statevector()\n",
        "\n",
        "# Print and visualize the statevector\n",
        "print(\"Encoded Quantum Statevector:\")\n",
        "print(statevector)\n",
        "\n",
        "# Visualize the Bloch sphere representation of the statevector\n",
        "#plot_bloch_multivector(statevector)\n"
      ],
      "metadata": {
        "id": "jVI4Py4APlCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the QNN architecture\n",
        "n_qubits = 8  # Number of qubits in the quantum circuit\n",
        "n_layers = 2  # Number of layers in the quantum circuit\n",
        "\n",
        "# Create a quantum circuit with trainable parameters\n",
        "def create_qnn_circuit(params):\n",
        "    qc = QuantumCircuit(n_qubits)\n",
        "\n",
        "    # Apply trainable quantum gates for each layer\n",
        "    for layer in range(n_layers):\n",
        "        for qubit in range(n_qubits):\n",
        "            # Add trainable parameters to the quantum gates\n",
        "            theta = params[layer * n_qubits + qubit]\n",
        "            qc.rx(theta, qubit)\n",
        "\n",
        "        # Apply CNOT gate to entangle the qubits\n",
        "        for qubit in range(n_qubits - 1):\n",
        "            qc.cx(qubit, qubit + 1)\n",
        "\n",
        "    return qc\n",
        "\n",
        "# Initialize random parameters for each client's QNN\n",
        "n_params = n_qubits * n_layers\n",
        "np.random.seed(0)  # Set a seed for reproducibility\n",
        "client_params = [np.random.rand(n_params) for _ in range(len(client_datasets))]\n",
        "\n",
        "\n",
        "# Simulate quantum circuit execution and calculate loss\n",
        "# Simulate quantum circuit execution and calculate loss\n",
        "# Simulate quantum circuit execution and calculate loss\n",
        "def simulate_qnn(params, data, true_labels):\n",
        "    # Create a quantum circuit for the QNN\n",
        "    circuit = create_qnn_circuit(params)\n",
        "\n",
        "    # Simulator backend\n",
        "    backend = Aer.get_backend('statevector_simulator')\n",
        "\n",
        "    # Execute the quantum circuit\n",
        "    job = execute(circuit, backend)\n",
        "    result = job.result()\n",
        "    statevector = result.get_statevector()\n",
        "\n",
        "    # Reshape the statevector to match the shape of data\n",
        "    statevector = statevector.reshape(-1, statevector.shape[-1])\n",
        "\n",
        "    # Calculate the prediction probabilities from statevector\n",
        "    prediction = np.abs(statevector) ** 2\n",
        "\n",
        "    print(prediction.shape)\n",
        "    print(true_labels.shape)\n",
        "\n",
        "    # Ensure true_labels and prediction have the same shape\n",
        "    if true_labels.shape != prediction.shape:\n",
        "        raise ValueError(\"Shapes of true_labels and prediction do not match.\")\n",
        "\n",
        "    # Calculate the loss (cross-entropy between target and prediction)\n",
        "    loss = -np.mean(np.sum(true_labels * np.log(prediction), axis=1))\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Federated learning loop\n",
        "n_iterations = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    # Perform training on each client's dataset\n",
        "    for i, data in enumerate(client_datasets):\n",
        "        client_data, client_labels = data\n",
        "        true_labels_onehot = tf.one_hot(client_labels, num_classes)\n",
        "        loss = simulate_qnn(client_params[i], data, true_labels_onehot)\n",
        "\n",
        "        # Calculate gradient and update parameters (simple gradient descent)\n",
        "        gradients = np.gradient(client_params[i], loss)\n",
        "        client_params[i] = client_params[i] - learning_rate * gradients\n",
        "\n",
        "    # Aggregate parameters from all clients and calculate global parameters\n",
        "    global_params = np.mean(client_params, axis=0)\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have the client_datasets and true_labels arrays from the previous code\n",
        "# And 'global_params' from the federated learning loop\n",
        "\n",
        "# Let's simulate QNN for the first client dataset:\n",
        "client_1_data, client_1_labels = client_datasets[0]\n",
        "loss_client_1 = simulate_qnn(global_params, client_1_data, tf.one_hot(client_1_labels, num_classes))\n",
        "print(f\"Loss for client 1: {loss_client_1}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tZowYjrJ4kG1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}